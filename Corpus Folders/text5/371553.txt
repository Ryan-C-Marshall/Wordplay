
@@45151224 @5151224/ <h> How to : Install software when it always displays there are pending actions from a previous install <h> How to proceed with an installation when the error message : " A Restart from a Previous Installation is Pending " or- " There are pending actions from a previous install " or " Setup can not continue because a restart from a previous installation or update is pending " keeps showing up . <p> In occasions you 'll find yourself unable to install software receiving an error message that indicates : " A Restart from a Previous Installation is Pending " . This is usually indicated as there are pending operations and generally speaking a restart of your system will force those operations to take place . Because of that new installations wait for the previous ones to complete their required file changes to avoid any potential issues . The real problem is that after you restart the machsesine you continue to get this error message . <p> In my case recently I was installing Lync Server when the SQL installer kept indicating it could n't proceed as there were pending @ @ @ @ @ @ @ @ @ @ some research it turned out that my Group Policy that installed network printers was causing this behavior . There were pending file renaming operations because of that . In other words , every time I restarted my server group policy would install the printers which would then as part of the driver install indicate the OS to perform file rename operations on reboot . The rest of the story makes sense , every time I restart the group policy would cause those operations to be pending causing SQL express never to install . In order to resolve this you just need to go into the registry ( via regedit ) and delete the entries that indicates the pending operations on restart . <p> This article contains information about editing the registry . Before you edit the registry , make sure that you understand how to restore the registry if a problem occurs . For information about how to restore the registry , view the " Restore the Registry " Help topic in Regedit.exe or Regedt32.exe . <p> To remove an orphaned UpdateExeVolatile registry key value <p> Open a registry editor @ @ @ @ @ @ @ @ @ @ HKLM / SOFTWARE / Microsoft / Updates <p> In the right navigation pane , double-click the- UpdateExeVolatile- key . <p> If you notice , all the files that are going to be renamed are in the spool directory which is the Windows Printer Spooler so you can rest assured that all the operations are due to the printers not any software you installed . <p> There are other situations where this can apply as well according to Microsoft . It makes sense as pending actions is a check that is perform by any major piece of software that is performing an installation . Below are other common cases : <p> The Exchange Server Analyzer reads the following registry key to determine whether a system restart is required after installation or removal of a software update such as a security update , critical update , or hotfix . <p> HKEYLOCALMACHINE SOFTWARE MicrosoftUpdates UpdateExeVolatile <p> The Exchange Analyzer also checks the following registry key to determine whether a previous software update installation was not completed and the system must be restarted to finish the installation . 
@@45151228 @5151228/ <p> If you want to completely wipe all traces of Exchange Server 2013 from your Active Directory then follow this simple instructions . This has worked thus far for me but perhaps I missed something so feel free to provide any feedback you may have . <p> Removing Exchange from AD is not something you might want to do carelessly . It should be obvious that wiping this information should be equivalent to starting your whole Exchange Infrastructure from scratch . This may result on losing mailboxes , emails , settings , email deliverability , etc . If you are familiar with the risks and understand what you are doing that 's fine , but if any of this sounds new to you then you should be most careful and seek professional help . <p> We are going to use the ADSIEdit tool which is used to edit Active Directory Metadata/Schema/etc . This is probably more delicate that messing with the Windows Registry so please proceed with extreme caution . <p> You 'll find this method is usually a last resort in some extreme cases like : <p> a ) The @ @ @ @ @ @ @ @ @ @ help you ( or you were doing an initial enterprise installation and the installer failed halfway and now you have an unclean installation from which you cant proceed ) <p> or <p> b ) You have a server that is kaput so you cant run the uninstaller and remove it from AD ( which in this case you might just want to remove some entries not entire subtrees like well do here . ) <h> I. Remove the entire Microsoft Exchange Configuration <p> First , open ADSIEdit ( which you can find on your start menu . Once opened go to Action -&gt; Connect to and there select Configuration like shown here : <p> Navigate to this path to delete the following two Exchange Subtrees : <p> CN=Configuration , DC=DOMAIN , DC=LOCAL <p> CN=Services <p> CN=Microsoft Exchange ( DELETE ) <p> CN=Microsoft Exchange Autodiscover ( DELETE ) <p> Once you are done open the connection but this time to the " Default Naming Context " in order to delete the Exchange security groups and objects : <p> CN=Default naming context , DC=DOMAIN , DC=LOCAL <p> CN=Microsoft Exchange Security Groups- ( @ @ @ @ @ @ @ @ @ @ <h> II . Remove automatically generated users / groups <p> There are a few Active Directory users that are generated automatically by Exchange . Some serve as Discovery services , others are used to monitor the health of the system . Regardless these will no longer be needed if you have permanently removed Exchange from your organization : <p> DiscoverySearch MailboxGUID <p> Exchange **25;0;TOOLONG <p> FederatedEmail.GUID <p> Migration.GUID <p> *SystemMailboxGUID <p> *HealthMailboxGUID <h> III . Remove settings from a server <p> If you have access to your Exchange server you can delete a few things to leave it almost in pre-Exchange state . I strongly suggest you simply start from a fresh Windows Installation as it is hard to truly leave a server in a pre-exchange state . 
@@45151229 @5151229/ <h> What is Store.exe ? Why does it use almost all my memory ? <h> Exchange 2010 store.exe service <p> Store.exe is a service that is part of Microsoft Exchange server 2010 and is found on your mailbox server . Many people come across this process as it is known to use a lot of memory from the host machine . If you are running an Exchange server on your machine you are most likely going to see this process . <p> The behavior described here consists of store.exe using a considerable- amount- of memory resources . On a low memory server this is usually not an issue as store does not have much memory at its disposal to grab . However , say you have 32gb of RAM on your machine , you might see Store.exe using half or more which would be a reason of alarm for you . If I-m not mistaken Exchange is not recommended to run on a virtual machine with Dynamic memory allocation as this could be an issue ( maybe SQL as well do n't  remember ) . <p> As far as @ @ @ @ @ @ @ @ @ @ the cache it uses is allocated as much as possible on the RAM . This improves considerably the experience of your client users . Now , Store.exe is smart so it releases memory as it sees demand from other applications coming in . <p> In my scenario I have a Windows Small Business Server 2011 machine and I keep adding more and more RAM to improve its performance . As you know as RAM starts running out you start swapping to the Hard drive taking a hit to performance . Increasing the RAM has allowed me to decrease the filepage use but as I keep increasing it I was being blown by how much the server kept using . As you can imagine the one process that was using the most by far was store.exe . After reading how it works and looking at the performance tools I found a good balance . Now my server has enough memory to not cause any serious memory swapping even though 94% of the physical memory is being used thanks to Exchanges Store.exe <h> Is it safe ? <p> Yes , this is @ @ @ @ @ @ @ @ @ @ part of Microsofts Exchange Server 2010 . If you do n't  have Exchange server installed then continue studying the process to determine its origin and legitimacy . <h> Can I remove it ? <p> No , this is a center piece of the Exchange Mailbox services so you would mess up your Exchange installation . If the amount of memory it uses alarms you or causes you problems there is a way to limit it ( as there is with SQL server ) . - By default , the **26;27;TOOLONG key is not set , which means the store can allocate the memory it needs dynamically . ESE ( store.exe ) will grow the cache to consume almost all available RAM on the server if there is no other memory pressure on the system . It 's not recommended to modify the **26;55;TOOLONG attribute of the information store object. - Lowering this value may degrade performance for the client users and on the host server . In most cases even if it uses all available memory it should release it as required . 
@@45151230 @5151230/ <h> How to : Delete a DHCP Failover relationship when the partner server is unreachable <h> How to : Delete a DHCP Failover relationship when the partner server is unreachable <p> One of the latest issues I 've come across is removing a DHCP Failover relationship . In a previous article I referenced a new Windows Server feature ( Error while adding a second DHCP Server ) which is DHCP Failover . This is a pretty cool feature as it allows you one of two things : <p> Obviously this is a pretty cool feature . The main issue with DHCP servers is that because they do n't  share a common database , a lease assigned by one server can have a conflicting IP address ( because the other DHCP server already assigned it ) , etc . <p> So when does this feature become an issue ? When both servers lose communication . This is something that should rarely happen and probably it is just a temporary problem , but there are a few scenarios when reaching the partner DHCP server might be practically a permanent position . In @ @ @ @ @ @ @ @ @ @ wont be able to unless the partner is reachable to delete the failover scope on that server . This could happen if you take the server to another physical location which is not connected via the network/dhcp or makes it unreachable . In my case , I changed the domain name of our Active Directory which caused a Server name change ( just the dns suffix ) . So , even though I maintained a reference to it in DNS with the old suffix and you could reach it via ping , I was not able to remove this relationship . Obviously , the big issue more than removing the relationship is that because both servers are not communicating you may lose the ability to prevent IP lease conflicts . <p> Unfortunately the workaround is to perform step by step what the wizard would had done and force said actions : <p> Pick the server you want to keep . Look at your DHCP leases and determine which might be the most complete . <p> Stop the partner DHCP server so only one server is responding to DHCP requests <p> @ @ @ @ @ @ @ @ @ @ to the server you want to keep . Renew leases so to prevent potential conflicts . <p> You are going to force the removal of the DHCP failover relationship . Because there is no communication the partner server wont transmit the latest changes and the scope wont be removed from it . Execute the following command in order to remove the relationship : <p> You will get a warning that says : " Failed to delete the failover relationship &lt;Name of the relationship , i.e. SFO-SIN-Failover&gt; on the partner server &lt;Name of partner server , i.e. **26;83;TOOLONG " . As I mentioned this is the expected behavior as you can not communicate with the partner server . <p> Manually delete the scope from the partner server to ensure no conflicts arise if it accidentally becomes active . <p> And that should do the trick . If you wanted you could form again the relationship in case the issue was a problem with the server names or you could form a new relationship with a new server . Either way just make sure you do n't  have 2 DHCP servers assigned @ @ @ @ @ @ @ @ @ @ failover relationship . 
@@45151231 @5151231/ <h> How to : Create a php file to check on the status of a Drupal Server <h> How to : Create a php file to check on the status of a Drupal Server <p> I was working on creating a php status file that would indicate if a server was able to handle client requests or not . The idea is that you can have a loadbalancer check periodically those status.php files to see if that server is online and able to handle traffic or not . In my case I was interested in creating a file for a WordPress which one day I hope I 'll get to it , but while looking around for something I found a good one for Drupal . <p> If you are looking into deploying a Highly Available Drupal server here is the source post that covers how to do so using Varnish as front end proxy servers : - LONG ... <p> I think there is lots of great content on the site but seeing that I already use NginX as a front end proxy server ( I once used Varnish @ @ @ @ @ @ @ @ @ @ stack and decided to stay with NginX as I was already using it as our web server ) what I was really interested on was the status file . Below I am copying that file for anyone who is interested and you can refer to the original site for more information on how to setup a Highly Available Website. 
@@45151232 @5151232/ <h> Resolved : fatal : Not a git repository ( or any of the parent directories ) : . git <h> Resolved : fatal : Not a git repository ( or any of the parent directories ) : . git <p> I just started using BitBucket as it offer private Git repositories which is something I was looking for and GitHub obviously charges for that . I am going to use my repository for something small and simple not a huge project so justifying the monthly fee was rather hard . Anyway , the instructions for adding a repository in BitBucket read like so : 
@@45151235 @5151235/ <p> # See http : **40;111;TOOLONG for how to upgrade to <p> # newer versions of the distribution . <p> deb http : **34;153;TOOLONG saucy main <p> deb-src http : **34;189;TOOLONG saucy main <p> If you want to add an additional repository you can do it via the command line : <p> sudo add-apt-repository ppa : &lt;repository-name&gt; <p> This will result in a new file created under- /etc/apt/sources.list.d/ where the . list files are included as part of the sources.list . This approach allows for enhanced ease when it comes to maintaining multiple ppa sources . 
@@45151238 @5151238/ 55329 @qwx465329 <h> How to : Build your own version of NginX <h> How to : Build your own version of NginX <p> I was following a tutorial on how to use NginX with WordPress and the NginX Fast CGI cache caught my interest . The only problem is that in order to NginX to purge the cached version it needs a custom third party module that does not come out of the box . There is a repository that has it but it is built for Ubuntu 12.0.4TLS and even when I try to deploy it in that environment it fails . I decided I would toy around with building from source for the first time ! <h> II . Dependencies <p> . /configure : error : the HTTP rewrite module requires the PCRE library . You can either disable the module by using " **25;225;TOOLONG option , or install the PCRE library into the system , or build the PCRE library statically from the source with nginx by using " with-pcre=&lt;path&gt; option . <p> apt-get install libpcre3 libpcre3-dev <h> b ) C++ Compiler <p> . /configure : @ @ @ @ @ @ @ @ @ @ OpenSSL library. - You can either disable the module by using " without-http-cache option , or install the OpenSSL library into the system , or build the OpenSSL library statically from the source with nginx by using " with-httpsslmodule " **25;252;TOOLONG options . <p> . /configure : error : SSL modules require the OpenSSL library . You can either do not enable the modules , or install the OpenSSL library into the system , or build the OpenSSL library statically from the source with nginx by using " **25;279;TOOLONG option . <p> sudo apt-get install openssl libssl-dev libperl-dev <h> d ) Xml xslt <p> . /configure : error : the HTTP XSLT module requires the libxml2/libxslt libraries . You can either do not enable the module or install the libraries . <p> apt-get install libxslt-dev <h> e ) GD Library <p> . /configure : error : the HTTP image filter module requires the GD library . You can either do not enable the module or install the libraries . 
@@45151241 @5151241/ <h> How to : Get the Sum of the Values from List in C#.Net ? <h> How to : Get the Sum of the Values from List in C#.Net ? <p> Currently I am working on a new project in which I need to calculate some statistics that involve some basic statistics which require the total amount . The easy way out is just to do a foreach statement and iterate through the list adding the values . But somehow , I wanted something that looked fancier but really cleaner and much easier to maintain and understand . Because of that , I came across the Sum method found in a list . In my case , because I was obtaining all the data from an API and it was returned in a Json it came as a string . So adding the complexity of adding a list of objects string properties . <p> In order to achieve the end result : " Summing all string representations of numbers stored as properties in a list of objects in a clean , easy to manage way " we rely on LINQ : 
@@45151242 @5151242/ 55329 @qwx465329 <h> Tag Archive : National Institute of Standards and Technology <p> What RSA key length should I use for my SSL certificates ? Recently I was working on setting up an SSL certificate for a site and Internet Explorer asked me what key length I wanted to use . Usually providers offer 2048 and 4096 as their standard options so I was tempted to give it a shot <p> How to : Configure your Domain Server to sync the time over the Internet ( Network Time Protocol NTP ) The Windows Time service provides time synchronization to peers and clients , which ensures consistent time throughout an enterprise . I-ve been struggling with getting my Domain to not end up with strange times . The issue at hand is 
@@45151243 @5151243/ <p> Recently I was working with my VMs and moving their storage files from one location to another . I do n't  recall exactly what I did that caused this issue but I was unable to start my VM . I would get every time the following error message : <p> Hyper-V Manager <p> An error occurred while attempting to start the selected virtual machine(s) . <h> The Problem <p> Not sure what originates this issue , but as the error message indicates Hyper-V does not have enough permissions ( /sufficient priviledge ) to open the attached hard drive . If I open a Virtual Machine Disk of a VM that works I can see there is an account on the Security settings tab that looks like virtual machine I 'd . When I go into the VHDX file for the one that is not working I can see it only has the standard accounts that are inherited by the parent folder . Clearly at some point the file permissions were lost , hence it can not be opened . <h> The Solution <p> There are many things you could @ @ @ @ @ @ @ @ @ @ of the day is to get that VM account the rights it needs to open the hard drive . <p> The most tempting solution is to simply go to the security tab and add the account . Set the permissions like those of other VMs ( At the very least read and write permissions . I fear there is a lot of room for error on this approach so it is not my favorite . There is always the possibility you missed a setting and something might not work as intended . This brings me to my second and favorite solution : <p> Use Hyper-V manager to remove the disk from the Virtual Machine and then go back in to add it again . Apparently every time you add a virtual hard disk it will execute the required commands to give that Virtual Machine account the required permissions to the attachments . I say apparently because I cant imagine how this did n't  happen in my case and I ended up getting the error message . Regardless , doing this did solve my issue and it is the Hyper-V manager @ @ @ @ @ @ @ @ @ @ get done correctly . <p> Finally there are many other ways to modify file permissions that you could use . Most importantly , there is always PowerShell . There is a " AddAccessRule " command you could apply to a VirtualMachine.HardDrive that will , of course , add the required access rules so that the VM can access said attached Virtual Hard Drive . Pretty neat right ? <p> There is one said script that you could use found here : - LONG ... I have n't tried it but it looks as it would work . I only had one VM to fix so it is much faster to do the Hyper-V manager operation than trying to download and then validate the script before executing it . But if you want to do this for a number of VMs you could use this to run it against 
@@45151244 @5151244/ 55329 @qwx465329 <h> Category Archive : Microsoft <p> What is http : **36;306;TOOLONG ? The site- http : **37;344;TOOLONG is generally used by Microsofts Online applications . For example , if you have downloaded the Active Directory Sync to Azure application you will see that the client uses the web address http : **36;383;TOOLONG to try to communicate . <p> Why is my Exchange 2013 Server generating a lot of emails from email protected and email protected ? At first I was very worried about this . My servers were sending out some- SPAM- ( forgot to turn on the Sender I 'd so a few of Non delivery emails were being sent out , whoops ) and there had been some port scans detected <p> How to : Resolve Windows- error 0x8004FE33 indicating client activation failure Ive been having a few issues- with my SharePoint 2013 installation which has forced me to dive into the event log- to see what possible errors might be occurring . One of those was error 0x8004FE33 . It turns out this had nothing to do with my problems , it was <p> Sharepoint @ @ @ @ @ @ @ @ @ @ counter DLL- ASP.NET2.0.50727 in a 64-bit- environment . Contact the file vendor to obtain- a 64-bit version . Alternatively , you can open the 32-bit- extensible counter DLL by using the 32-bit version of Performance Monitor . To use this tool , open the Windows folder , open the Syswow64 folder , and then start Perfmon.exe . I am <p> How to : Publish Exchange Server 2010 with Forefront UAG and Forefront TMG I 've been trying to publish Exchange Server 2013 with Forefront TMG with no avail . However , I did find a good guide on how to publish Exchange Server 2010 with TMG so I thought I would share . I cant find the link to the <p> What is : fsdmhost.exe Fsdmhost.exe is the executable file for the Microsoft- File- Server Data Management Host process . Thus far a lot of people run into this process on a Windows 2012 server because it is in charge of data de-duplication . As most of you would have found out , this process can become resource intensive . The service uses <p> What is : urs.microsoft.com The URL http @ @ @ @ @ @ @ @ @ @ monitoring/protection device quite often as it is the URL used by the Microsoft SmartScreen Filter Services . Generally when you see a Microsoft URL you know some part of the Windows- system is trying to communicate with some sort of Microsoft web service , but <p> What is : **27;421;TOOLONG ? As of late I have been paying- closer attention to my firewall logs in an effort to- increase security but most importantly reduce the lag time we sometimes face when accessing the Internet . Having efficient rules allow for traffic to move more quickly through the firewall and in this case , examining denied connections reveal <p> How to : Use the Certificate Enrollment MMC in the TMG host machine Behavior : When you are using the Certificate MMC snap-in and/or try to perform- a certificate auto-enrollment in your localhost/TMG server you 'll most likely run into an error message- on-screen- that reads " RPC- failure " . If you try requesting a certificate on other computers joined to your <p> How to : Move Mailboxes to another Exchange Server 2013 So again , another fun day thanks to the @ @ @ @ @ @ @ @ @ @ debating on that one ) . For some reason my CAS server- stopped working properly and after spending a whole day trying to figure out why I ca n't log back in I 
@@45151245 @5151245/ <h> Resolved : The " **37;450;TOOLONG " task could not be loaded from the assembly <h> Resolved : The " **37;489;TOOLONG " task could not be loaded from the assembly <p> I recently have been working on an ASP.Net application and at some point I started to get build errors that although they were not stopping the build and allowed me to continue with the applications execution , they were starting to get quite annoying . <p> This is a sample of the error message that tormented me for ages but because it was n't a show stopper I left it there for almost a week : <p> The " **37;528;TOOLONG " task could not be loaded from the assembly LONG ... Could not load file or assembly LONG ... or one of its dependencies . The system can not find the file specified . Confirm that the declaration is correct , that the assembly and all its dependencies are available , and that the task contains a public class that implements **31;567;TOOLONG . <p> Thankfully the solution was a simple one . If you take a look at the @ @ @ @ @ @ @ @ @ @ - It turns out that NuGet packages were committed to the repository and are breaking everything . To resolve this , you simply need to delete that packages folder ( usually under &lt;project name&gt; &lt;project name&gt; packages . ) I am afraid of deleting anything , so I went ahead and simply renamed the folder packages.broken and after that proceeded to build all again . The nice thing is that NuGet fetches the packages automatically on build so the folder got repopulated , nothing broke and my error was gone ! After that I proceeded to delete the packages.broken folder and move on with my life . 
@@45151246 @5151246/ <h> DFSR : How to properly Size the Staging Folder and Conflict and Deleted Folder <h> DFSR : How to properly Size the Staging Folder and Conflict and Deleted Folder <p> The Distributed File System Replication ( DFSR ) service is a new multi-master replication engine that is used to keep folders synchronized on multiple servers . <p> Replicating data to multiple servers increases data availability and gives users in remote sites fast , reliable access to files . DFSR uses a new compression algorithm called Remote Differential Compression ( RDC ) . RDC is a " diff over the wire " protocol that can be used to efficiently update files over a limited-bandwidth network . RDC detects insertions , removals , and rearrangements of data in files , enabling DFSR to replicate only the deltas ( changes ) when files are updated . <p> DFSR offers a lot of benefits from high data availability in local networks to faster file access to remote ones . One important thing to consider though is how to properly size the staging folder . <p> Optimizing the size of the staging folder @ @ @ @ @ @ @ @ @ @ . If a staging folder quota is configured to be too small , DFS Replication might consume additional CPU and disk resources to regenerate the staged files . Replication might also slow down , or even stop , because the lack of staging space can effectively limit the number of concurrent transfers with partners . <p> Microsoft recommends you size the staging folder to the size of the 32 largest files you have in your shared folder . In the case of read-only mesh then you only need the space of the 16 largest files . This becomes tricky as you can imagine . In my case for example I store . IMG files which can size about 4gbs for installation purposes . Multiply 4 x 32 and you 've got yourself a quite large staging size ( 128 gb ) . It is important to understand that staging during initial replication would benefit of the 128gb but on a day to day operation it all boils down on how active changes happen . If most of my files are word files and changes at most do n't  account for 1gb @ @ @ @ @ @ @ @ @ @ . Another approach you can follow is to initially assign the calculated staging folder size and then resize as you see fit . This can be done by observing the staging folder size and the event logs . <p> During normal operation , if the event that indicates the staging quota ( event I 'd 4208 in the DFS Replication event log ) is over its configured size and is logged multiple times in an hour , increase the staging quota by 20 percent . Remember this only applies during normal operation , not while you are doing an initial replica to a server . There is also a low watermark and high watermark messages you can use to gauge how close you are getting : <p> The DFS Replication service has detected that the staging space in use for the replicated folder at local path U : is above the high watermark . The service will attempt to delete the oldest staging files . Performance may be affected . <p> Ideally you want to avoid error 4208 at all costs but warnings such as the one above are acceptable . @ @ @ @ @ @ @ @ @ @ normal operation you need to observe them carefully . If you are getting them you risk that if you lose network connectivity you will start getting error 4208 . If you are constantly below the Low Watermark you might have oversized your staging folder . I know this sounds like too much trouble but proper sizing is going to provide you with the best performance . If you size this horribly wrong not only is performance going to be awful but you will risk DFSR not doing its job properly . I recommend over-sizing it over under-sizing it if it comes down to those two . <p> Below is a PowerShell commandlet you can execute to determine the size of your largest files : 
@@45151247 @5151247/ 55329 @qwx465329 <h> Category Archive : WordPress <p> How to : Redirect users to a given site when the visited sub-domain does not exist in a WordPress Multisite deployment Now , this is a tricky one and I say that because I searched and could n't find the answer and I know it is out there ! Perhaps I need to word my search better . Anyway , for <p> How to : Install Zend OpCache for php and WordPress OpCache is a mayor tool to increase the performance of your site . Every time a WordPress page is visited the system needs to compile the php code in order to render the page . OpCache allows you to store those compiled bits in memory and serve them <p> How to : Install Memcache for php and WordPress In what seems the eternal quest of having a good performing hosting server one of the tools that are generally referred to is Memcache . What Memcache offers you is a centralized server/cluster that holds in memory cached information . Some of the reasons why it is so popular <p> How to @ @ @ @ @ @ @ @ @ @ by W3 Total Cache Honestly the problem statement is more like : " How to prevent W3 Total Cache from serving the mobile theme to all my clients " . The issue arises when the first client that visits a page is a mobile client . W3 Total Cache <p> Changing file permissions and recommendations for WordPress in a Linux system In order to harden your WordPress installation , having the recommended set of file permissions is important . Below are a few recommendations on that gathered from around the web . First , we have a brief summary of the recommendations : # reset to safe defaults find <p> WordPress MultiSite : Remove render-blocking JavaScript : script LONG ... type=text/javascript So this is the latest in the " I want to improve my Google-s PageSpeed " saga . I was running Googles PageSpeed to determine what fixes I need to implement on my site to get a higher ranking and hopefully improve the rank it gives to my site . One <p> Importing WordPress sites : Why wont the Import tool finish ? Lately I have been consolidating WordPress @ @ @ @ @ @ @ @ @ @ but one of the issues I 've come across is that the Export / Import tool wont completely import an entire site . Sure , as a site grows the work of moving <p> How to : Connect to a MySql server using SSL from a WordPress Site Truly you could title this post simply How to Connect to a MySql server using SSL from a PHP application . The key is in the connection construct , which by default in WordPress it looks sort of like this : $this-&gt;dbh = mysqlconnect ( $this-&gt;dbhost , <p> What does Error 330 ( net : : ERRCONTENTDECODINGFAILED ) in Google Chrome means Lately I have been playing around with WP caching , web compression and the sort . As I was toying with some of the settings I realized my site was throwing error 330 when accessing it from Google Chrome ( IE meanwhile just said the site was down ) . It 
@@45151248 @5151248/ 55329 @qwx465329 <h> Category Archive : Web <p> Resolve : Nginx 413 Request Entity Too Large When working with WordPress you might come across a message that indicates HTTP Error when uploading any media to your site . Obviously there are tons of things that could cause this error , like the memory you have allocated to PHP , or even some NginX configuration might cause <p> How to : Configure NAXSI with NginX As part of a series of articles regarding NginX , today I will cover how to configure NAXSI with NginX . Currently there is a distribution of NginX that comes with NAXSI ( you can read about some of the standard distributions here : LONG ... which you can find with the ngxinx dev <p> Resolve : Upstream sent too big header while reading response header from upstream when using NginX PHP FPM When working with NginX you will be using an upstream PHP FPM server . Because of the way NginX works it will treat this upstream server as a proxy server which you will use to server PHP requests . One <p> How @ @ @ @ @ @ @ @ @ @ a custom Cache log One of the great features of NginX is its ability to cache content from a number of sources including but not limited to a proxy server or a fast cgi server . But in order to understand truly how well your cache <p> Changes with NginX 1.4.4 Below are the list of changes from the initial release of NginX to stable version 1.4.4 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.4 19 Nov 2013 * ) Security : a character following an unescaped space in a request line was handled incorrectly ( CVE-2013-4547 ) ; the <p> How to : Redirect an URL in NGINX One might need to redirect to another URL in a number of scenarios . I will cover two examples for scenarios I have come across and how I addresses those issues : I. Redirect from a naked domain to the www subdomain : server listen 80 ; servername test.com ; return 301 <p> Custom NginX Distribution Available Packages As part of the custom NginX distribution available on this site @ @ @ @ @ @ @ @ @ @ depending on your needs that can be deployed . Below is the list of packages , additional information and description : Different packages available : Package : - nginx Architecture : all Depends : nginx-full nginx-light , This <p> Changes with- NginX- 1.5.8 Below are the list of changes from the 1.5.7 release of NginX to version 1.5.8 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.8 17 Dec 2013 * ) Feature : IPv6 support in resolver. * ) Feature : the " listen " directive supports the " fastopen " parameter . Thanks to Mathew Rodley. * ) <p> Changes with NginX 1.5.7 Below are the list of changes from the 1.5.6 release of NginX to version 1.5.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.7 19 Nov 2013 * ) Security : a character following an unescaped space in a request line was handled incorrectly ( CVE-2013-4547 ) ; the bug <p> Changes with NginX 1.5.6 Below are the list of changes from the @ @ @ @ @ @ @ @ @ @ is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.6 01 Oct 2013 * ) Feature : the " fastcgibuffering " directive . * ) Feature : the " proxysslprotocols " and " proxysslciphers " directives . Thanks to Piotr Sikora. * ) Feature : 
@@45151252 @5151252/ <p> To comment on this content or ask questions about the information presented here , please use our- Feedback guidance . <p> This topic provides online help content for cloud services , such as Windows Intune and Office 365 , which rely on Windows Azure Active Directory for identity and directory services . <p> The Windows Azure Active Directory Module for Windows PowerShell cmdlets were previously known as the Microsoft Online Services Module for Windows PowerShell cmdlets . <p> As an administrator , you can use the Windows Azure Active Directory Module for Windows PowerShell cmdlets to accomplish many Windows Azure AD tenant-based administrative tasks such as user management , domain management and for configuring single sign-on . This topic includes information about how to install these cmdlets for use with your tenant . <p> Install Updates : Ensure you have installed all of the required updates required by the Microsoft cloud services to which you have subscribed . For example , some cloud service features may not work properly without the appropriate versions of operating systems , web browsers , and software . <p> If you are using @ @ @ @ @ @ @ @ @ @ at least Windows PowerShell 2.0 , and you must have administrator privileges on the AD FS server . Using remote access to the AD FS server is recommended when you run these cmdlets . To do so , you can use Windows PowerShell remoting . For information , see- AboutRemoteRequirements . <h> Connect to Windows Azure AD <p> Click the- Windows Azure Active Directory Module for Windows PowerShell- shortcut to open a Windows PowerShell workspace that has the cmdlets . Alternatively , you can load the cmdlets manually by typing- import-module MSOnline- at the Windows PowerShell command prompt . <p> Before you can run any of the cmdlets discussed in this article , you must first connect to your online service . To do so , run the cmdlet- connect-msolservice- at the Windows PowerShell command prompt . You will then be prompted for your credentials . If you want , you can supply your credentials in advance , for example : <p> $msolcred = get-credential connect-msolservice -credential $msolcred <p> The first command prompts for credentials and stores them as $msolcred . The next command uses those credentials as $msolcred to @ @ @ @ @ @ @ @ @ @ the Windows Azure Active Directory Module for Windows PowerShell cmdlets , you can do the following : <p> To create a folder for help , list the cmdlets , and then open the file in notepad , you can run the following commands at the Windows PowerShell command prompt : <p> View the examples for a cmdlet , run the following command at the Windows PowerShell command prompt : - get-help &lt;cmdlet-name&gt; -examples <p> View the name , synopsis , description , parameter descriptions , and any examples provided for a cmdlet , run the following command at the Windows PowerShell command prompt : - get-help &lt;cmdlet-name&gt; -detailed <p> View the name , synopsis , description , detailed parameters , and any examples provided for a cmdlet , run the following command at the Windows PowerShell command prompt : - get-help &lt;cmdlet-name&gt; -full <h> Manage users <p> The **25;600;TOOLONG cmdlet is used to update a user in a domain that was recently converted from single sign-on ( also known as identity federation ) to standard authentication type . A new password must be provided for the user . <p> The @ @ @ @ @ @ @ @ @ @ Note that this cmdlet should be used for basic properties only . The licenses , password , and User Principal Name for a user can be updated through Set-MsolUserLicense , Set-MsolUserPassword and **25;627;TOOLONG cmdlets respectively . <p> The **25;654;TOOLONG cmdlet is used to change the User Principal Name ( Template Token Value ) of a user . This cmdlet can be used to move a user between a federated and standard domain , which will result in their authentication type changing to that of the target domain . <p> The Redo-MsolProvisionUser cmdlet can be used to retry the provisioning of a user object in Windows Azure Active Directory when a previous attempt to create the user object resulted in a validation error . <p> The Get-MsolUserRole cmdlet is used to retrieve all of the administrator roles that the specified user belongs to . This cmdlet will also return roles that the user is a member of through security group membership . <p> The Redo-MsolProvisionGroup cmdlet can be used to retry the provisioning of a group object in Windows Azure Active Directory when a previous attempt to create the group object @ @ @ @ @ @ @ @ @ @ <p> The Set-MsolServicePrincipal cmdlet updates a service principal in Windows Azure AD . It can be used to update the display name , enable/disable the service principal , trusted for delegation , the service principal names ( SPNs ) or the addresses . <p> The New-MsolServicePrincipal cmdlet creates a service principal that can be used to represent a Line Of Business ( LOB ) application or an on-premises server such as Microsoft Exchange , SharePoint or Lync in Windows Azure AD as " service principal " objects . Adding a new application as a service principal allows that application to authenticate to other Microsoft Online Services . <p> The **34;681;TOOLONG cmdlet can be used to add a new credential to a service principal or to add or roll credential keys for an application . The service principal is identified by supplying either the object I 'd , application I 'd , or service principal name ( SPN ) . <p> The **37;717;TOOLONG cmdlet can be used to remove a credential key from a service principal in the case of a compromise or as part of credential key rollover expiration . The service @ @ @ @ @ @ @ @ @ @ application I 'd , or service principal name ( SPN ) . The credential to be removed is identified by its key I 'd . <h> Manage domains <p> The Confirm-MsolDomain cmdlet is used to confirm ownership of a domain . In order to confirm ownership , a custom TXT DNS record must be added for the domain . The domain must first be added using the Add-MsolDomain cmdlet , and then the **29;756;TOOLONG cmdlet should be called to retrieve the details of the DNS record that must be set.Note that there may be a delay ( 15 to 60 minutes ) between when the DNS update is made and when the cmdlet is able to confirm ownership of a domain . <p> The New-MsolDomain cmdlet is used to create a new domain object . This cmdlet can be used to create a domain with managed or federated identities , although the New-MsolFederatedDomain cmdlet should be used for federated domains in order to ensure proper setup . <p> The Set-MsolDomain cmdlet is used to update settings for a domain . Using this cmdlet , the default domain can be changed , or @ @ @ @ @ @ @ @ @ @ be changed . <p> The **28;815;TOOLONG cmdlet is used to change the domain authentication between standard identity and single sign-on . This cmdlet will only update the settings in Windows Azure AD ; typically the **28;845;TOOLONG or **29;875;TOOLONG should be used instead . <p> The New-MsolFederatedDomain cmdlet adds a new single sign-on domain ( also known as identity-federated domain ) to Windows Azure AD and configures the relying party trust settings between the on-premises Active Directory Federation Services 2.0 server and Windows Azure AD . Due to domain verification requirements , you may need to run this cmdlet several times in order to complete the process of adding the new single sign-on domain . <p> The **28;906;TOOLONG cmdlet converts the specified domain from single sign-on ( also known as identity federation ) to standard authentication . This process also removes the relying party trust settings in the Active Directory Federation Services 2.0 server and Windows Azure AD . After the conversion , this cmdlet will convert all existing users from single sign-on to standard authentication . Any existing user who was configured for single sign-on will be given a new @ @ @ @ @ @ @ @ @ @ converted user name and new temporary password will be recorded in a file for reference by the administrator . The administrator can then distribute the new temporary password to each converted user to enable the user to sign in to the cloud service . <p> The **29;936;TOOLONG cmdlet converts the specified domain from standard authentication to single sign-on ( also known as identity federation ) , including configuring the relying party trust settings between the Active Directory Federation Services 2.0 server and Windows Azure AD . As part of converting a domain from standard authentication to single sign-on , each user must also be converted . This conversion happens automatically the next time a user signs in ; no action is required by the administrator . <p> The **26;967;TOOLONG cmdlet removes the specified single sign-on domain from Windows Azure AD and the associated relying party trust settings in Active Directory Federation Services 2.0 . Note : If the domain specified has objects associated with it , you will not be able to remove the domain . <p> The Set-MsolADFSContext cmdlet sets the credentials to connect to Windows Azure AD and @ @ @ @ @ @ @ @ @ @ 2.0 ) server . This cmdlet must be run before making other single sign-on ( also known as identity federation ) cmdlet calls . If this cmdlet is called without parameters , the user will be prompted for credentials to connect to the different systems . When the AD FS 2.0 server is used remotely , the user must specify the computer name of the primary AD FS 2.0 server . Note that the specified logfile is shared by all single sign-on cmdlets for the session . A default logfile is created if one is not specified . <p> The **26;995;TOOLONG cmdlet changes settings in both the Active Directory Federation Services 2.0 server and Windows Azure AD . It is necessary to run this cmdlet whenever the URLs or certificate information within Active Directory Federation Services 2.0 change due to configuration changes or through regular maintenance of the certificates , such as when a certificate is about to expire . This cmdlet should also be run when changes occur in Windows Azure AD . To confirm that the information in the two systems is correct , the **26;1023;TOOLONG cmdlet can @ @ @ @ @ @ @ @ @ @ cmdlet creates a new License Options object . This cmdlet disables specific service plans when assigning a user a license using the Add-MsolUser and Set-MsolUserLicense cmdlets . <p> The Set-MsolUserLicense cmdlet can be used to adjust the licenses for a user . This can include adding a new license , removing a license , updating the license options , or any combination of these actions . <h> Manage company information and service <p> Use the following cmdlets to perform tasks related to managing your company 's information and connecting to a Microsoft cloud service . There are also cmdlets for tasks performed by partner companies . <p> The Connect-MsolService cmdlet will attempt to initiate a connection to Windows Azure AD . The caller must either provide their credential ( a PSCredential object ) , or use the UseCurrentCredential option if the current logged in user is federated with Windows Azure AD . This cmdlet may return a warning or error if the version of the module being used is out of date . <p> The Get-MsolPartnerContract cmdlet should only be used by partners , as it is used to retrieve @ @ @ @ @ @ @ @ @ @ to this cmdlet should be a domain to look up , which must be verified for the tenant . If the company exists and the partner has access to this company , then the corresponding contract will be returned . <p> The **25;1051;TOOLONG cmdlet can be used to retry the provisioning of a contact object in Windows Azure Active Directory when a previous attempt to create the contact object resulted in a validation error . <p> The **33;1078;TOOLONG cmdlet is used to set company-level contact preferences . This includes email addresses for billing , marketing , and technical notifications about the cloud service . <h> More about Windows PowerShell <p> Windows PowerShell is a task-based command-line shell and scripting language designed for system administration . Unlike most shells , which accept and return text , Windows PowerShell is built on top of the Microsoft . NET Framework common language runtime ( CLR ) and the . NET Framework , and accepts and returns . NET Framework objects . Windows PowerShell introduces the concept of a cmdlet ( pronounced " command-let " ) , a simple , single-function command-line tool built @ @ @ @ @ @ @ @ @ @ : a verb and noun separated by a dash ( - ) , such as Get-Help , Get-Process , and Start-Service . Windows PowerShell includes more than one hundred basic core cmdlets . For more information about Windows PowerShell , see the- Windows PowerShell Getting Started Guide . 
@@45151253 @5151253/ 55329 @qwx465329 <h> Category Archive : Servers <p> How can I verify if I can connect to my email server ? Im having issues sending email using an SMTP server and I want to know if it is because of my code or due to an external issue . What is an alternative to verify connection settings to an SMTP Host ? <p> Why would the following error message occur ? : " Communication with the underlying transaction manager has failed . " I also got the following inner exception in case this helps : " The MSDTC transaction manager was unable to pull the transaction from the source transaction manager due to communication problems . Possible causes are : a firewall is present and it does n't  
@@45151255 @5151255/ 55329 @qwx465329 <h> Category Archive : NginX <p> Resolved : 400 Bad Request Request header or Cookie too large when using NginX One of the issues I ran into when using NginX was : " 400 Bad Request Request header or Cookie too large . " This was strange to me as my site had been working properly and this behavior was only experienced when doing <p> What is : NginX Lately I have been writing a lot about NginX so I thought I would write a small post to describe NginX and what it is.Although I like to write it NginX because its origins are in the Linux world they write it all in lower case nginx ( I know , boring right ) . The <p> How to : Install NginX If you want to Install NginX , we currently host a PPA that tries to keep up with the latest updates from NginX . Sometimes we make changes to the build when something breaks or does n't  work but most of the time we use the same package that is provided by the NginX <p> Changes @ @ @ @ @ @ @ @ @ @ the 1.4.4 release of NginX to stable version 1.4.5 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.5 11 Feb 2014 * ) Bugfix : the $sslsessionid variable contained full session serialized instead of just a session i 'd . Thanks to <p> Changes with- NginX- 1.5.10 Below are the list of changes from the 1.5.9 release of NginX to version 1.5.10 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.10 04 Feb 2014 * ) Feature : the ngxhttpspdymodule now uses SPDY 3.1 protocol . Thanks to Automattic and MaxCDN for sponsoring this work . * ) Feature : <p> Changes with- NginX- 1.5.9 Below are the list of changes from the 1.5.8 release of NginX to version 1.5.9 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.9 22 Jan 2014 * ) Change : now nginx expects escaped URIs in " X-Accel-Redirect " headers. * ) Feature : the " sslbuffersize " directive . * ) @ @ @ @ @ @ @ @ @ @ : Reload your NginX Configuration Sometimes when working with NginX you come across the need to reload your configuration for the changes made to your . conf files take place . Although this is a simple task some considerations and best practices should be followed for optimal results : Verify that your configuration files " compile " . Execute nginx <p> Resolve : Nginx 413 Request Entity Too Large When working with WordPress you might come across a message that indicates HTTP Error when uploading any media to your site . Obviously there are tons of things that could cause this error , like the memory you have allocated to PHP , or even some NginX configuration might cause <p> How to : Configure NAXSI with NginX As part of a series of articles regarding NginX , today I will cover how to configure NAXSI with NginX . Currently there is a distribution of NginX that comes with NAXSI ( you can read about some of the standard distributions here : LONG ... which you can find with the ngxinx dev <p> Resolve : Upstream sent too big header while @ @ @ @ @ @ @ @ @ @ When working with NginX you will be using an upstream PHP FPM server . Because of the way NginX works it will treat this upstream server as a proxy server which you will use to server PHP requests . One 
@@45151256 @5151256/ 55329 @qwx465329 <h> Category Archive : Technologies <p> How to : Install the Memcached Agent for NewRelic in an Ubuntu server Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . For example , take this instructions on how to install the Memcached agent to get monitoring on your web server : <p> How to : Install Ruby in an Ubuntu Server to use it with NewRelics Plugins : Memcached &amp; NginX Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . Also , most of them require other applications to be installed as well like Ruby <p> How to : Use NginX as a Forward Proxy server Generally people use NginX as a Reverse Proxy server , which is what it was designed for . However , after working with NginX for sometime now , I realized conceptually a proxy server could work both ways , right ? The thought is an interesting one but its market might @ @ @ @ @ @ @ @ @ @ via Socket As I keep looking into how to improve the performance of website one of the recurrent points mentioned is to use Linux sockets where possible . I really do n't  have much experience and I can see how avoiding the TCP stack might help but I figured at <p> How to : Move the header widget down in Graphene to place your Ads closer to the post If you 've visited our site before you might remember how our Google Add for the header was all the way up overlapping the good old banner . If not then you are like most users ; Advertising that is not <p> How to : Install the NginX Agent for NewRelic in an Ubuntu server Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . For example , take this instructions on how to install the NginX agent to get monitoring on your web server : <p> Resolved : 400 Bad Request Request header or Cookie too large when using NginX One of the issues I ran @ @ @ @ @ @ @ @ @ @ Request header or Cookie too large . " This was strange to me as my site had been working properly and this behavior was only experienced when doing <p> What is : NginX Lately I have been writing a lot about NginX so I thought I would write a small post to describe NginX and what it is.Although I like to write it NginX because its origins are in the Linux world they write it all in lower case nginx ( I know , boring right ) . The <p> Quick Fix : WordPress SEO by Yoast sitemapindex.xml giving blank page After the latest somewhat recent update made to Yoasts WordPress SEO I have had issues with my sitemap . If I do a view source on the blank page that shows up now when I open my sitemap I can see the XML and it is <p> How to : Install NginX If you want to Install NginX , we currently host a PPA that tries to keep up with the latest updates from NginX . Sometimes we make changes to the build when something breaks or does n't  @ @ @ @ @ @ @ @ @ @ package that is provided by the NginX 
@@45151257 @5151257/ <h> How to : Uninstall an assembly ( dll ) from the GAC <p> So , you probably have already tried going to the GAC and selecting uninstall , if not , below are the instructions : <p> Navigate to the GAC , which is located at **28;1113;TOOLONG . <p> Right-click on the assembly you wish to uninstall , click- Uninstall , and then click- Yes- to confirm . <p> In my case I kept wondering why I cant possibly follow those simple instructions . The reason is that you have to be able to write to the directory ( i.e. have permissions to write to the directory ) which you would think as an administrator you would , but it turns out you have to be a local administrator to do that . Anyway , so what should the people who do n't  want to bother doing that can do ? Well , you can run a command line to uninstall Dlls pretty easily , just remember to execute it using elevated permissions : <p> Open a Visual Studio command prompt and do n't  forget to right click @ @ @ @ @ @ @ @ @ @ 20xx- Command Prompt. * <p> At the command prompt , type the following command : <p> gacutil /u- &lt;fully qualified- assembly name&gt; <p> In this command , assembly name is the name of the assembly to uninstall from the GAC. 
@@45151259 @5151259/ <p> One of the issues I ran into when using NginX was : " 400 Bad Request Request header or Cookie too large . " This was strange to me as my site had been working properly and this behavior was only experienced when doing a migration from a large site to our new deployment . As this indicates either the Request Header or the Cookie being sent across was too large for NginX to handle well , larger than what it is willing to handle . As you might know , NginX allows you to configure the sizes of a number of buffers for different parts of the communication . The answer though its simpler than understanding the cause ( or properly sizing the buffer ) . <p> The answer lies in using the **25;1143;TOOLONG directive . Its only valid inside the- http- or- server- contexts so be sure to place it where it makes the most sense for your application . <h> largeclientheaderbuffers <p> Directive assigns the maximum number and size of buffers for large headers to read from client request . <p> The request line can @ @ @ @ @ @ @ @ @ @ if the client send a bigger header nginx returns error " Request URI too large " ( 414 ) . <p> The longest header line of request also must be not more than the size of one buffer , otherwise the client get the error " Bad request " ( 400 ) . <p> Buffers are separated only as needed . <p> By default the size of one buffer is 8192 bytes . In the old nginx , this is equal to the size of page , depending on platform this either 4K or 8K , if at the end of working request connection converts to state keep-alive , then these buffers are freed . <p> What generally happens is that all the cookies used by your site get combined into one header and that may cause you to go over the default limit which is 8192 bytes . <p> This is a tricky error to catch as it only affects people who have cookies over the allotted capacity . Some of your users might experience issues when their cookie size exceeds 8k or like in my case , some @ @ @ @ @ @ @ @ @ @ the limit . In the first scenario once the user has cookies that are over the limit they wont be able to use the site any more while other users might access the same pages with no problem while their cookies are under the limit . In the second scenario only certain pages under certain conditions might cause the cookie size to go over the limit . In my case when I was exporting and then importing a site I got this error . It did not happen for those sites which had few posts but for a large site I ended up encountering this error . Under the untrained eye this seems like a random issue ; At least you get the " 400 Bad Request Request header or Cookie too large " hint instead of a bogus error message . <p> In this case the solution is to increase the buffer size not the number of buffers . As the wiki indicates " The request line can not be bigger than the size of one buffer " which by default is 8k at the time of writing this post . 
@@45151261 @5151261/ <h> W Dallas Victory Hotel <p> Summary : Not impressed . As a fan of the W brand , the W Dallas Victory Hotel is one of my least favorites ( I 'll put it out there with the W Atlanta in Buckhead ) . The pool is up around the 15th floor which is cool , and the gym is spacious but old . The service is not good though , they take forever to answer the phone and for a while this lady would pick up and she sounded very uninterested in whatever my request was . I also had an unfortunate experience while staying there which prompted me to change hotels to The Joule which I find it to be a better hotel . Craft has a restaurant in the hotel and my experiences have been mixed . The Craft in Atlanta was much better . My last experience in the restaurant included slow service and lackluster overpriced food ( my first experiences were quite good and I 'm still a huge fan of their french toast ) . <p> SPG upgrades : Asked repeatedly for an upgrade @ @ @ @ @ @ @ @ @ @ request an upgrade on my behalf with no luck ( it was the last week of one of our team members and we wanted a large room to gather and hang out ) . I also had a bad experience which required a room change and even after that they would n't try to accommodate me in a better room . Very disappointed . 
@@45151262 @5151262/ <h> Travel <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <p> Jiuzhai Valley National Park , China Photo by : Richard Janecki Blue and green lakes as well as waterfalls dot Jiuzhai Valley National Park in southwestern China north of Chengdu . The area was declared a UNESCO World Heritage Site in 1992 , in part for the natural beauty and in part for its endangered plant and animal species . <p> Las Pozas , Xilitla , Mexico World Monuments Fund Las Pozas , which means " the pools " in Spanish , is a collection of surrealist structures created by English aristocrat Edward James . Born into wealth , James left his English mansion to create a fantasyland amid central Mexicos jungle . The 20 acres also include a staircase to nowhere . <p> United Airlines : Update Priority This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it @ @ @ @ @ @ @ @ @ @ a <p> The Q1 promotion for Hilton " More Points " has been announced and will continue through 3/31/12 . Details are provided below : Promotion period : 1/1/12-3/31/12 ; eligible nights are within these dates only ( ie , a stay 3/30-4/1 would only earn bonus points for 3/30-31 ) You must register prior to your hotel stays to be eligible Most Hilton <p> LONG ... Attached is the PDF containing the list of international phone numbers you can use to call the 1k United line. - intlcallsupport1Konly <p> Rates that do not qualify are those that have been discounted , including free night stays , third-parties ( ex : Expedia ) , airline interrupted-trip vouchers , or contracted block rooms ( ex : discounted rate for a wedding party ) . <p> If you are in Mexico or you are thinking about traveling there and fly within the country Volaris is a very reasonable alternative to other major airlines and is offering a 20% off promotion during the month of September if you answer a couple of questions correctly . - Use code : 20VIAJA <p> Q : @ @ @ @ @ @ @ @ @ @ least 500 miles credited to your account ( specially when you fly very small distances ) . Is it true many Airlines are changing their policies so that now you 'll accrue miles based on actual milage ? 
@@45151264 @5151264/ <h> Entity Framework Why cant I see the child entities of my entity <p> So currently I 'm working on an entity model in which there are a lot of parent child relationships and for obvious reasons the entity framework is a great choice to build that entity model . However , I 've ran into the issue that although in the database several of this relationships already exist , when trying to access the child entities via the property at the parent level I get an empty list . After doing some research I found out that this is due to the fact that there are several loading mechanisms that are available as part of the Entity Framework . <p> In order to resolve this I used therefore the forced loading command : . Load() at the child collection entity . Please remember that you 'll need your context to be active in order to perform this action . If you have lost your context just create a new one , find the entity you currently have through a query using your new context and then use the . Load() method @ @ @ @ @ @ @ @ @ @ the loading mechanisms please take a look at the table below obtained from the referenced article : <h> Referenced Articles : <p> You can compose an Entity SQL or LINQ to Entities query that explicitly navigates relationships by using navigation properties . When you execute such a query , the related entities- that are included as navigation properties in the outmost projection of the query are returned . For more information , seeHow to : Navigate Relationships Using Navigation Properties . <p> Explicit loading <p> Explicitly loading entities into the- ObjectContext requires multiple round-trips to the database and might require multiple active result sets , but the amount of data returned is limited to only the entities being loaded . Use the- Load method on an- EntityCollection or- EntityReferenceor the- LoadProperty method on the- ObjectContext to explicitly retrieve the related entities from the data source . Each call to the- Loadmethod opens a connection to the database to retrieve the related information . This ensures that a query is never executed without an explicit request for the related entity . Explicit loading is the default behavior of the Entity Framework @ @ @ @ @ @ @ @ @ @ a small amount of information about the related entity is already loaded into the- ObjectContext . <p> For more information , see the- Explicitly Loading Related Objects section of this topic . <p> Lazy loading <p> In this type of loading , related entities are automatically loaded from the data source when you access a navigation property . With this type of loading , be aware that each navigation property that you access results in a separate query executing against the data source if the entity is not already in the- ObjectContext.For more information , see the- Lazy Loading section of this topic . <p> Eager loadingor <p> Defining Query Paths with Include <p> When you know the exact shape of the graph of related entities that your application requires , you can use the- Include method on theObjectQueryto define a query path that controls which related entities to return as part of the initial query . When you define a query path , only a single request against the database is required to return all entities defined by the path in a single result set , and all related @ @ @ @ @ @ @ @ @ @ with each object that the query returns.For more information , see the- Defining a Query Path to Shape Query Results section of this topic . 
@@45151266 @5151266/ <h> Travel <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <p> Jiuzhai Valley National Park , China Photo by : Richard Janecki Blue and green lakes as well as waterfalls dot Jiuzhai Valley National Park in southwestern China north of Chengdu . The area was declared a UNESCO World Heritage Site in 1992 , in part for the natural beauty and in part for its endangered plant and animal species . <p> Las Pozas , Xilitla , Mexico World Monuments Fund Las Pozas , which means " the pools " in Spanish , is a collection of surrealist structures created by English aristocrat Edward James . Born into wealth , James left his English mansion to create a fantasyland amid central Mexicos jungle . The 20 acres also include a staircase to nowhere . <p> United Airlines : Update Priority This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it @ @ @ @ @ @ @ @ @ @ a <p> The Q1 promotion for Hilton " More Points " has been announced and will continue through 3/31/12 . Details are provided below : Promotion period : 1/1/12-3/31/12 ; eligible nights are within these dates only ( ie , a stay 3/30-4/1 would only earn bonus points for 3/30-31 ) You must register prior to your hotel stays to be eligible Most Hilton <p> LONG ... Attached is the PDF containing the list of international phone numbers you can use to call the 1k United line. - intlcallsupport1Konly <h> The Joule , Dallas <p> Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to 11 PM ) , small pool and decent gym . Overall my favorite hotel in Dallas , Tx . <p> SPG upgrades : Got a Joule suite on my first stay . I was welcomed with a letter signed by the hotel staff ( I assume ) and even @ @ @ @ @ @ @ @ @ @ took a look around I was also recognized by the staff and warmly welcomed . <h> Cane Rosso <p> Summary : One of the best pizza place Ive been in . Great variety and different from the ordinary , great quality of ingredients and superior taste . It is not expensive or cheap but their rating makes it a great value for the price. - <p> GET AWAY WITH DOUBLE THE MILESEarn double bonus miles for a limited time on your next flight . Right now is a great time to rack up bonus miles. - Register by November- 1st , then book your ticket and complete your travel between September 15 and November 15 , 2011 , to earn your double flown bonus miles on select flights- to Central America , South America and the Caribbean.Youll earn double bonus miles on every nonstop flight you take . So your next trip will be even more of a pleasure . <p> Amtrak has once again released its fall promotion , giving rewards members an opportunity to earn twice the points between 9/7 and 11/23 . In order to qualify , @ @ @ @ @ @ @ @ @ @ number when making your reservation . <p> From 9/15-12/31 , Priority Club members can earn 2x , 3x , or 4x bonus points for stays at the IHG hotel brands OR 16k bonus miles at participating airlines . How does it work ? You earn 500 bonus points per night ( up to 20k points ) " and then your multiples come in when you stay at different brands : <p> Stay at 2 brands , earn 2x the bonus points <p> Stay at 3 brands , earn 3x the bonus points <p> Stay at 4+ brands , earn 4x the bonus points <p> If you 're signed up for earning miles , you can follow the same program and earn up to 16k miles . You must register in advance of your stay to be eligible . Your bonus points/miles will be deposited immediately , and your multiplier points will be deposited by 1/31/12. 
@@45151267 @5151267/ <p> Sometimes you need to publish a site anyone should be able to see or maybe even add information to . In order to do this you need to have Anonymous access enabled on the site . One step many people miss is that anonymous access has to be enabled at the web application level otherwise it is <p> Lately as part of my work I came across the need to validate some fields and realized probably there should be some common Regex expressions out there I could leverage . This came about because I was doing some code reviews and the validations were not working as intended and I wanted to find some generally <p> If you are looking for a way to programatically hide rows you do n't  wish to show from your data source whether by triggering an event or by a condition you can do it . Its actually pretty simple once you figure out the method exposed for you to use . Below is an example of what I <p> As a SharePoint developer you probably find yourself in the situation where using the @ @ @ @ @ @ @ @ @ @ of them take several clicks and it is not easy to obtain a nice tree view of all the information you need and take actions on them . However , if you look <p> The grouping functionality of the Ajax control RadGrid from Telerik is probably one of its most attractive features . However , many users might encounter themselves in a situation where you can take action on any given element and you wish to update the information being displayed on-screen to correspond to the action you 've taken on the <p> The answer can be found if you look at the error message when ASP.Net ca n't find the assembly . Below is a list of places it would look for the assembly for Telerik.Web.UI : LOG : Attempting download of new URL LONG ... ASP.NET LONG ... LOG : Attempting download of new URL LONG ... ASP.NET LONG ... LOG : Attempting download of new <p> So , you probably have already tried going to the GAC and selecting uninstall , if not , below are the instructions : Navigate to the GAC , which is located at **28;1170;TOOLONG @ @ @ @ @ @ @ @ @ @ click- Uninstall , and then click- Yes- to confirm . In my case I kept wondering why I cant possibly follow those simple instructions . <p> Recently I decided I wanted to start creating children classes to handle my Exceptions , but I ran into the issue that I could n't quite call the base constructor and I kept asking myself why if I did n't  define a constructor the class couldnt use the base class and automatically expose them . So below is kind 
@@45151268 @5151268/ <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got @ @ @ @ @ @ @ @ @ @ ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <h> . Net : Supported Console Colors <p> The days of writing applications that write to the console seem to be over for a while now . Notwithstanding , we find ourselves using the console from time to time to help us debug or quickly test application logic . In my case , I am currently using the console to monitor logging information from my application . It is terribly useful to be able to see progress as it happens . It sort of reminds me of using tail -f in Ubuntu against a log file . As part of my effort to make this log reading more easy I decided I wanted to color code the messages based on what they were : Errors / Warnings / Info / Debug , etc . I kept asking myself if Windows @ @ @ @ @ @ @ @ @ @ and I am happy to report it does ! So here is when the question arises : <h> Pro Tools : Json <p> I 've recently started working on a small programming project on the site . It has been a while since Ive done any programming , so seeing all the new technologies and cloud offerings I must confess I am as excited and fascinated as I am daunted by how much there is and how far it has evolved . This brings us to todays post : " Pro Tools : Json . " As you probably know Json is used widely nowadays to exchange information over API calls . Seems the days of XMLs are in the past , and although I ignore the reasons I do- welcome Json ( mostly because I have no choice I guess , but still . ) So as I am welcomed into this new world full of Json ( I had already used it extensively in configuration files for different applications out there ) I figured I would start a collection of different tools I have found that have helped a @ @ @ @ @ @ @ @ @ @ Json2CSharp.com <p> I am not sure how it is with other programming languages , but in C# you 've just got to serialize and deserialize your objects . As you work with any dataset there is nothing quite like representing the data in your familiar object data structure . Because of this , you need a tool that can help you get the classes you require- to be able to deserialize the json into something you can manage with your applications logic . Here comes Json2CSharp . As the name implies , you just provide it some json and it will generate for you the classes . Makes life much simper specially when you are starting or are having a hard time reading the json . A must have tool ! <h> Pro Tip : Never use type double for money / financial applications <p> When you are new into coding financial applications one of the mistakes people tend to do is using a data type that its precision / significant digits are not enough to properly represent numers . If you 've taken some CS classes you know how decimals are @ @ @ @ @ @ @ @ @ @ <p> To avoid this common pitfall , there are a variety of techniques . One for example , is that if you need only 4 positions after the decimal place you simply store them as integers on the backend and dividing- them by a power of 10 when presenting- them . Some languages offer support of this behind scenes . Take for example System.Decimal in . Net ; It is still a floating point but instead of being a floating binary point it is a floating decimal point , and that makes all the difference . <p> If you do n't  believe me , try this code to see how a double wo n't work for a financial/money application : <p> So if you are using . Net , you should consider using System.Decimal since decimal uses a scaling factor of multiples of 10 , numbers like 0.1 can be represented exactly . In essence , the decimal type represents this as 1 / 10 1 , whereas a double would represent this kind of like 104857 / 2 20 . <p> A decimal can exactly represent any base 10 value @ @ @ @ @ @ @ @ @ @ . <h> References : <h> decimal ( C# Reference ) <p> Visual Studio 2015 <p> - The decimal keyword indicates a 128-bit data type . Compared to floating-point types , the decimal type has more precision and a smaller range , which makes it appropriate for financial and monetary calculations . The approximate range and precision for the decimal type are shown in the following table . <p> You can format the results by using the String.Format method , or through the Console.Write method , which calls String.Format() . The currency format is specified by using the standard currency format string " C " or " c , " as shown in the second example later in this article . For more information about the String.Format method , see String.Format . <h> Problem <p> If you are like me , you are probably sick by now of having the certificate error page pop up everytime you visit the controllers page . I personally think Ubiquiti should make it as easily as uploading a web certificate via the GUI / Web page but for now I am just thankful there is @ @ @ @ @ @ @ @ @ @ should note I have only been able to install a web certificate on the Unifi Controller ( The one that controls networking equipment ) and the Unifi Voip Controller . The Mfi controller and Video controller seem a bit behind the times and I have not been able to assign them proper web certificates yet . <h> Solution <p> UniFi relies on HTTPS for extra security . This means that the browser will check for valid certificates when making a secure connection to the web server . Although the alert message may prove annoying , there 's no risk to the connecting user . To avoid this error : <p> I have used this instructions ( obtained from : - LONG ... successfully . Remember the unifi base folder changes depending on what unifi controller you are using ( video , voip , mfi , base , etc . ) The instructions imply only the base controller has this fuctionality but I was able to do it with the voip controller as well . <p> I have been working with Ubuntu more lately and ran into the need to direct traffic @ @ @ @ @ @ @ @ @ @ to a new server but I could n't change the clients configuration . Because part of the services were already migrated over I could not change the IP address to the old server . Because of this I required a way for the new server to answer on the new IP address assigned to it but also listen to the old servers IP address . To do this you need your server to have more than one IP address which is possible ( have done it with Windows Servers for several years ) so just needed to find out how to do it on Ubuntu <h> Solution <p> If you need an additional IP address just for the moment you can add it to any interface on your machine with <p> sudo ip address add **34;1200;TOOLONG dev &lt;interface&gt; <p> for- example <p> sudo ip address add 172.16.100.17/24 dev eth0 <p> would add 172.16.100.17 using a 24bit netmask to the list of addresses configured for your eth0 . <p> You can check the result with <p> ip address show eth0 <p> and you can delete this address again with <p> sudo ip @ @ @ @ @ @ @ @ @ @ are lost when you reboot your machine . <p> To make the additional addresses permanent you can edit the file /etc/network/interfaces by adding as many stanzas as needed of the form : <p> Note we left the dhcp on the first line , so you will end with 3 ip addresses , 2 static ones provided above and one assigned to you by the DHCP server which will be your primary address . <p> To activate these settings without a reboot use ifdown/ifup likeso : <p> sudo ifdown eth0 &amp;&amp; sudo ifup eth0 <p> It is essential to put those two commands into one line if you are remoting into the server because the first one will drop your connection ! Given in this way the ssh-session will survive and youll be able to reconnect . Exercise caution as if there is an error networking will be dropped but because of the config error it wo n't come back up . <h> Resolved : DNS Lookup Order with VPN on OSX <p> After performing an update on my Mac I was n't able to access intranet resources over my VPN connection . @ @ @ @ @ @ @ @ @ @ servers but the DNS resolution was n't working . <h> Answer : Because you are not using your intranets DNS Server <p> Many people are having issues when connecting to the VPN because the DNS preference of your local connection vs your remote one . However , the solution to his problem is quite simple and it is almost given by the question itself ( when talking about connection preference . ) <p> There are several workarounds , but the easy solution is that you need to give your VPN connection a higher order in the network settings so that the VPN DNS Server is queried before the main internet connections DNS is . <p> To do this simply follow this instructions : <p> Go to the Network section in the System Preferences . <p> On the bottom use the gear icon and select the ( probably fourth ) option that reads roughly " Establish the order of the services " ( I say roughly because my system is in Spanish and I do n't  have one in English to see the exact text ) <p> There , simply drag and @ @ @ @ @ @ @ @ @ @ for that matter above it all so its number 1 . This will make the VPNs DNS Server the preferred choice helping you resolve DNS names for your intranet . <p> and you 're done ! <p> I am not sure why , but probably during an update or something the " service order " in " Network Preferences " moved around a little and when I originally set up my VPN connection it was in the correct order for me so I never realized this was a feature/option . Took a while to find but glad it was n't something overly complex . <h> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker <p> I am going to have to start with a confession , " Resolved " might be a bit of a stretch . For starters , the best case scenario looks like recovering the information but you 'll need a temporary place where to store it , test it ( make sure its fine not corrupted ) , etc . I cant stress enough that as soon as you @ @ @ @ @ @ @ @ @ @ can to avoid following one method and screwing your chances of real recovery with it . I say this because if you write on top of your old data its gone probably for good . You have to be extra careful and be mindful that one method might mess up your chances with another one , so err on the side of caution . By this point all bets are off , you should have had made a backup and this should not be your only alternative . If- the information is valuable you 're better off hiring a professional than trying to fix it yourself . So all I can really say now is I take no responsibility , this method seemed to have worked for others and it worked for me thankfully it does not mean it will work for you . So now that you understand the risks involved ( if not please abort ) , let 's get on with it ! <p> So as you probably figured out , I deleted a volume ( partition/drive ) accidentally . I was trying to delete one that had an @ @ @ @ @ @ @ @ @ @ , then moved it over to another , and then I deleted it so I could recreate elsewhere oh wait its still there oh wait where is my other data volume oh sh ! t ! And that is how I lost my entire long easter weekend . I tried several utilities on the internet ( albeit I distrust so many third parties on my server but what choice did I have ) , and no good . Finally I found an article from Microsoft : - How To Recover an Accidentally Deleted NTFS or FAT32 Dynamic Volume . Basically it states : <h> To Recover a Deleted NTFS Volume <p> Re-create the exact same volume but choose not to format it . This may be difficult if you do not remember the exact size you had created originally , especially because the Disk Management snap-in tends to round partition sizes . <p> Using Dskprobe.exe , recover the backup boot sector for the NTFS volume from the end of the volume . Because it is a dynamic volume you may need to use Dmdiag.exe to help find the backup boot @ @ @ @ @ @ @ @ @ @ on the Tools menu , click Search Sectors ) . <p> After rewriting the NTFS boot sector , quit Dskprobe . <p> In Disk Management , click Rescan Disks on the Action menu . This should mount the volume for immediate use . <p> so I did that the best I could . Fortunately my partition covered the whole drive so I just used the max size while creating it . Please not you should NOT FORMAT the partition/volume , if you do then you just made the recovery considerably less likely and harder and out of the scope of this post . By now the partition showed up as RAW ( not formatted ) and all the recovery utilities where unable to recover the information I needed . By now I had lost almost all hope until I started to look at the actual data looking for any information on any sector when I realized everything looked like garbage eh I mean , random text that might as well be encrypted data ! So here is where things took a turn and my immediate future looked brighter ! The @ @ @ @ @ @ @ @ @ @ that the OS was unable to recognize the formatting on the drive and what not . Doing some research I found a system utility designed for scenarios such as this . <h> Solution <p> Find your decryption key for BitLocker . You get a few choices between : <p> Recovery Key <p> Recovery Password <p> Password <p> Key Package <p> Find a suitable storage location to store your decrypted data <p> The volume/partition must not be the one you are trying to recover <p> The volume/partition must have at least the same amount of available- space as the one you are trying to recover ( total size of both include used and free space ) <p> so now that you have everything you need let 's get started ! <p> First of , take a deep breath and make sure you do n't  accidentally make this thing worse . Double , Triple check everything to make sure you are doing the right thing . So let 's get started <h> Step I Recreate the partition from Disk Management <p> Ill start off with a big warning : Do NOT format the volume/partition @ @ @ @ @ @ @ @ @ @ compmgmt.msc ) , so go ahead and use your favorite method and launch it with administrative rights ( as you 'll need them to create the partition . ) Now that you have it open , time to go look for the Unallocated space where you had the partition you are trying to restore . <p> Once identified , let 's proceed to recreate the partition . In my case , I right clicked the Unallocated space and created a New Simple Volume . Keep in mind we are trying to recreate the lost partition , so you need to provide all the information identically as it was on the deleted volume ( size , etc . ) For more advanced scenarios you could specify the start and end sectors but if you are like me and used the entire disk the wizard should be enough . Again , be careful , one of the steps in the wizard reads " Format Partition . " Make sure you select the option " Do not format this volume " to avoid data loss . Do n't  forget to mount it to a drive @ @ @ @ @ @ @ @ @ @ in my case I have assigned it the letter E. ) Once you 're done you 'll see in the Disk Management console that your partition appears as RAW . If you did n't  have BitLocker on your partition it should ( based on what I read ) show your original partition and you should had recover access to your data . In our case because we used BitLocker the information is encrypted and the OS does not recognize it as a BitLocker enabled drive . Here is where Step II comes in <h> Step II Use- repair-bde to unencrypt your BitLocker volume <p> Due to Microsoft wanting to be careful and safe with your data , this tool is basically a read-only tool . It will not repair your lost volume , but rather would read it , decrypt it , and save the unencrypted information elsewhere . This is why we need a storage location where we can store the entire content of the encrypted volume ( not just the used space , but the entire space of the volume . ) To do so , we are going to @ @ @ @ @ @ @ @ @ @ what Ive seen most people use . I have n't troubleshoot enough but I did ran into some issues using an Imagine file . I tried to mount it with no luck ( The disk image file is corrupted. ) and people suggested using 7Zip to open the image file . That worked fine but all the information I got out was corrupted . I did experience an issue where the progress got stuck at 17% and I had to click enter to have it continue progressing . The same thing happened at different progress %s which might had been the root cause of the data corruption . <p> Because of those issues , I prepared a new volume where to store the data . Instead of using an image file I used a volume instead like so : <p> and voil+ ! You got yourself a volume ( B : ) with the information that used to be stored in the encrypted drive ! <p> I am still double checking all my data is there and that no information is corrupt but thus far it is incredibly promising . All @ @ @ @ @ @ @ @ @ @ displaying properly . Until the users come in tomorrow morning I wo n't know for sure if this was 100% successful but from what Ive seen I believe so . Hopefully this guide saves your life- data as well ! <h> Additional knowledge <p> So here are a few bits and pieces of additional knowledge that might help you when facing this issue : <h> Q : I get an error when running repair-bde : " Failed to authenticate using supplied recovery information . ( 0x80310000 ) " <p> A : You are not providing the right key/password to decrypt your BitLocker drive . As the message at the end says " ERROR : BitLocker is not suspended on this volume . Try another key protector . " <h> Q : The image file shows up as corrupted . Could not open it with 7Zip either <p> A : If your drive is over 2 TB in size it most likely is GPT formatted . If you use diskpart you 'll see that such a formatted disk comes with a " Reserved " system partition : <p> In my case , when @ @ @ @ @ @ @ @ @ @ there so I had no issues . You should use diskpart to see if that is the case . If you do n't  have it then partition alignment wont be the same without it . A potential solution would be to grab a similar drive , partition it , and recreate the partitions identically to replicate as they were before you deleted them in the troubled drive . I am no expert on this , so its up to you to pick the right tool and not overwrite your data . <h> Q : This imagine file looks familiar , can I mount it on a Mac ? <p> A : Sure can . Should work with windows as well not sure what to make it of though . <h> Resolved : " ( initramfs ) unable to find a medium containing a live file system " while installing Ubuntu from a USB device <p> Like I mentioned in a previous post I am working on installing an Ubuntu machine in order to do some GPU mining ( this time around Ethereum ) and now I am running into more @ @ @ @ @ @ @ @ @ @ to perform the installation I am getting the following error message : <p> Resolved : " ( initramfs ) unable to find a medium containing a live file system " while installing Ubuntu from a USB device <h> Solution <p> The solution is to use USB 3.0 ports only . So what I had to do is sacrifice the mouse ( I only have 2 USB 3.0 ports available ) and proceeded to have the keyboard and USB drive connected so I could proceed with the installation . To my surprise that did the trick ! <p> I am currently working on setting up an ethereum mining rig and I wanted to use Ubuntu as my operating system ( no need to pay for a windows license . ) However , as I was installing Ubuntu 14.0.4 LTS on my new computer I ran into a curious detail . As soon as I started the installation process my keyboard and mouse stopped working . I was able to go through the initial screen where you select on the boot options that you wish to install Ubuntu . However , as @ @ @ @ @ @ @ @ @ @ and keyboard on the language selection screen . This was so weird I try to reboot several- times until I started reading the error messages and realized it was an issue with the USB controller ( by then I had unplugged a lot of peripherals and tried different video cards . ) <h> Solution <p> As I pointed out the issue was with the USB controller . After reading on the internet this is what I was able to gather . This issue happens on newer hardware , particularly the one that comes with USB 3.0 . In order for the USB devices ( mouse and keyboard ) to work you need to make sure you do the following : <p> Have enabled legacy mode for the USB controller , <p> enable- USB 3.0 devices , <p> and here is the key : Connect your Mouse and Keyboard on USB 3.0 ports <p> and that 's it ! Some people have reported simply unplugging the devices and connecting them to different ports do the trick ; I am guessing the connected them to USB 3.0 ports afterwards . I tried to @ @ @ @ @ @ @ @ @ @ case . I went from a USB 2.0 port to another 2.0 port just to have the light in the mouse turn on initially and then it went off . Others report just enabling " USB Device Legacy Support " would do the trick . In my case that came enabled right off the bat on the BIOS so that was n't enough . It was until I read further and realized there was a conflict with new controllers and that you needed to stick to USB 3.0 ports that I got it to work , hence my recommendation and solution to enable legacy mode and use USB 3.0 ports . Hope that helps ! <h> How to : Configure a Dynamic DNS Client ( DDClient ) with NameCheap ? <p> Recently I came across a new set of DNS service providers that offer Dynamic DNS services ( for free ! ) One that I ended up really liking was NameCheap . They have a nice user interface , and what 's best is that you can use your own domain name with them and theyll provide Dynamic DNS services to you @ @ @ @ @ @ @ @ @ @ top of the line , but as far as free is concerned and their ability to customize the DNS name , I am sold . Downside is that you need a domain name , which really for most of us that 's not really an issue nowadays . Now you may ask , why are you writting about this ? Well , I had to set up this service and I really had no clue what to enter in the different fields my router was asking for . So I went ahead and did some research to figure out how to get this new service to work <h> How do I use the browser to dynamically update hosts IP ? <p> If you want to update an IP address for bare domain ( e.g. yourdomain.tld ) , then you should specify the following details : <p> Host = @Domain Name= yourdomain.tldDynamic DNS Password = Domain List &gt;&gt; click Manage next to the domain &gt;&gt;Advanced DNS tab &gt;&gt; Dynamic DNS . If it is not enabled , enable it to check the password.IP Address= an optional value . If you do n't  @ @ @ @ @ @ @ @ @ @ accessing this URL will be set for the domain . <p> NOTE : The values for host name and domain must be in lower case . Please make sure your are using your Dynamic DNS password but not the Namecheap accounts one . <p> and there you go ! The trick here is to use echoip which returns your current ip address ( well , the one you are using to access the site . ) It is a dependancy on this working but at least this gives you an idea of how you could manually/automatically update your ip address . Hope this helps ! 
@@45151269 @5151269/ <p> How can you distribute a Trusted Root Certification Authority 's certificate to all the computers in a Domain ? This happens a lot when you create a self-signed certificate or the Certification Authority you are using to obtain your certificates does not come pre-loaded in Windows . The usual work-around is to manually install the certificate on each <p> As of late I 've been running into problems locating the emails I am looking for . Before I was able to type a keyword or two and I could easily find the emails I was searching , but now the amount of similar emails has made search a more cumbersome task . Gmail offers advanced search features that <p> Currently I am working on ReWriting some URLs based on the subdomain name provided ( e.g. you type Restaurants.JCBauza.com and it takes you to a page that is dedicated to restaurants . ) Seeing that different content lives on different directories , I wanted to facilitate the process of URL Rewriting from the administrator 's perspective . The issue : When rewriting <p> So I 've been working on @ @ @ @ @ @ @ @ @ @ customize the 404 page as the default one does n't help people who are visiting your site very much . I decided to use something simple and easy to implement so here are the details . Any other methods , suggestions or more appealing 
@@45151270 @5151270/ <p> I know that title just reads like a mouthful but trust me , could not think of a better one that would let the user know how to install Office 365 in say MultiPoint Server 2012 . Weve been- promoting MultiPoint Server as an alternative to the traditional desktop model as even older desktops running even Ubuntu if licensing costs are an issue can- serve as terminal ( thin clients ) . On other scenarios , we have the Remote App functionality on a Windows Server . However , the only way to install Office on a Server product ( Windows Server / Terminal Server , MultiPoint Server , etc. ) was to install via a Volume License Key . So , what about those of us who purchased through either Open , Open Value , etc. several copies of Office ? Well , as far as I knew we were all out of luck . Fortunately that has recently changed . Microsoft released late last year Office 365 Shared Computer Activation . This bring several advantages to companies using Office 365 , but the most noteworthy obviously @ @ @ @ @ @ @ @ @ @ Server . Note : Back then the only supported product was Office 365 ProPlus , so for those of us with Office 365 Business we are not officially supported . I believe I figured out how to get it working but I will confirm once I have an Office 365 Business user up and running . <p> Shared computer activation let 's you to deploy Office 365 ProPlus to a computer in your organization that is accessed by multiple users . <p> Shared computer activation gets enabled during the installation of Office 365 ProPlus , Project Pro for Office 365 and Visio Pro for Office 365 using the Office Deployment Tool . Once enabled , Office installs without being activated . When a user signs in to a computer with Office installed via shared computer activation , Office will check to see if the user has been provisioned for Office 365 ProPlus and temporarily activates Office 365 ProPlus . If a second user signs in to the same computer , the activation does not persist from the first user and process is repeated . <p> So there is the second beauty @ @ @ @ @ @ @ @ @ @ . You can imagine what Microsoft decades ago would have done , making you use one of your precious computer quotas on each of the shared computers . But thankfully they thought really how in certain scenarios having the activation persist would result in people not adopting the shared computer model or purchasing Office 365 . <p> It 's also important to note that deploying Office 365 ProPlus using shared computer activation does not count against a user 's five total installations of Office 365 ProPlus or Office for Mac . <p> Here are the steps to deploy Office 365 with Shared Computer Activation . <p> II . Run and extract the tool to a folder on your computer or preferably prepare a network share if you are deploying this enterprise wide . <p> III . Edit and configure the configuration XML file as follows : ( change the SourcePath to your network share . Also , Consider I already added the Small Business Premium Retail Product I 'd and the Spanish language . This is not necessary ) <p> &lt;Configuration&gt; <p> &lt;Add SourcePath= " DomainNameDFSo365 " OfficeClientEdition= " 32 ? @ @ @ @ @ @ @ @ @ @ ID= " en-us " /&gt; <p> &lt;Language ID= " es-es " /&gt; <p> &lt;/Product&gt; <p> &lt;Product ID= " O365SmallBusPremRetail " &gt; <p> &lt;Language ID= " en-us " /&gt; <p> &lt;Language ID= " es-es " /&gt; <p> &lt;/Product&gt; <p> &lt;/Add&gt; <p> &lt;Property Name= " SharedComputerLicensing " Value= " 1 ? /&gt; <p> &lt;/Configuration&gt; <p> - IV . From an elevated Command Prompt , run Setup to download the installation files . Please note that there is no GUI / progress bar / etc. to get you know how it is doing . You can check the network share to see if the 1.02gb have been downloaded or not ( at the time of writing it is 1.02gb , could be more or less in the future . ) <p> Setup.exe /download configuration.xml <p> V. From an elevated Command Prompt , run Setup to install Office Click-to-Run . <p> Setup.exe /configure configuration.xml <p> VI . Youre done ! <p> I recommend you read more on this topic . Microsoft has the following 4 resources I found very helpful in figuring this out : <p> You 're absolutely right . An agent told me @ @ @ @ @ @ @ @ @ @ would work as it is an enterprise agreement but no . You necessarily require Office 365 E3 or above ( Enterprise version ) . <p> No , I have resorted to purchasing E3s for all the users that require shared activation . For everyone else I am getting business premium licenses . If you manage to get them working let us know , it would be nice to get cheaper licenses for everyone . 
@@45151271 @5151271/ <p> The grouping functionality of the Ajax control RadGrid from Telerik is probably one of its most attractive features . However , many users might encounter themselves in a situation where you can take action on any given element and you wish to update the information being displayed on-screen to correspond to the action you 've taken on the datasource . The big problem that occurs at this point is that on each postback ( you can do async calls too but the principle is the same ) the groups that were collapsed and expanded all default back to their original condition which could result in a frustrated user having to re-collapse groups to get back to the view they had configured . In order to overcome this you need to save the grouping state before a databinding event and then restore it after the grid is databound . Below is a code snippet on how to achieve this : <p> - - - - /// &lt;summary&gt; <p> - - - - /// Page load function <p> - - - - /// &lt;/summary&gt; <p> - - - - ///The parameter @ @ @ @ @ @ @ @ @ @ parameter is not used . <p> - - - - protected void PageLoad ( object sender , EventArgs e ) <p> - - - - <p> - - - - - - if ( ! IsPostBack ) <p> - - - - - - <p> - - - - - - - - // Set the session used to store the index of collapsed groupings to null as this is a new visit to the page . <p> Sorry for the late response . I have n't used this control in a while perhaps they have addressed this issue in a new version . I could think of 3 alternatives although I am not sure how viable/helpful they would be : 1 ) Perform the binding operation on the client side instead of the server side 2 ) Use entity framework or other sort of query to just return the index of the collapsed items 3 ) If there is a way to identify only what items have been collapsed track that in session rather than go through the entire array/list to re-record them . The restore operation seems fine , @ @ @ @ @ @ @ @ @ @ the state . <p> Finally , I could suggest you control the amount of data you show on screen . Oh wait , I just realized something . This code goes through your columns not rows . In that case the code should not take too long unless you have a super wide ( guessing long means rows and wide columns ) table which is doubtful . If the code on your screen is slow it could be because you are sending again the information in the grid from the server to the client . In theory the time it takes to load your screen the first time should be no more than the time it takes after you do a post back . This is because if you store the information in session you save the time from querying the db and if you perform the collapsing client side that should be even faster I would assume . I hope this helped and good luck ! 
@@45151273 @5151273/ <p> Every now and then you-ll come across the need to force directory synchronization as this usually takes place very few hours ( I believe 3 ) . So what is the issue here ? Well , you could had done a massive update on your servers and you really can-t wait 3 hours for those changes to replicate . Or for instance , say one of your users updated his/her password and can-t log in now as the old one is in use online ( granted , they could log in with their old password ) performing the sync would sync their passwords as well ( if password sync was selected during config ) . <p> In order to force synchronization , first you-ll have to log into the server that has the Directory Synchronization tool installed . Open up PowerShell ( preferably as an Administrator ) and navigate to- C : Program FilesWindows Azure Active Directory Sync. - Then start the Directory Sync Configuration Shell by typing- . DirSyncConfigShell.psc1 <p> This will launch the Directory Synchronization Configuration Shell. - Once this is open you can type the @ @ @ @ @ @ @ @ @ @ 2014 ) : The newer versions of the Active Directory Sync tool have placed this script on a different path : <p> C : Program FilesWindows Azure Active Directory SyncDirSync&gt; <p> Now you wont be launching a new shell , but rather , importing the modules into the current shell . In order to import the required modules you need to run : - . ImportModules.ps1 <p> **27;1236;TOOLONG <p> If you did n't  execute powershell with Admin Credentials ( Run As Administrator ) then you 'll get this warning : <p> - WARNING : Event logging may fail . The current user ( ) is not a member of the Local Administrators group on this computer . <p> As noted this is only a warning and the sync will proceed as normal . The issue here is that logging into the Event Log may fail and you wont be able to keep a record of what happened during that sync . Not a huge issue most of the time but just as a good practice Run this command as an Administrator to get full functionality . <p> Once the sync is @ @ @ @ @ @ @ @ @ @ happened . Unfortunately it measures it by hours so if you are perfoming synchronizations every minute you won-t know unless you visit the event log if they have completed or not as the best you-ll get from the site is " Sync performed less than an hour ago " . <p> Note that all synchronization events can be found in the Application Event Log on the server that the Directory Synchronization Tool is installed . 
@@45151274 @5151274/ <h> Error 0x800f0922 while adding DHCP role on Windows Server <p> If you are installing the DHCP server role on a Windows Server machine you might get the following 0x800f0922 error message : <p> Feature Installation <p> The request to add or remove features on the specified server failed . <p> Installation of one or more roles , role services , or features failed . Error : 0x800f0922 <p> It actually turns out the reason behind this is very simple but somewhat dumb I think . If you already have an active DHCP server on your network then you are going to get error 0x800f0922 thrown every time you try to install another DHCP server . This makes sense specially back in the old day when having multiple DHCP servers was a bad idea as there is no shared central database to keep lease information and IP conflicts may arise from it ( two or more computers with the same IP address ) . So the reason why I think this behavior is not ideal is because a simple warning/pre-requisite check would be more informative than a plain old @ @ @ @ @ @ @ @ @ @ to have more than one DHCP server to balance or fail over . So based on that functionality it should let you install as now it is a valid scenario for an infrastructure . <p> So , to install the DHCP server the solution is simple ( and perhaps obvious once you understand what throws the error 0x800f0922 ) : Install the DHCP server by doing either one of the following two things first : <p> Disconnect the server from the network so it does not detect another active DHCP server <p> Temporarily Stop other DHCP servers on your network <p> And after that , at least to me , the DHCP server role installs successfully and no more 0x800f0922 errors . 
@@45151279 @5151279/ <p> Ive been having a few issues- with my SharePoint 2013 installation which has forced me to dive into the event log- to see what possible errors might be occurring . One of those was error 0x8004FE33 . It turns out this had nothing to do with my problems , it was simply Windows complaining it could not automatically active Windows Server . As I am reinstalling left and right servers as sometimes installations fail ( how complex have they gotten that they do n't  work on the first try as before ) I have not given them Internet access and hence the error appears . Below is more information on how to resolve this issue if you are having problems activating your windows product . Most likely than not it is an internet connectivity issue . <h> Cause <p> These issues may occur when you connect to the Internet through a proxy server on which Basic authentication- is enabled . If the proxy server is configured for Basic authentication , the server requires that you type a username and a password . However , the activation user interface @ @ @ @ @ @ @ @ @ @ the Basic authentication fails , and activation fails . <h> General users <h> Method 1 : Use the telephone to activate Windows <p> Start the Windows Activation Wizard to use the automated telephone system and activate Windows . To start the wizard , click Start , click Run , type SLUI 04 , and then click OK . If you are running Windows 8 , follow these steps : <p> Swipe in from the right edge of the screen , and then tap Search . Or , if you are using a mouse , point to the lower-right corner of the screen , and then click Search . <p> Type SLUI 04 , and then tap or click the displayed icon to open the wizard . <h> Advanced users <p> This method is intended for advanced computer users . If you are not comfortable with advanced troubleshooting , you might want to ask someone for help or contact support . For information about how to contact support , visit the following Microsoft website : 
@@45151281 @5151281/ <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got @ @ @ @ @ @ @ @ @ @ ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <p> Shared computer activation let 's you to deploy Office 365 ProPlus to a computer in your organization that is accessed by multiple users . <p> Shared computer activation gets enabled during the installation of Office 365 ProPlus , Project Pro for Office 365 and Visio Pro for Office 365 using the Office Deployment Tool . Once enabled , Office installs without being activated . When a user signs in to a computer with Office installed via shared computer activation , Office will check to see if the user has been provisioned for Office 365 ProPlus and temporarily activates Office 365 ProPlus . If a second user signs in to the same computer , the activation does not persist from the first user and process is repeated . <p> So there you have it . We love shared computer activation as @ @ @ @ @ @ @ @ @ @ and Windows MultiPoint Server , allowing us to offer the latest Office experience to our licensed Office 365 users . Not only that , but it does not count against the users 5 total installations ! <p> It 's also important to note that deploying Office 365 ProPlus using shared computer activation does not count against a user 's five total installations of Office 365 ProPlus or Office for Mac . <p> So now that we have tauted the features we like the most , it is time to get to work . From the title you may gather that this installation instructions are meant to be used in connection with an Office 365 subscription and for the deployment of Office 2016 . If you would like to deploy Office 2013 , please see our previous post referenced on the top of this article as the steps change a little and the Office Deployment Tool referenced here is exclusively for Office 2016 . <p> Here are the steps to deploy Office 365 with Shared Computer Activation . <p> V. - Edit and configure the configuration XML file . I am @ @ @ @ @ @ @ @ @ @ network share . Also , Consider I already added the Small Business Premium Retail Product I 'd and the Spanish language . This is not necessary . Simply pick from those two product IDs which one you want and then add the languages you wish to install . In my case it was English AND Spanish , so both versions get installed. - <p> &lt;Configuration&gt; <p> &lt;Add SourcePath= " DomainNameDFSo365 " OfficeClientEdition= " 32 ? &gt; <p> &lt;Product ID= " O365ProPlusRetail " &gt; <p> &lt;Language ID= " en-us " /&gt; <p> &lt;Language ID= " es-es " /&gt; <p> &lt;/Product&gt; <p> &lt;Product ID= " O365SmallBusPremRetail " &gt; <p> &lt;Language ID= " en-us " /&gt; <p> &lt;Language ID= " es-es " /&gt; <p> &lt;/Product&gt; <p> &lt;/Add&gt; <p> &lt;Property Name= " SharedComputerLicensing " Value= " 1 ? /&gt; <p> &lt;/Configuration&gt; <p> - VI . - From an elevated Command Prompt , run Setup to download the installation files . Please note that there is no GUI / progress bar / etc. to get you know how it is doing . You can check the network share to see if the space used to see @ @ @ @ @ @ @ @ @ @ the time of writing it is 1.02gb , could be more or less in the future . ) <p> Setup.exe /download configuration.xml <p> VII . - From an elevated Command Prompt , run Setup to install Office Click-to-Run . <p> Setup.exe /configure configuration.xml <p> VIII . Youre done ! <p> I recommend you read more on this topic. - Using the Deployment Tool you can also deploy Office 365 / Office 2016 Enterprise Wide using Group Policy in Active Directory , etc . <p> Update : <p> You can use the following XML which is meant to only active Shared Licensing mode ( no extra languages , no network share , no extra office products , etc . ) Now you can skip the /download command straight to the /configure command . You will need an internet connection as it will download from the web the installation files to deploy Office365 on the system . <h> Resolved : Your server may not be able to connect to sites running on it . Error message : SSL certificate problem : self signed certificate <p> So recently I ran across an error @ @ @ @ @ @ @ @ @ @ an update , you are taken to a screen to individually update all your subsites ( I am guessing to update the database schema they are running under . ) Not sure why this is n't done at the get go but surely there is a reason for it . So the problem was that several subsites did update without much problem but then one of them caused the process to stop and showed the following error below : <h> Your server may not be able to connect to sites running on it . Error message : SSL certificate problem : self signed certificate <p> I am not certain what this error really means . All my subsites run under the same certificate so why some of them worked and this particular one did n't  is beyond me . We also use two reverse proxies to accelerate our web traffic , and our front end web certificate is signed by a CA of public trust . So it is truly a bizarre situation that did n't  happen before . Fortunately we were able to find a workaround to get the process @ @ @ @ @ @ @ @ @ @ sure what this workaround does . For that reason my suggestion is that you implement the workaround , perform the update on WordPress , and disable the workaround until you need it again . I suggest this as the implications of the workaround with WordPress as a whole are unknown to me , so this might cause a security risk or problem as it is modifying intended behavior . Please proceed with care as usual . <p> The key to get this to work as mentioned lies on modifying WordPress behavior . To do so , we are going to rely on the feature called mu-plugins . This is code that is executed when using the multi site plugin / functionality limiting the impact the code may have on other- functionality . In order to do achieve this you need to follow this simple steps : <p> As you can probably tell from it , we are adding filters to bypass SSL verifications . The reason why we chose a MU plugin was so that this change : a ) Impacts only multisite functionality and b ) Remains in place @ @ @ @ @ @ @ @ @ @ look at the **30;1265;TOOLONG file , you 'll see around like 68 something like the following . You need to indicate you do n't  want to perform an SSL Verification to avoid getting this error message . Keep in mind the implications of changing this . <h> Solution <p> The solution probably lies in using a valid public trust certificate for your site . If you cant afford one , get a test one . If not you are going to need to execute the upgrade against a non SSL site which may carry security implications . So , if for some reason you cant or do n't  want to get a public trust certificate , then go ahead and use the Workaround . Best of luck ! <h> How to : Create Web query files for use with Excel for Mac <p> Web queries allow you to query data from a specific World Wide Web , Internet , or intranet site and retrieve the information directly into a Microsoft Excel worksheet . Microsoft Excel includes some sample Web queries . <h> Definition of a Web Query File <p> A Web @ @ @ @ @ @ @ @ @ @ to four lines of text . You can create Web query files in any text editor , such as SimpleText or TextEdit ( with preferences set to text ) . <p> Note You must save query files as text files with no formatting . Rich Text Format ( RTF ) files are not recognized . <p> Web query files are saved in the following folder on your computers hard disk : <p> Microsoft Office 2001:Office:Queries <p> -or- <p> Microsoft Office X/Office/Queries ( in Mac OS X ) <h> How to Create a Web Query File <p> To create a Web query file , follow these steps : <p> Start a text editor , such as SimpleText or TextEdit ( with preferences set to text ) . <p> Type the four lines of text in the text editor ; use the following information : <p> First Line : Type of Query <p> The first line in the Web query file tells Microsoft Excel what type of query the file contains . At this time , the following are the only valid query types : <p> WEB &lt;line omitted&gt; <p> If you @ @ @ @ @ @ @ @ @ @ <p> Second Line : Version of Query <p> The second line of a Web query file tells Microsoft Excel what version of the query is being executed . At this time , the following are the only valid versions : <p> 1 &lt;line omitted&gt; <p> Note If you specify the type of query in the first line , you must specify a version in the second line . If you omit the type , you must also omit the version . <p> Third Line : Uniform Resource Locator ( URL ) <p> The third line of a Web query file determines the Web document on which the query acts . Unless the Web document is a POST type ( see the " Fourth Line : POST Parameters " section ) , this information is the only required value in the Web query file . <p> where file is the name of the document , drive is the drive that contains the file , and folder is the folder name that contains the file . <p> Fourth Line : POST Parameters <p> The fourth line of a Web query file contains @ @ @ @ @ @ @ @ @ @ must be included only if the third line ( the URL ) exceeds 200 characters in length as a result of adding parameters . <p> When you query a Web document for information , the parameters sent to the Web document can be sent in one of two ways : GET or POST . <p> When you use the GET method , data values are included in the same line as the URL . The following example illustrates how to type the line <p> http : //server/file ? parameters <p> where server is the name of the server that contains the Web document , and file is the name of the document . <p> When you use the POST method , data values are sent in a separate line . The following example illustrates how to type the line <p> http : //server/file parameters <p> where server is the name of the server that contains the Web document , and file is the name of the document . <p> After you type all the required lines , save the new file as a text file in the following folder : @ @ @ @ @ @ @ @ @ @ ( in Mac OS X ) <p> Quit the text editor . <p> Use Finder to navigate to the saved text file . <p> Click the file name to highlight the name of the file in the a box . <p> Note Do not open the file . <p> Change the file extension from . txt to . iqy , and then click in an area outside the box to complete the change . When you are prompted to , select Keep . iqy . <p> Note If you do not change the file extension to . igy the Web query will appear in the list when you click Data , click Get External Data , and then click Run saved query . However , the query will be dimmed and you will not be able to select it . <h> How to Use Static and Dynamic Parameters in a Web Query <p> In Web queries , you can use both static and dynamic parameters . Static parameters send query data without prompting you for any values . Dynamic parameters prompt you to type one or more values when the @ @ @ @ @ @ @ @ @ @ as follows <p> parameter=value string <p> where parameter is the name of a parameter ( for example , stock ) and value string is a value . <h> How to : Erase a log file in Ubuntu <p> Ever had this huge error log , full of nightmares and bad memories ? Well , once you 're done fixing the problems then you 're stuck with megs if not gigs worth of bad memories that perhaps you wish to get rid off . Well , in my case one of my errors sent so much information to the log file it was on the gigs arena . This obviously becomes an issue as much of that information is not important to me any more and it can have a performance impact on the system . <p> Usually I would move the original file to a backup location and create a new one . The issue with this approach is that the permissions or attributes of the original file need to be copied over to the new one more work basically . So I did some research and it is possible to simply @ @ @ @ @ @ @ @ @ @ getting rid of the data within and keeping all the attributes and the like untouched . <p> There are a couple of ways to do this : You can do some fancy output redirection or you could use the truncate command . Either way the result is the same and all it takes is one command line . Here it goes : <h> Option 1 : Redirect output to the file to clear it : <p> If you simply redirect null output to the file , basically you 're wiping it clean . Simply do the following : <p> &gt;error.log <h> Option 2 : Use the truncate command : <p> For those who feel more at home with a traditional command instead of ninja magic : <h> Resolved : hphp Warning : Parameter 1 to W3PluginTotalCache : : obcallback() expected to be a reference , value given in **26;1297;TOOLONG on line 3282 <p> As you probably know ( otherwise you would probably not be here ) , after deciding to move form PHP to HHVM , if you are running a WordPress site and use W3 Total Cache you will encounter @ @ @ @ @ @ @ @ @ @ reference : <p> Sun May 17 18:41:51 2015 hphp **28;1325;TOOLONG nWarning : Parameter 1 to W3PluginTotalCache : : obcallback() expected to be a reference , value given in **26;1355;TOOLONG on line 3282 <p> I have good and bad news for you . The good news is that there is a fix for this ( yay ! ) The bad news is that next time you update or re-install W3 Total Cache you might or will need to perform this fix again . I know , it is a pain but thus far this is the best answer I could find ( or rather , the only one ) . <p> So basically what we are going to do is fix some W3 Total Cache php code so that this warning does not show again . I recommend you back up your system just in case something goes wrong ; and by system I think just your plugins directory or the file we are going to modify should suffice . <p> The file we need to modify is located here ( relatively speaking , change /var/www/ for your sites actual www @ @ @ @ @ @ @ @ @ @ the file on your favorite text editor ( say nano ) , look for the following line ( for me it is on line 512 ) : <p> **32;1383;TOOLONG <p> The key here is to change the parameter to just $buffer , like so : <p> **27;1417;TOOLONG <p> This should take care of the issue . If you look at the error log ( /var/log/hhvm/error.log ) you 'll notice . In my case I had several messages per second so with tail -f- /var/log/hhvm/error.log you can tell it is working . Hope this worked for you too ! <p> I know that title just reads like a mouthful but trust me , could not think of a better one that would let the user know how to install Office 365 in say MultiPoint Server 2012 . Weve been- promoting MultiPoint Server as an alternative to the traditional desktop model as even older desktops running even Ubuntu if licensing costs are an issue can- serve as terminal ( thin clients ) . On other scenarios , we have the Remote App functionality on a Windows Server . However , the only way @ @ @ @ @ @ @ @ @ @ / Terminal Server , MultiPoint Server , etc. ) was to install via a Volume License Key . So , what about those of us who purchased through either Open , Open Value , etc. several copies of Office ? Well , as far as I knew we were all out of luck . Fortunately that has recently changed . Microsoft released late last year Office 365 Shared Computer Activation . This bring several advantages to companies using Office 365 , but the most noteworthy obviously is the ability to use Office 365 on a Windows Server . Note : Back then the only supported product was Office 365 ProPlus , so for those of us with Office 365 Business we are not officially supported . I believe I figured out how to get it working but I will confirm once I have an Office 365 Business user up and running . <p> Shared computer activation let 's you to deploy Office 365 ProPlus to a computer in your organization that is accessed by multiple users . <p> Shared computer activation gets enabled during the installation of Office 365 ProPlus , @ @ @ @ @ @ @ @ @ @ 365 using the Office Deployment Tool . Once enabled , Office installs without being activated . When a user signs in to a computer with Office installed via shared computer activation , Office will check to see if the user has been provisioned for Office 365 ProPlus and temporarily activates Office 365 ProPlus . If a second user signs in to the same computer , the activation does not persist from the first user and process is repeated . <p> So there is the second beauty of Shared Computer Activation : The Activation does not persist . You can imagine what Microsoft decades ago would have done , making you use one of your precious computer quotas on each of the shared computers . But thankfully they thought really how in certain scenarios having the activation persist would result in people not adopting the shared computer model or purchasing Office 365 . <p> It 's also important to note that deploying Office 365 ProPlus using shared computer activation does not count against a user 's five total installations of Office 365 ProPlus or Office for Mac . <p> Here are @ @ @ @ @ @ @ @ @ @ . <p> II . Run and extract the tool to a folder on your computer or preferably prepare a network share if you are deploying this enterprise wide . <p> III . Edit and configure the configuration XML file as follows : ( change the SourcePath to your network share . Also , Consider I already added the Small Business Premium Retail Product I 'd and the Spanish language . This is not necessary ) <p> &lt;Configuration&gt; <p> &lt;Add SourcePath= " DomainNameDFSo365 " OfficeClientEdition= " 32 ? &gt; <p> &lt;Product ID= " O365ProPlusRetail " &gt; <p> &lt;Language ID= " en-us " /&gt; <p> &lt;Language ID= " es-es " /&gt; <p> &lt;/Product&gt; <p> &lt;Product ID= " O365SmallBusPremRetail " &gt; <p> &lt;Language ID= " en-us " /&gt; <p> &lt;Language ID= " es-es " /&gt; <p> &lt;/Product&gt; <p> &lt;/Add&gt; <p> &lt;Property Name= " SharedComputerLicensing " Value= " 1 ? /&gt; <p> &lt;/Configuration&gt; <p> - IV . From an elevated Command Prompt , run Setup to download the installation files . Please note that there is no GUI / progress bar / etc. to get you know how it is doing . You can check @ @ @ @ @ @ @ @ @ @ downloaded or not ( at the time of writing it is 1.02gb , could be more or less in the future . ) <p> Setup.exe /download configuration.xml <p> V. From an elevated Command Prompt , run Setup to install Office Click-to-Run . <p> Setup.exe /configure configuration.xml <p> VI . Youre done ! <p> I recommend you read more on this topic . Microsoft has the following 4 resources I found very helpful in figuring this out : <h> How to : Create a Self-Signed SAN Certificate on PowerShell <p> In the past I have wrote about creating self signed certificates- on different architectures as well as creating SAN ( Subject Alternative Name ) Certificates . The main twist is always how to create SAN certificates as the need for them seems to be on the rise . In case you are not familiar , a SAN certificate in simple terms is one certificate which can be used for more than one thing . A regular certificate only protects one domain or a very particular DNSName , for example : KX . CloudIngenium.com . Modern day CAs will issue certificates for @ @ @ @ @ @ @ @ @ @ Kx . CloudIngenium.com " . So if you wanted to protect www.CloudIngenium.com or www.JCBauza.com , you would need a separate certificate . Using SANs you can include additional DNSNames you want your certificate to be good for . But I digress . <p> Getting back on topic , in Windows 8 ( or Windows Server 2012 ) using PowerShell there is a new command at your disposal for creating Self Signed Certificates . This used to be somewhat of a pain , but Microsoft finally wrapped it all up in one nice PowerShell commandlet- for you : **25;1446;TOOLONG . Isnt that awesome ! <p> To make the magic happen , all you really need is the -DnsName and the -CertStoreLocation . Providing those two parameters you will be golden . One thing to keep in mind is that the first DnsName you provide is going to be the Common Name of the certificate , so you might want to pick that one . Here is one example if you want to get started : <p> So , voil+ ! It even gets it installed on your local certificate store ( @ @ @ @ @ @ @ @ @ @ any self signed certificate , they are not trusted by any computer . If you need to avoid those warnings while you perform your tests using the certificate , you will need to add it to the Trusted Root CA store of each computer that is going to access the ( web ) server using that certificate . Simply export your certificate using the Certificate MMC ( you do n't  need to export the private key + it is no recommended you do so for this exercise ) and then import it into the Trusted Root CA store of each computer . <p> Below is the general help available for the command in case you are curious about how to tweak the parameters : <p> **25;1473;TOOLONG Help <p> Description <p> - - - - - - - The **25;1500;TOOLONG cmdlet creates a self-signed certificate for testing purposes . Using the CloneCert parameter , a test certificate can be created based on an existing certificate with all settings copied from the original certificate except for the public key . A new key of the same algorithm and length will be created @ @ @ @ @ @ @ @ @ @ an existing certificate is not being cloned , then an SSL server certificate with the following default settings is created : <p> Subject : - - Empty <p> Key : - - RSA 2048 <p> EKUs : - - Client Authentication and Server Authentication <p> Key Usage : - - Digital Signature , Key Encipherment ( a0 ) <p> ValidityPeriod : - - One year <p> Delegation may be required when using this cmdlet with Windows PowerShell- remoting and changing user configuration . <p> Parameters <p> CertStoreLocation &lt;String&gt; <p> - - - - - - - Specifies the certificate store in which a new certificate will be stored . The current path is the default value . <p> Required ? False <p> Position ? Named <p> Default value . <p> Accept pipeline input ? False <p> Accept wildcard characters ? False <p> CloneCert &lt;Certificate&gt; <p> - - - - - - - - Identifies the certificate to copy when creating a new certificate . The certificate being cloned can be identified by an X509 certificate or the file path in the certificate provider . When this parameter is used , @ @ @ @ @ @ @ @ @ @ except the public key ( a new key of the same algorithm and length will be created ) and the NotAfter and NotBefore fields ( the validity period for the NotBefore field is set to ten minutes in the past ) . <p> Required ? False <p> Position ? Named <p> Default value <p> Accept pipeline input ? true ( ByValue ) <p> Accept wildcard characters ? False <p> -DnsName &lt;String&gt; <p> - - - - - - - Specifies one or more DNS names to put into the Subject Alternative Name extension of the certificate when a certificate to be copied is not specified via the CloneCert parameter . The first DNS name is also saved as Subject Name and Issuer Name . <p> Required ? False <p> Position ? Named <p> Default value <p> Accept pipeline input ? False <p> Accept wildcard characters ? False <p> Confirm &lt;SwitchParameter&gt; <p> - - - - - - - Prompts you for confirmation before running the cmdlet . <p> Required ? False <p> Position ? Named <p> Default value <p> Accept pipeline input ? False <p> Accept wildcard characters ? False @ @ @ @ @ @ @ @ @ @ - Shows what would happen if the cmdlet runs . The cmdlet is not run . <p> Required ? False <p> Position ? Named <p> Default value <p> Accept pipeline input ? False <p> Accept wildcard characters ? False <p> LONG ... <p> The Certificate object can either be provided as a Path object to a certificate or an X509Certificate2 object . <h> Resolved : Where and How to use Name Management function in Excel 2011 for Mac <p> So this is one of those things I keep referring back to . One of the key things I do when I work in Excel is control the values that can be entered in certain fields . I am clearly not good at remembering possible options and every now and then I have typos . So imagine if you will a Table where I track different things and one of those columns clearly becomes a list of things . So when I am trying to provide a list of possible values for a field , I want it to bind to that column listing the valid options . It makes programatic @ @ @ @ @ @ @ @ @ @ with the method of " Data Validation " you know you can bind to a list but a table / column ? No . So clearly we are in dire need of binding to a list , but how can we convert the values of a column to a list ? Using the Name Management function to define a Name ( " list " ) which is defined as the values of a column in a table . <p> Most people know exactly where to find this option on Excel for PC ( say Excel 2013 ) . Simply go to Formulas -&gt; Name Management . But where is Name Management exactly on Excel 2011 for Mac ? I looked and looked but no luck . This forced me to restudy the situation and find a way to add it to a menu . Fortunately that 's exactly what you need to do , but I am still dumbfounded by it being missing on Excels ribbon . But anyway , below are the steps you need to follow in oder to add this to a Menu : <p> Right-click on the @ @ @ @ @ @ @ @ @ @ ribbon ) and then select from the drop down the option : - Customise Toolbars and Menus <p> In the- Commands- tab , - select- All Commands <p> Find the- Define- command ( If you- start typing after selecting any item on the list it will take you to what you 've typed ) <p> Click and drag that command from the list onto the toolbar or menu you wish you use . <p> If this is a one time deal or you are more of a shortcut guy , you can open the Name Management function via keyboard shortcut as well : Command-F3 . Note that I am using F3 , so on a Mac keyboard that is usually 3 keys : Command ( The Apple key ) + the function key ( fn ) + F3 . If you do n't  use the function key it would do the keyboard shortcut drawn on your keyboard . If on System Preferences you already indicated your Mac that you want to treat F1 F12 as Function keys not fancy control keys then there is no need to hold- the function key @ @ @ @ @ @ @ @ @ @ ? <p> MS-OCSP stands for " Online Certificate Status Protocol ( OCSP ) Extensions " from Microsoft . Although this might seem a bit daunting the plain english version of that is n't : " Microsoft publishes Open Specifications documentation for protocols , file formats , languages , standards as well as overviews of the interaction among each of these technologies . " This is a protocol extension provided by Microsoft to check on the Certificate Status . Performing a whois on the domain name confirms this is a domain owned by Microsoft so it is considered safe . If your firewall software is notifying you of connections to ocsp.msocsp.com you can relax . I am including below the WhoIs information at the date of publishing confirming it is a Microsoft domain . 
@@45151282 @5151282/ <h> Category Archive : Programming <p> Unfortunately custom errors is a setting that lives at the SharePoint site level , not at the application page / layouts level , so you will have to edit the web.config that applies to the entire SharePoint site which is located at : - **37;1527;TOOLONG Once there you 'll have to enable 3 different settings that should already be there for <p> The generic- HashSet&lt;T&gt; class is a not ordered- collection for containing unique elements which is the equivalent of the HashTable but for use with generics . For more information about this collection , see- HashSet Collection Type at the MSDN website . <p> Although I believe that as part of the . Net framework version 4 you can specify the Enumeration type that you need when you are creating a generic method , this is not available in the . Net framework 3.5 ( or in previous releases for that matter ) . The problem then becomes that you do n't want to have a <p> How to : Initialize a Dictionary with a Collection Initializer Sometimes in your project you want @ @ @ @ @ @ @ @ @ @ the declaration level so you do n't  have to manually use the . add() methods and write a separate method in which to execute them . A prime example is a dictionary that <p> I am running into an issue when I import a db into a SQL Database project which indicates that " Error 1 SQL03006 : User : x has an unresolved reference to Login x " . After doing some research I found out that to be able to execute the Create User command the x.user.sql file contains you need to <p> I have a method that queries Active Directory and it works fine on a domain computer , however , when I try using another machine that has connectivity towards AD and I pass the domain credentials that have the- necessarily- rights to connect . I 'm not sure what to try next below is the error message I get : <p> A question on many development projects that use a database is how to share that database with the entire development team and keep up a centralized , version controlled version of the schema which can @ @ @ @ @ @ @ @ @ @ 2010 steps up to the challenge with the database projects . - You can easily 
@@45151283 @5151283/ <h> General Commands Manual : unlink <h> General Commands Manual : unlink <h> NAME <h> SYNOPSIS <h> DESCRIPTION <p> The unlink utility shall perform the function call : unlink(file) ; A user may need appropriate privilege to invoke the unlink utility . <h> OPTIONS <p> None . <h> OPERANDS <p> The following operands shall be supported : file The pathname of an existing file . <h> STDIN <p> Not used . <h> INPUT- FILES <p> Not used . <h> ENVIRONMENT- VARIABLES <p> The following environment variables shall affect the execution of unlink : LANG Provide a default value for the internationalization variables that are unset or null . ( See the Base Definitions volume of IEEE Std 1003.1-2001 , Section 8.2 , Internationalization Variables for the precedence of internationalization variables used to determine the values of locale categories . ) LCALL If set to a non-empty string value , override the values of all the as opposed to multi-byte characters in arguments ) . @ @ @ @ @ @ @ @ @ @ the format and contents of diagnostic messages written to standard error . NLSPATH Determine the location of message catalogs for the processing of LCMESSAGES . <h> APPLICATION- USAGE <h> EXAMPLES <h> RATIONALE <h> FUTURE- DIRECTIONS <h> SEE- ALSO <h> COPYRIGHT <p> Portions of this text are reprinted and reproduced in electronic form from Open Group Base Specifications Issue 6 , Copyright ( C ) 2001-2003 by the Institute of Electrical and Electronics Engineers , Inc obtained online at http : **36;1566;TOOLONG . 
@@45151284 @5151284/ 55329 @qwx465329 <h> How to : Get the size of your Databases in MySQL Query Browser <h> How to : Get the size of your Databases in MySQL Query Browser <p> Imagine you want to transfer data from one server to another and you need to know the size of the databases you are thinking of transfering . Unlike Microsoft SQL Server which graphical interface allows you to see the size of the database files and the amount of data stored MySQL WorkBench does not have a section that allows you to do just that ( or at least I have n't found it yet ) . - The good news is that you only need to run a simple query in order to obtain a table listing the different tables in your server as well as the size of the databases . Below is the SQL script : 
@@45151287 @5151287/ 55329 @qwx465329 <h> What is " Andrea ST Filters Service " ? <h> What is " Andrea ST Filters Service " ( AESTFilters ) ? <p> Many people wonder what is " Andrea ST Filters " windows service when going- through the list of processes that were running on their machine . I was one of them as I was trying to find out what is causing it to run so slowly and one of the processes that was on the top was the Andrea ST Filters . It was n't using many resources it just a process Ive never seen before and the list was sorted alphabetically so I noticed it . It turns out that if you happen to have an HP machine odds are you 'll see that process . It is nothing to be alarmed of , it actually is a legit service that is used by the Sigmatel Audio software . Sigmatel uses the Andrea ST Filters service mostly for noise cancellation ( for your microphone of course ) . So I 'm relieved to find out its not some- unnecessary- service or malware . My guess @ @ @ @ @ @ @ @ @ @ SigmaTel ? ) but I have n't looked that up yet . Many people would be alarmed not only as Andrea ST Filters shows up on their task manager , but also you can see it when you launch- msconfig.exe . <h> Is it safe ? <p> Yes , as mentioned above it is a service that comes preinstalled with computers that use the Sigmatel Audio Software for your microphone , etc . Hewlett Packard ( or Compaq ) has used this provider in the past with their machines . <h> Can I remove it ? <p> Sure , in theory the worst that could happen would be that the noise filtering in your microphone would not work . Some people are picky as to what runs on their machines and if they hardly ever use the microphone there is no harm on removing this service . You can use msconfig.exe to disable its automatic launch . Personally I would leave it there in my experience it hardly uses any resources . <h> What is a good practice regarding checking up on windows services ? <p> It is always a good @ @ @ @ @ @ @ @ @ @ running on your machine . Sometimes these processes can be viruses , malware , or simply things you do n't  really need but are using your computer resources . I only do those checks when my computer is running slowly and I try to tackle the ones that are using the most memory , processor capacity or accessing the hard drive the most . Like they say , 20% of your processes will be using 80% or your resources , so focus on those ! 
@@45151291 @5151291/ <h> Why does my Mac OSX say " update needed " on every boot ? <p> If you recently reinstalled your Mac with OS X you might notice that every time you reboot when you try to login instead of showing the username of the first user account- created it shows an icon with the text " Update Needed " . You can of course enter your master password and then the user icon will be displayed and you can log in as normal . This does n't  seem like a huge deal , except when you 've had this issue for months and you are sick and tired of having to take an extra step to log in . <p> Anyway , apparently the root of the issue is that you are using an encrypted hard drive- and reinstalled OS X. My guess is that part of the operating system is unable to perform an action- on the hard drive after you reinstall and even though you provide the master password it does n't  do much . But once you have identified the problem the solution become a bit @ @ @ @ @ @ @ @ @ @ an update is needed " when you reboot disappear you are going to have to turn encryption on your hard drive off and then back on . Obviously when you perform an action- that is going to write over all your data on your hard drive your best bet is to backup- your entire computer . After you have performed a backup the next step would be to turn off Firevault . This would make your OS take all your information and write it back to the hard drive unencrypted . Depending on the speed of your computer and amount of information this could take a long time . After you hare completed unencrypting- all your information you can either stop here or if you want it encrypted you know what to do now : Turn on Firevault- again . The message by now should stop appearing , so you do n't  need to turn on firevault- again if that was your sole objective . But more likely than not , if you encrypted your hard drive you probably want it to be encrypted still . Again , this is @ @ @ @ @ @ @ @ @ @ as now all your unencrypted that is going to get encrypted and written to your hard drive . I would go as far as recommending erasing all your free space after you are done- to be safe if you have sensitive information . 
@@45151292 @5151292/ <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <h> Travel advise : Disneyland <p> After recently visiting Disneyland there were a couple of noteworthy tips to take into consideration when visiting this attraction in Anaheim , CA . <p> First of all I would highly recommend staying in a hotel within the property . Not only are the usually nicer but the- convenience- they offer being right in the park as well as some added perks make the extra money you pay worth it . " Disneys Grand Californian Hotel &amp; Spa " is the nicest of the three , but you can use other hotels pools like- Disneyland Hotels Monorail slide . Currently there is construction going on as an old pool is getting an upgrade there but no big deal . Part of the benefits you receive when you stay in a hotel includes early access to the park in the morning . This is a great benefit as it gives you access to the park 1 hour before the park opens to the general public . This one hour @ @ @ @ @ @ @ @ @ @ you run to the first ride you 'll find there is little to no line and we got to ride 3 different attractions within that hour . Not to mention the Grand California Hotel is right in the middle of downtown Disney and there is an entrance to the California Adventures park . You might not think its such a big deal but when you have to walk to find your hotel shuttle and wait for it you 'll appreciate staying within the property much more . <p> Going back to the hotels , similar to other hotel chains they offer Concierge services which include snacks and other benefits you might be interested looking into like the getting a Fastpass ticket good for immediate access to two attractions . Also parking costs is something you might want to consider . When I stayed there the parking for guests was $15 a day while parking for guests was more expensive . Nearby hotels like the Sheraton charged $10 a night and you can take the shuttle to Disney , but again , taking the shuttle is no fun right ? <p> The Fastpass @ @ @ @ @ @ @ @ @ @ a spot in line . You check-in a kiosk and then it gives you a ticket good to cut a significant part of the line during a specified window of time . You can only use it every so many hours but if used wisely this can help you maximize the number of rides you can get on so plan ahead . There are apps available that allow you to track the wait times for the rides at any given time that might be while downloading before you hit the park . 
@@45151293 @5151293/ <h> Biography <p> A dedicated and versatile pianist , Cha Young Park has performed in solo and chamber recitals across the United States and Asia . She most recently has made her solo debut at the Weill Recital Hall , Carnegie Hall and Steinway Hall in New York City . <p> Ms. Park is a highly regarded piano teacher with over ten years ' experience . She works with students across a wide range of repertoires and age levels , offering lessons in English and Korean . Many of her students have gone on to summer music festivals , participated in competitions , and performed in solo recitals . In addition to maintaining a private studio , she held a piano faculty position for the Pre-College Young Pianist Program at the Jacobs School of Music , Indiana University . Furthermore , she has taught group lessons , coached chamber groups , and led seminars at the Butler University Summer Piano Camp and Levine Music and Arts Camp . <p> After immigrating from South Korea to the United States in 1997 , Ms. Park continued her musical studies , winning @ @ @ @ @ @ @ @ @ @ Butler University on a full scholarship , earning a Bachelor of Music in Piano Performance in 2008 . While at Butler , she received an Audition Award from Jordan College of Fine Arts , the Pi Kappa Lambda Scholarship , and won first prize at the Jordan College of Fine Arts Concerto Competition . In 2011 , Ms. Park received the Artistic Excellence Award , a full scholarship from the Jacobs School of Music , Indiana University , where she earned her Master of Music in Piano Performance in 2013 . <p> Currently , Ms. Park is pursuing a Doctorate of Musical Arts degree in Piano Performance at the Catholic University of America , Benjamin T. Rome School of Music under the guidance of Professor Jose Ramos Santana . She serves as a piano faculty at the Levine School of Music in Washington , D.C. and maintains a private piano studio in Arlington , Virginia . She is frequently invited to serve as an adjudicator and is an active member of- the Northern Virginia Music Teachers Association and the Washington D.C. Music Teachers Association . 
@@45151294 @5151294/ <h> Category Archive : Microsoft <p> For a long time now every time I try to download something from a Microsoft site that uses the download manager I come across different error messages in Internet Explorer , the latest one being : - " An add-on for this website failed to- run . Check the security settings in Internet Options for Potential Conflicts . " and it pretty much <p> How can I verify if I can connect to my email server ? Im having issues sending email using an SMTP server and I want to know if it is because of my code or due to an external issue . What is an alternative to verify connection settings to an SMTP Host ? <p> Why would the following error message occur ? : " Communication with the underlying transaction manager has failed . " I also got the following inner exception in case this helps : " The MSDTC transaction manager was unable to pull the transaction from the source transaction manager due to communication problems . Possible causes are : a firewall is present and it does @ @ @ @ @ @ @ @ @ @ I create dynamic controls , even though I re-create them they still lose their respective ViewState . How can I ensure that my Dynamic ASP.Net controls and User Web Controls save their ViewState ? 
@@45151298 @5151298/ <p> Migration notes from Windows SBS 2011 Standard to Windows Server 2012 Essentials As you probably have already found out , Microsofts successor for Windows Small Business Server 2011 is no other than Windows 2012 Essentials . Many people have consider this a let down as Windows 2012 Essentials loses a lot of the SBS 2011 premium features <p> Windows Server 2012 list of- Powershell commandlets &amp; a few common ones With the release of Windows Server 2012 server powershell got a few changes as well . There are some quick start guides available here : LONG ... that will make most tasks in the command prompt much easier . Below are a few common commandlets that can be useful <p> Symptom When name resolution is provided by root hints , Windows Server 2008 DNS and Windows Server 2008 R2 DNS Servers may fail to resolve queries for names in- certain top-level domains . When this happens , the problem will continue until the DNS Server cache is cleared or the DNS Server service is restarted. - The problem can be seen <p> Exchange 2010 store.exe service Store.exe is @ @ @ @ @ @ @ @ @ @ and is found on your mailbox server . Many people come across this process as it is known to use a lot of memory from the host machine . If you are running an Exchange server on your machine you <p> How to resolve " Network Path Not Found " in Exchange 2010 Management Console on an Edge or Hug transport server All of a sudden one day my users started to report that emails were not arriving to the exchange mailboxes . When I looked into the queue of my Edge server the error mentioned above ( An <p> How to Configure TMG logging to use a central SQL Server store The default log settings for Forefront TMG are set to a local Microsoft SQL Server Express 2008 SP1 database . During the Forefront TMG installation a local Microsoft SQL Server Express database will be installed . If you want to change this local SQL Server <p> Deploying Microsoft Lync 2010 mobility services for iPad and iPhone After much struggle to get my iPad to work with Lync server ( I tried everything from certificates to configuring the edge @ @ @ @ @ @ @ @ @ @ those mobile devices connect via web services but you need to install them . The installation surprisingly seems <p> How to resolve common HTTPS inspection issues while using Microsoft TMG 2010 While working with Microsoft Forefront Threat Management Gateway 2010 ( previously known as Internet Security and Acceleration server ) https inspection is a big new component of it . If you ever look through your logs and you find https-inspection as the protocol that is causing 
@@45151304 @5151304/ <h> Resolve : Upstream sent too big header while reading response header from upstream when using NginXPHP FPM <p> When working with NginX you will be using an upstream PHP FPM server . Because of the way NginX works it will treat this upstream server as a proxy server which you will use to server PHP requests . One of the settings you can control is the buffer size used for the headers . You want to size your buffer size correctly otherwise you might get a lot of errors like this one : 
@@45151307 @5151307/ <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got @ @ @ @ @ @ @ @ @ @ ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <h> General Commands Manual : dpkg-buildpackage <h> NAME <p> dpkg-buildpackage - build binary or source packages from sources <h> SYNOPSIS <p> dpkg-buildpackage option ... <h> DESCRIPTION <p> dpkg-buildpackage is a program that automates the process of building a Debian package . It consists of the following steps : 1 . It prepares the build environment by setting various environment variables ( see ENVIRONMENT ) and calls **25;1604;TOOLONG ( unless -T or --target has been used ) . 2 . It checks that the build-dependencies and build-conflicts are satisfied ( unless -d is specified ) . 3 . If a specific target has been selected with the -T or --target option , it calls that target and stops here . Otherwise it calls **25;1631;TOOLONG to clean the build-tree ( unless -nc is specified ) . 4 . It calls dpkg-source-b to @ @ @ @ @ @ @ @ @ @ been requested with -b , -B or -A ) . 5 . It calls debian/rulesbuild-target followed by **33;1658;TOOLONG ( unless a source-only build has been requested with -S ) . Note that build-target and binary-target are either build and binary ( default case , or if -b is specified ) , or build-arch and binary-arch ( if -B is specified ) , or build-indep and binary-indep ( if -A is specified ) . 6 . It calls gpg to sign the . dsc file ( if any , unless -us is specified ) . 7 . It calls dpkg-genchanges to generate a . changes file . Many dpkg-buildpackage options are forwarded to dpkg-genchanges. 8 . It calls gpg to sign the . changes file ( unless -uc is specified ) . 9 . If -tc is specified , it will call **25;1693;TOOLONG again . Finally it calls dpkg-source--after-build . <h> OPTIONS <p> -b Specifies a binary-only build , no source files are to be built and/or distributed . Passed to dpkg-genchanges. -B Specifies a binary-only build , limited to architecture dependent packages . Passed to dpkg-genchanges. -A Specifies a binary-only @ @ @ @ @ @ @ @ @ @ dpkg-genchanges. -S Specifies a source-only build , no binary packages need to be made . Passed to dpkg-genchanges. -F Specifies a normal full build , binary and source packages will be built . This is the same as the default case when no build option is specified . **37;1720;TOOLONG Calls debian/rulestarget after having setup the build environment and stops the package build process here . If --as-root is also given , then the command is executed as root ( see -r ) . Note that official targets that are required to be run as root by the Debian policy do not need this option . --as-root Only meaningful together with --target . Requires that the target be run with root rights . LONG ... Passed unchanged to dpkg-genchanges . See its manual page . -aarchitecture Specify the Debian architecture we build for . The architecture of the machine we build on is determined automatically , and is also the default for the host machine . -tgnu-system-type Specify the GNU system type we build for . It can be used in place of -a or as a complement to override the @ @ @ @ @ @ @ @ @ @ -jjobs Number of jobs allowed to be run simultaneously , equivalent to the make(1) option of the same name . Will add itself to the MAKEFLAGS environment variable , which should cause all subsequent make invocations to inherit the option . Also adds parallel=jobs to the DEBBUILDOPTIONS environment variable which allows debian/rules files to use this information for their own purposes . The parallel=jobs in DEBBUILDOPTIONS environment variable will override the -j value if this option is given . -D Check build dependencies and conflicts ; abort if unsatisfied . This is the default behavior. -d Do not check build dependencies and conflicts . -nc Do not clean the source tree ( implies -b if nothing else has been selected among -B , -A or -S ) . -tc Clean the source tree ( using **34;1759;TOOLONG ) after the package has been built . -rgain-root-command When dpkg-buildpackage needs to execute part of the build process as root , it prefixes the command it executes with gain-root-command if one has been specified . Otherwise , if none has been specified , fakeroot will be used by default , if the command @ @ @ @ @ @ @ @ @ @ a program on the PATH and will get as arguments the name of the real command to run and the arguments it should take . gain-root-command can include parameters ( they must be space- separated ) but no shell metacharacters. gain-root-command might typically be fakeroot , sudo , super or really . su is not suitable , since it can only invoke the user 's shell with -c instead of passing arguments individually to the command to be run . -Rrules-file Building a Debian package usually involves invoking debian/rules as a command with several standard parameters . With this option it 's possible to use another program invocation to build the package ( it can include space separated parameters ) . Alternatively it can be used to execute the standard rules file with another make program ( for example by using **33;1795;TOOLONG as rules-file ) . -psign-command When dpkg-buildpackage needs to execute GPG to sign a source control ( . dsc ) file or a . changes file it will run sign-command ( searching the PATH if necessary ) instead of gpg . sign-command will get all the arguments that @ @ @ @ @ @ @ @ @ @ or any other shell metacharacters. -kkey-id Specify a key-ID to use when signing packages . -us Do not sign the source package . -uc Do not sign the . changes file . -iregexp -Ipattern-snsAkurKUR -z , -Z Passed unchanged to dpkg-source . See its manual page . --source-option=opt Pass option opt to dpkg-source. --changes-option=opt Pass option opt to dpkg-genchanges. **27;1830;TOOLONG Change the location of the dpkg database . The default location is /var/lib/dpkg. - ? , --help Show the usage message and exit . --version Show the version and exit . <h> ENVIRONMENT <p> Even if dpkg-buildpackage exports some variables , debian/rules should not rely on their presence and should instead use the respective interface to retrieve the needed values . LONG ... is called with the -a and -t parameters forwarded . Any variable that is output by its -s option is integrated in the build environment . **32;1859;TOOLONG Between versions 1.14.17 and 1.16.1 , dpkg-buildpackage exported compiler flags ( CFLAGS , CXXFLAGS , FFLAGS , CPPFLAGS and LDFLAGS ) with values as returned by dpkg-buildflags . This is no longer the case . <h> BACKWARD- COMPATIBILITY <p> dpkg-buildpackage @ @ @ @ @ @ @ @ @ @ . Those targets are thus mandatory . But to avoid breakages of existing packages , and ease the transition , it will fallback to using the build target if **33;1893;TOOLONG returns 2 as exit code . <h> BUGS <p> It should be possible to specify spaces and shell metacharacters in and initial arguments for gain-root-command and sign-command . <h> How to : Install , Update and Remove RubyGems <p> I just had a brief experience installing a RubyGem so I thought I would share a little bit of what I learned about managing RubyGems . Similar to packages ( apt-get ) you can search through them , install them , remove them , and update them with simple commands . Below is a brief summary designed just to quickly point individuals to the instructions needed to accomplish the management of RubyGems : <p> Searching ( where mysql is the the search term : it will find any gem that contains the word mysql . The address after remote indicates the online library where the search will be performed ) <p> gem search mysql remote gems.rubyforge.org <p> Installing ( where mysql @ @ @ @ @ @ @ @ @ @ automatic answer to yes when it asks to install a needed dependency ) <p> sudo gem install mysql <p> sudo gem install mysql include-dependencies <p> Updating ( you may update all gems or specify a particular one like mysql ) <p> sudo gem update <p> sudo gem update mysql <p> Removing ( where mysql is the gem you wish to uninstall ) <p> sudo gem uninstall mysql <p> Clean / Removes old versions ( I still struggle to understand the difference with update ) <h> General Commands Manual : unlink <h> NAME <h> SYNOPSIS <h> DESCRIPTION <p> The unlink utility shall perform the function call : unlink(file) ; A user may need appropriate privilege to invoke the unlink utility . <h> OPTIONS <p> None . <h> OPERANDS <p> The following operands shall be supported : file The pathname of an existing file . <h> STDIN <p> Not used . <h> INPUT- FILES <p> Not used . <h> ENVIRONMENT- VARIABLES <p> The following environment variables shall affect the execution of unlink : LANG Provide a default value for the internationalization variables that are unset or null . ( See the Base @ @ @ @ @ @ @ @ @ @ Internationalization Variables for the precedence of internationalization variables used to determine the values of locale categories . ) LCALL If set to a non-empty string value , override the values of all the as opposed to multi-byte characters in arguments ) . LCMESSAGES Determine the locale that should be used to affect the format and contents of diagnostic messages written to standard error . NLSPATH Determine the location of message catalogs for the processing of LCMESSAGES . <h> APPLICATION- USAGE <h> EXAMPLES <h> RATIONALE <h> FUTURE- DIRECTIONS <h> SEE- ALSO <h> COPYRIGHT <p> Portions of this text are reprinted and reproduced in electronic form from Open Group Base Specifications Issue 6 , Copyright ( C ) 2001-2003 by the Institute of Electrical and Electronics Engineers , Inc obtained online at http : **36;1928;TOOLONG . <h> NAME <h> SYNOPSIS <h> DESCRIPTION <p> In the 1st form , create a link to TARGET with the name LINKNAME . In the 2nd form , create a link to TARGET in the current directory . In the 3rd and 4th forms , create links to each TARGET in DIRECTORY . Create hard links by default , symbolic links with --symbolic . By default , each destination ( name of new link ) should not already exist . When creating hard links , each TARGET must exist . Symbolic links can hold arbitrary text ; if later resolved , a relative link is interpreted in relation to its parent directory . Mandatory arguments to long options are mandatory for short options too . --backup=CONTROL make a backup of each existing destination file -b like --backup but does not accept an argument -d , -F , --directory allow the superuser to attempt to hard link directories ( note : will probably fail due to system restrictions , even @ @ @ @ @ @ @ @ @ @ files -i , --interactive prompt whether to remove destinations -L , --logical dereference TARGETs that are symbolic links -n , --no-dereference treat LINKNAME as a normal file if it is a symbolic link to a directory -P , --physical make hard links directly to symbolic links -r , --relative create symbolic links relative to link location -s , --symbolic make symbolic links instead of hard links -S , --suffix=SUFFIX override the usual backup suffix -t , **28;1966;TOOLONG specify the DIRECTORY in which to create the links -T , --no-target-directory treat LINKNAME as a normal file always -v , --verbose print name of each linked file --help display this help and exit --version output version information and exit The backup suffix is ' ' , unless set with --suffix or SIMPLEBACKUPSUFFIX . The version control method may be selected via the --backup option or through the VERSIONCONTROL environment variable . Here are the values : none , off never make backups ( even if --backup is given ) numbered , t make numbered backups existing , nil numbered if numbered backups exist , simple otherwise simple , never always make simple @ @ @ @ @ @ @ @ @ @ the last option specified controls behavior when a TARGET is a symbolic link , defaulting to -P . <h> SEE- ALSO <p> link(2) , symlink(2) The full documentation for ln is maintained as a Texinfo manual . If the info and ln programs are properly installed at your site , the command **26;1996;TOOLONG ' should give you access to the complete manual . <h> How to : Add or Remove Symbolic links in Ubuntu <p> Unlike Windows , in Linux based systems symbolic links are use quite frequently . The advantage of using symbolic links is that they allow you to have the same file in multiple locations , but the content to remain the same across all of them regardless of where you changed the file . For example , Apache or NginX use symbolic links to enable sites from a repository of web sites available . By adding a symbolic link you can enable a site while by removing it you can disable it . This is extremely useful as it solves the issue of keeping track of updates to files that should be the same . <p> @ @ @ @ @ @ @ @ @ @ you need to remember when using symbolic links : <h> II . Use unlink to remove a previously created symbolic link like this : <p> unlink- **36;2024;TOOLONG <p> - Note the use of -s in the ln command to indicate a soft link . Hard links will link to a specific version of the file and therefore it would appear as if their content is not updated when the source files is . <p> I recommend reading the man pages for ln and unlink if you want to learn more about what you can do with them . <h> How to : Move the header widget down in Graphene to place your Ads closer to the post <p> If you 've visited our site before you might remember how our Google Add for the header was all the way up overlapping the good old banner . If not then you are like most users ; Advertising that is not close to the content gets lost , specially if it is above the header . This is why Google has recommended to : Move your ad unit below the header . <h> @ @ @ @ @ @ @ @ @ @ of the page ? <p> Ads placed above the header are often hidden , and accordingly have lower CTRs than ads further down the page . Move ads at the top of your page down and you could earn more . <p> So keeping that in mind and to please Google ( who after all just cares the site getting more clicks so they make more money ) I looked into improving the Graphene Theme for WordPress . The problem is that Graphenes point of view is that the header widget should be place at the header obviously . But if you are trying to place advertising on the top of your post via widgets this presents a challenge . Regardless of how you look at it , editing the code behind is necessary ( and if it is not please correct me ! ) The options include : <p> Create a new Widget Area for your advertising <p> Find a plugin that could insert that advertising in for you <p> Move the header widget to a more friendly area . <p> On this post we will focus on option @ @ @ @ @ @ @ @ @ @ location that should work better than the current one . For this you need to edit the header.php file of the theme . You could do this via a number of ways : Your WordPress admin site , FTP into your web host , or if you have a VPN just log in and edit the file . The key here is to move the Header Widget Area code which is shown below : <p> Once you find this section of the code , you are going to cut and paste it so go ahead and cut the text . Now you need to find the area were you need to paste this code . Below are the two lines you need to find were you will paste this code inbetween them as shown here : <p> Once you have done this , you will notice that your widget area no longer resides at the header but now below it somewhere between your navigation and your posts . This makes it so that your ad does n't  get lost as easily . Also , if you were as lazy as @ @ @ @ @ @ @ @ @ @ look hideous and out of place much better now ! <p> NOTE : <p> Every time there is a new update to the theme you are going to have to make this change . With that regards the simplest solution might just be to find a plugin to insert that advertising unit above the post . I have n't found a great one so I am sticking to making this change every update . <h> How to : Install the NginX Agent for NewRelic in an Ubuntu server <p> Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . For example , take this instructions on how to install the NginX agent to get monitoring on your web server : <p> Run- . **26;2062;TOOLONG start- to start the agent in daemon mode . <p> Seems simple enough but I really wish I could just copy paste some code and be done . For starters , I do n't  even have Ruby installed or Bundler and I cant remember how to properly extract . tar files so @ @ @ @ @ @ @ @ @ @ Taking into consideration that not all users are savvy to the bone Linux users I decided to get a more low level detailed step by step instructions to make this task easier : <h> O. Summary ( Just the commands ) <p> For those whove done this in the past and just want to get it over with quickly : <p> sudo bash <p> apt-get install ruby2.0 <p> apt-get install ruby2.0-dev <p> apt-get install- build-essential <p> gem install bundler <p> bundle install <p> ( Download the plugin to a folder like /etc/newrelic and extract the files to a subdirectory ) <h> Resolve : Error 0x800B0100 when you try to install Windows Updates or Microsoft Updates on a fresh MultiPoint 2012 Server <p> If you read my previous post that reads quite similarly to this then you know you get lots of 0x800B0100 error messages when you run Windows Update for the first time on a fresh MultiPoint 2012 Server installation . What 's happening from what I 've gathered is that some of the newer updates interfere with some of the older updates . Usually Microsoft does a pretty good job handling @ @ @ @ @ @ @ @ @ @ at the time of writing this collisions happen . I have a MultiPoint server installed that has been keeping up to date with the latest releases from Microsoft which experiences no problems . However , I started the installation of a new server to replace the old one and updates were failing left and right . Let 's keep in mind this is a fresh installation so this should not be happening . I figured maybe I screwed something up , the hard drive wrote the wrong byte you know very low probability things that could explain this . But when I tried in to install it in a new VM and still got the error messages for all those updates I decided it was not my environment but Microsofts updates that were messing up the system . <p> It seems the problem is MultiPoints 2012 Cumulative Update #2 . If you install this update first without any others the system will proceed and the errors wont happen . Unfortunately once you 've messed up the system you need to reinstall a fresh copy or restore from a backup / snapshot . @ @ @ @ @ @ @ @ @ @ following updates in this order . The trick is to avoid updates that collide with one another ( . Net language pack for some reason crashes when installing anything . Net security update ) : <p> Install all updates that have nothing to do with Windows ( IE , Silverlight , etc ) but avoid- KB2858726 and- KB2898870 ( and all . Net except . Net 4.5.1 ) . In this case I installed updates : KB2900986 , . Net 4.5.1 update ( KB2881468 ) , KB2668562 , KB2909921 , KB2520426 and KB890830 . <p> Install again the ones that have nothing to do with Windows but avoid the . Net language pack as you have . Net security updates . This time around I installed : KB2898870 , KB2942802 , KB2890788 and KB2901127 . <p> Now you should avoid installing the . Net 4.5.1 Language pack ( KB2858726 ) update . When I did it generated error ( 0x80073712 ) . <p> I was doing a lot of selective installation reverting the system back to a previous state with no luck . I have finally given up as it @ @ @ @ @ @ @ @ @ @ system . <p> I am downloading again the WIndows MultiPoint 2012 Premium image from Microsoft to see if this resolves the issue and downloading as well the Operating System English language version instead of the Spanish one . I will update this post once I figure out if any of those two options fix the issue . <p> UPDATE : The Spanish version of the system even after a new download continued to crash while in the English version even though I installed all the updates in one sweep it worked ! It seems somehow the Spanish packages are corrupted or something . I remember reading that you should always install a server in its English version ( the English patches used to be the fist released ) , but this is the year 2014 not 1994 . Although I am reluctant to install MultiPoint in English as many final users use the system , other benefits do seem to outweight this trouble . For now my solution to this problem is to install the system in English not Spanish. - <p> I think finally I figured out what 's going @ @ @ @ @ @ @ @ @ @ 2 VM on Windows Server 2012 R2 . As soon as I turned to a Gen 1 VM most of the errors were gone and I was able to install most of the updates without any issues ( note most but not all ) . So what I did next was reinstall the system but this time combine my two previous assumptions : Order of updates and Generation 1 for the VM and thus far the results have been great . <p> Below is the order of the updates I did : <p> Install the Cumulative Update ( it seems this time it will install a bunch of other things while at it ) <p> Install the only update left which is the . Net 4.5.1 <p> Install the optional windows update <p> From the two new updates that appear install the one smallest in size which is the oldest <p> Install the non Windows Update ( Flash , IE , etc ) <p> Below is a list of update installed by default by Windows Update and their status after the installation : <p> One of the issues I ran @ @ @ @ @ @ @ @ @ @ Request header or Cookie too large . " This was strange to me as my site had been working properly and this behavior was only experienced when doing a migration from a large site to our new deployment . As this indicates either the Request Header or the Cookie being sent across was too large for NginX to handle well , larger than what it is willing to handle . As you might know , NginX allows you to configure the sizes of a number of buffers for different parts of the communication . The answer though its simpler than understanding the cause ( or properly sizing the buffer ) . <p> The answer lies in using the **25;2090;TOOLONG directive . Its only valid inside the- http- or- server- contexts so be sure to place it where it makes the most sense for your application . <h> largeclientheaderbuffers <p> Directive assigns the maximum number and size of buffers for large headers to read from client request . <p> The request line can not be bigger than the size of one buffer , if the client send a bigger header nginx @ @ @ @ @ @ @ @ @ @ ) . <p> The longest header line of request also must be not more than the size of one buffer , otherwise the client get the error " Bad request " ( 400 ) . <p> Buffers are separated only as needed . <p> By default the size of one buffer is 8192 bytes . In the old nginx , this is equal to the size of page , depending on platform this either 4K or 8K , if at the end of working request connection converts to state keep-alive , then these buffers are freed . <p> What generally happens is that all the cookies used by your site get combined into one header and that may cause you to go over the default limit which is 8192 bytes . <p> This is a tricky error to catch as it only affects people who have cookies over the allotted capacity . Some of your users might experience issues when their cookie size exceeds 8k or like in my case , some pages that set additional cookie value might push you over the limit . In the first scenario once @ @ @ @ @ @ @ @ @ @ wont be able to use the site any more while other users might access the same pages with no problem while their cookies are under the limit . In the second scenario only certain pages under certain conditions might cause the cookie size to go over the limit . In my case when I was exporting and then importing a site I got this error . It did not happen for those sites which had few posts but for a large site I ended up encountering this error . Under the untrained eye this seems like a random issue ; At least you get the " 400 Bad Request Request header or Cookie too large " hint instead of a bogus error message . <p> In this case the solution is to increase the buffer size not the number of buffers . As the wiki indicates " The request line can not be bigger than the size of one buffer " which by default is 8k at the time of writing this post . 
@@45151309 @5151309/ <h> What is csrss.exe ? ( Client/Server Runtime Subsystem ) <h> What is csrss.exe ? <p> Many people while running through the Task Manager come across csrss.exe . With the amount of spyware , trojans and other malicious code running around the Internet it is always good to do a check every now and then that your computer is okay . Csrss.exe actually stands for Client/Server Runtime Subsystem and it is a component of the Microsoft Windows NT Operating System ( pretty much anything after 2003 has that ) . So what csrss.exe- does for you is handle the Graphical User Interface. - Practically- every Windows user nowadays has that process running . Because of that termination of the process would result in a system failure . <h> Is it safe ? <p> Generally yes . There are known viruses that take advantage of the name to disguise themselves ( csrss.exe ) . Known cases are the Nimda.E virus and the Nesky.ab &amp; VBMania . If you look at the properties of the process in the task manager you can see where the file is located . The one @ @ @ @ @ @ @ @ @ @ so if yours is there then you are probably fine . Having more than one instances is also possible ( although in most cases generally only one would be active ) . Say for example you have multiple users logged on into your machine . Each will be running a copy of- csrss.exe . In a Terminal Server environment this would be the case . If you are a home user if you use the switch user functionality in Windows then this could also be your scenario . <p> I would recommend regularly running an anti virus as they usually look out for threats like this one . Microsoft has a free anti virus I would recommend if you do n't  have a preference and like free : Microsoft Security Essentials ( is supported on- Windows- 7 , Windows- Vista , and- Windows- XP ) . <h> Can I remove it ? <p> Definitively NO . As mentioned above this is a critical Microsoft Windows process . This is not your average- unnecessary- service installed by your computer manufacturer that is just eating resources . You need this if you want your computer to function . 
@@45151314 @5151314/ <p> WordPress MultiSite : Remove render-blocking JavaScript : script LONG ... type=text/javascript So this is the latest in the " I want to improve my Google-s PageSpeed " saga . I was running Googles PageSpeed to determine what fixes I need to implement on my site to get a higher ranking and hopefully improve the rank it gives to my site . One <p> Importing WordPress sites : Why wont the Import tool finish ? Lately I have been consolidating WordPress sites into one single installation to simplify the management process but one of the issues I 've come across is that the Export / Import tool wont completely import an entire site . Sure , as a site grows the work of moving <p> How to : Connect to a MySql server using SSL from a WordPress Site Truly you could title this post simply How to Connect to a MySql server using SSL from a PHP application . The key is in the connection construct , which by default in WordPress it looks sort of like this : $this-&gt;dbh = mysqlconnect ( $this-&gt;dbhost , <p> What does Error @ @ @ @ @ @ @ @ @ @ means Lately I have been playing around with WP caching , web compression and the sort . As I was toying with some of the settings I realized my site was throwing error 330 when accessing it from Google Chrome ( IE meanwhile just said the site was down ) . It <p> How to : Improve the performance / speed of WordPress running in IIS To be entirely honest I am still struggling with this topic . However I have identified a few points that should help improve the performance / speed of a WordPress site running in IIS . Now , if a distinction should be made it would be <p> How to secure WordPress This is a coleciton of let 's call them best practices for securing your WordPress installation . I used to be very carefree when it came to the Internet . But as you have probably heard hacking is a much more common activity nowadays . Back in the day usually the main targets were large <p> How to : delete default admin user in Multisite As a Multisite user I have struggled with removing @ @ @ @ @ @ @ @ @ @ as to delete/remove that user . Unfortunately WordPress does not allow you to change your username , and in case of the user Admin everyone trying to hack into your <p> How to resolve Jetpack error : Invalid request , please go back and try again.Error Code : invalidrequest . Error Message : Mismatch in redirecturi . I have a multisite installation and thus far on all my sites I have been able to relink all of them to Jetpack . However , in one of my sites when I click the Link account <p> How to resolve : Fatal error : Call to undefined function nocacheheaders() in /wp-admin/admin.php on line 32 It seems this happens more often than one would expect . After it happened to me I was reading through a bunch of blogs and several people after an upgrade and my other scenarios en up encountering this issue . The first <p> How to get WooCommerce to automatically recreate pages As a WooCommerce user you are likely to encounter the need to recreate the pages the system creates during install at one point in your life . @ @ @ @ @ @ @ @ @ @ scenarios you are messing with the backend ( database ) and need to reinstall 
@@45151316 @5151316/ 55329 @qwx465329 <h> Tag Archive : Servers <p> Microsofts DHCP Failover One of the new cool features in Windows Server is DHCP Failover . Think of it as a way to replicate your DHCP across two servers in order to provide failover or load balancing a DHCP Service. - DHCP failover in Windows Server- 2012 is a new feature that enables two Microsoft DHCP servers to <p> How to : Delete a DHCP Failover relationship when the partner server is unreachable One of the latest issues I 've come across is removing a DHCP Failover relationship . In a previous article I referenced a new Windows Server feature ( Error while adding a second DHCP Server ) which is DHCP Failover . This is a pretty cool feature <p> Error 0x800f0922 while adding DHCP role on Windows Server If you are installing the DHCP server role on a Windows Server machine you might get the following 0x800f0922 error message : Feature Installation The request to add or remove features on the specified server failed . Installation of one or more roles , role services , or features failed . <p> @ @ @ @ @ @ @ @ @ @ running in IIS To be entirely honest I am still struggling with this topic . However I have identified a few points that should help improve the performance / speed of a WordPress site running in IIS . Now , if a distinction should be made it would be <p> How to : Install the Language Packs for SharePoint 2013 to deploy site on a different language While installing- SharePoint- 2013 I ran in to several issues that appear- to have corrected by installing SharePoint 2013 in English on a Windows 2012 Server in English ( same language ) . I should note I was trying to deploy a SP server in <p> What is www.msftncsi.com ? If you are running a firewall one of the most common sites you 'll run across is www.mstfncsi.com . This is because the url www.msftncsi.com is generally used by Windows machines to verity that there is network connectivity . A Windows machine ( Windows 8 , Windows 7 and even Windows Vista I believe ) will try to <p> What is : fsdmhost.exe Fsdmhost.exe is the executable file for the Microsoft- File- Server Data @ @ @ @ @ @ @ @ @ @ run into this process on a Windows 2012 server because it is in charge of data de-duplication . As most of you would have found out , this process can become resource intensive . The service uses <p> How to : Move Mailboxes to another Exchange Server 2013 So again , another fun day thanks to the folks at Microsoft ( or my karma I am really debating on that one ) . For some reason my CAS server- stopped working properly and after spending a whole day trying to figure out why I ca n't log back in I 
@@45151317 @5151317/ 55329 @qwx465329 <h> Category Archive : NginX <p> How to : Override a Location directive on NginX Sometimes when coding you are in need to re-use a lot of the logic across different systems but find yourself needing to overwrite some of that functionality on a special case . In this particular scenario I needed to overwrite a Location directive on NginX . What do I <p> How to : Move your NginX website to HTTPs- / SSL It comes at no surprise that a lot of people are looking into moving their sites to HTTPs due to recent events : Googles decision to give ranking points to sites that use SSL / HTTPs and eavesdropping by governments world wide . There are a number <p> Resolved : Blank pages when using NginX with php-fpm As of late I have been a bit busy so keeping up with updates to the web server has been pretty much neglected . Because of that I decided to switch to the nginx.org supported distribution to get the latest updates although that means a distribution without any <p> Changes with- NginX- 1.6 Below @ @ @ @ @ @ @ @ @ @ NginX to version 1.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update makes the development / mainline version 1.5.13 part of the stable branch. - Because of this several new features are included and we highly suggest people upgrade to <p> Changes with- NginX- 1.7 Below are the list of changes from the 1.5.13 release of NginX to version 1.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several new Features- so updating to this version is not considered urgent/critical . Changes with nginx 1.7.0 24 Apr 2014 * ) Feature : backend SSL certificate verification . * ) <p> How to : Improve SSL performance on NginX You would be surprised but a lot of people face SSL performance issues when using NginX . I recently deployed SPDY over SSL for my sites and came to realize that SPDY was in fact much slower than using standard HTTP . I proceeded to leave SSL alone and see <p> Changes with- NginX- 1.5.13 Below are the list of @ @ @ @ @ @ @ @ @ @ . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several bugfixes but no critical- security updates . Changes with nginx 1.5.13 08 Apr 2014 * ) Change : improved hash table handling ; the default values of the " variableshashmaxsize " <p> Changes with- NginX- 1.5.12 Below are the list of changes from the 1.5.11 release of NginX to version 1.5.12 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes a security fix so it is considered an urgent update to deploy . Changes with nginx 1.5.12 18 Mar 2014 * ) Security : a heap memory <p> Nginx : How to correctly use Time and Size postfix / parameters I was struggling on figuring out why my cache size was not growing as intended . I had setup my configuration so that pages would be cached for a really long time but refreshed often . The idea being able to cache my entire site but <p> How to : Use NginX as a Forward Proxy server Generally @ @ @ @ @ @ @ @ @ @ is what it was designed for . However , after working with NginX for sometime now , I realized conceptually a proxy server could work both ways , right ? The thought is an interesting one but its market might be 
@@45151320 @5151320/ <h> Replacing Self Signed Remote Desktop Services Certificate on Windows <p> So one of the reasons why we moved from a . local domain environment to a corp.Bauzas.com Active Directory domain name was so that we could use a public CA certificates for Remote Desktop Services . We used to rely on self signed certificates and then moved to using the corporate CA but when using devices that do not have the enterprise CAs root certificate installed we struggled a little . Moving to public CA certificates seemed to make sense and does let us avoid trouble like making our CA publicly accessible , etc . Regardless , the next challenge we faced was that I have only used the " Remote Desktop Session Host Configuration " to select which certificate to use for Remote Desktop Services . To my shock ( I guess not really ) this tool is not readily available with Windows Server 2012 or a Windows Workstation . You can probably install it as you enable the appropriate role/feature but that seemed like too much work . Instead it seems you can simply use the @ @ @ @ @ @ @ @ @ @ use in the right folder . I recommend you go the normal route before you try to simply force another certificate into the store . <p> In the Select Computer dialog box , select whether you want to connect to the local computer or to another computer . If you select Another computer , either type in the name of the computer or use Browse to search for the computer . <p> Click OK . <p> In the Add or Remove Snap-ins dialog box , click OK . <p> If all that fails then here is how you replace the certificate on the certificate store : <h> NM <p> Is there any way to completely disable the creation of self-signed certificates in Windows 2012r2 ? If you delete the self-signed certificate , when you restart the terminal server services , it recreates it regardless of whether you have a registered one or not installed . 
@@45151323 @5151323/ 55329 @qwx465329 <h> Category Archive : Windows Servers <p> The IO operation at logical block address 0 for Disk 7 was retried . When working with Multipath IO ( MPIO ) like when using iSCSI it is possible you might run across the message : The IO operation at logical block address X for Disk X was retried . This initially looks like trouble although truly it is more <p> How to : Resolve error " Computer/Name Domain Changes . The following error occurred attempting to join the domain : The requested resource is in use . " One of my latest missteps was not completing the migration of all servers to the new domain name when I performed an Active Directory Domain Name change . Well , one of the unintended consequences <p> How to : Rename an Active Directory Domain Name There are many reasons why you might want to rename an Active Directory Domain Name . Microsoft names situations were there is a reorganization of the enterprise which could simply be internal or due to an adquisition , etc . In my case I have been @ @ @ @ @ @ @ @ @ @ Size the Staging Folder and Conflict and Deleted Folder The Distributed File System Replication ( DFSR ) service is a new multi-master replication engine that is used to keep folders synchronized on multiple servers . Replicating data to multiple servers increases data availability and gives users in remote sites fast , reliable access to files . <p> Replacing Self Signed Remote Desktop Services Certificate on Windows So one of the reasons why we moved from a . local domain environment to a corp.Bauzas.com Active Directory domain name was so that we could use a public CA certificates for Remote Desktop Services . We used to rely on self signed certificates and then moved to <p> Microsofts DHCP Failover One of the new cool features in Windows Server is DHCP Failover . Think of it as a way to replicate your DHCP across two servers in order to provide failover or load balancing a DHCP Service. - DHCP failover in Windows Server- 2012 is a new feature that enables two Microsoft DHCP servers to <p> How to : Delete a DHCP Failover relationship when the partner server is unreachable One of @ @ @ @ @ @ @ @ @ @ Failover relationship . In a previous article I referenced a new Windows Server feature ( Error while adding a second DHCP Server ) which is DHCP Failover . This is a pretty cool feature <p> Error 0x800f0922 while adding DHCP role on Windows Server If you are installing the DHCP server role on a Windows Server machine you might get the following 0x800f0922 error message : Feature Installation The request to add or remove features on the specified server failed . Installation of one or more roles , role services , or features failed . <p> Windows Server 2012 " File System Resiliency ( ReFS ) or Data Deduplication ( NTFS ) ? One of the features of Windows Server 2012 R2 that Ive been doing research as of late is the new file system Resilient File System ReFS ) . The big question as you can imagine is whether to use it or not . What became clear <p> How to force an authoritative and non-authoritative synchronization for DFSR-replicated SYSVOL ( like " D4/D2 " for FRS ) Fixing Broken SYSVOL Replication Consider the following scenario : You want @ @ @ @ @ @ @ @ @ @ controller . In the File Replication Service ( FRS ) , this was controlled through the- D2- and- D4- data values for the- Burflags- registry values , but these values do 
@@45151326 @5151326/ 55329 @qwx465329 <h> Tag Archive : Microsoft Exchange Server <p> How to : Prevent items to be Deleted from an Exchange 2013 / Online Mailbox The answer to this lies in what you are looking to accomplish . The Litigation hold allows the user to delete messages and other items from the mailbox but they remain in the server so that they can be searched and discovered <p> How to : Delete Microsoft Online Windows Azure " phantom " users I really have no better way to describe this issue than " phantom users " . Background : I deployed an Exchange 2013 organization and tried to use the Microsoft Online services . I did Active Sync , assigned licenses and tried to move over my users Mailboxes . Unfortunately back then <p> How to : Connect to a Remote Office 365 Tenant PowerShell Session Probably at some point of using the new Microsoft-s Office 365 offering you-ll need to run some PowerShell commands against your Tenant sesison . Below I have gather a few commands to reference when working on a remote Office 365 Tenant PowerShell Session : @ @ @ @ @ @ @ @ @ @ to Configure TMG for Office 365 ( Exchange ) Hybrid deployments The purpose of this article to give some general guidance on how to configure TMG for use with Office 365 Exchange related components . The idea is to give some general guidance mainly around authentication settings needed on the TMG rule <p> Why is my Exchange 2013 Server generating a lot of emails from email protected and email protected ? At first I was very worried about this . My servers were sending out some- SPAM- ( forgot to turn on the Sender I 'd so a few of Non delivery emails were being sent out , whoops ) and there had been some port scans detected <p> How to : Publish Exchange Server 2010 with Forefront UAG and Forefront TMG I 've been trying to publish Exchange Server 2013 with Forefront TMG with no avail . However , I did find a good guide on how to publish Exchange Server 2010 with TMG so I thought I would share . I cant find the link to the <p> How to : Use the Certificate Enrollment MMC in the TMG host @ @ @ @ @ @ @ @ @ @ snap-in and/or try to perform- a certificate auto-enrollment in your localhost/TMG server you 'll most likely run into an error message- on-screen- that reads " RPC- failure " . If you try requesting a certificate on other computers joined to your <p> How to : Move Mailboxes to another Exchange Server 2013 So again , another fun day thanks to the folks at Microsoft ( or my karma I am really debating on that one ) . For some reason my CAS server- stopped working properly and after spending a whole day trying to figure out why I ca n't log back in I <p> How to : Manage the Certificate Store on your local machine using the command prompt or PowerShell It seems that as of late I am playing a lot with certificates in order to- authenticate traffic across the network . Some of the most useful shell commands I have found are listed below , hopefully theyll help you manage your 
@@45151329 @5151329/ 55329 @qwx465329 <h> Pro Tip : Upgrade to PHP7 <h> Pro Tip : Upgrade to PHP7 <p> As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got it quite working . If you also use WordPress you probably also ran into some quirks when using hhvm . Because of that , we decided to remain with PHP 5. x . But the php developer community took notice , and decided to make the new version a worthy competitor of hhvm and the like . So here we are , with PHP 7 already released for sometime we decided to dive in and see what we got , and we liked it . <p> So how faster is php7 when compared to php5 ? Much I would say . I woud love to have tons of metrics to share but we do n't  collect that many @ @ @ @ @ @ @ @ @ @ few where you can see what were talking about : <p> 1 ) Web Transaction Time <p> We had an average web transaction time of 350 ms with the low end around 300 ms and the high end around 500 ms . Php processing time took around 100 ms per request , so most of the time was spent doing web external calls . Fast forward to php 7 and we got an average web transaction time of 140 ms with the low end close to 100 ms and the high end around 230 ms . Basically we got it down to half ! the php processing time ranges from 37.5 ms to 87.5 ms , but most is basically at 50 ms . This means that overall we were able to cut our web processing times in half both as a total and as just php processing times . <p> On a site note : We also saw a drop in our throughput with the same number of visitors ( thereabouts ) so we are not sure what is driving that and if that is having an impact . @ @ @ @ @ @ @ @ @ @ was a caching service that was preloading pages to accelerate the site although seems at the end of the day it really was slowing it down . We also implemented better security at the login page which resulted in lower number of requests . So at the end of the day we would need to go back to php5 to make a fair comparison seeing we did other changes that also resulted in this positive results . <p> We are also observing a higher number of php warnings , hopefully the wordpress team will address those soon in a release . <p> 2 ) Server usage <p> Overall our server usage also dropped . However , we also upgraded to a newer version of the OS which could also have an impact . Regardless , we are observing 1/3 memory usage overall like 1/5- CPU usage although we never used much to begin with . <p> 3 ) Front end <p> This is an even more subjective perspective . I have though , had a very positive impression using the web application . The time it takes for it to @ @ @ @ @ @ @ @ @ @ means something if a human being can notice it . Remember that although the page is generated in milliseconds on the web server , on average our pages are fully rendered after 9 seconds on the users browsers . Most of the time spent doing that is precisely page rendering right after COM processing . Most of our visitors use Google Chrome . Our faster load times are round across the norther part of the world ( North America , Europe , Russia , while the slowest load times are found in Argentina , India , China and surprisingly Australia . We do n't  receive visitors from all over the world , there are hardly any from Africa for example so take this information lightly . <p> Well , that pretty much ends the review of php7 from our side . We are really excited to see such positive results and glad the technology stack we use finally is mostly compatible with it . We look forward this improving our readers experience and satisfaction . Thanks ! 
@@45151330 @5151330/ <h> Does Jetpacks Photon damage Image SEO ? <p> Thus far I have n't been able to find concrete proof of this but it probably does have an impact . Why is that ? Because the URL being served is from a different site than yours which results in Google or other search engines associating that picture to them not you . Obviously there are lots of people out there using CDNs so I believe it is a matter of time until Google and the other search engines recognize this and properly address this issue . In the mean time from what I have been able to gather from other users in the Internet is that there is an impact to the number of visitors and page rank . <p> My recommendation therefore is that until Google and other search engines are smarter on how they handle CDNs it is best to host it yourself . Also , many CDNs like Microsoft *Windows Azure* allow you to have your own virtual machine which uses the same hard ware they host your files so there is little incentive now to use @ @ @ @ @ @ @ @ @ @ You could also do that yourself but then it becomes a matter of costs ) . 
@@45151334 @5151334/ 55329 @qwx465329 <h> What is : fsdmhost.exe <h> What is : fsdmhost.exe <p> Fsdmhost.exe is the executable file for the Microsoft- File- Server Data Management Host process . Thus far a lot of people run into this process on a Windows 2012server because it is in charge of data de-duplication . As most of you would have found out , this process can become resource intensive . The service uses a lot of RAM , CPU and Disk- resources as it is computing the de-duplication chunks while reading and writing to the hard drive . It is perfectly normal for this process to be one of your most demanding at times . <p> If this process is continuously demanding most of your serve resources , look into improving the de-duplication performance by better adjusting your de-duplication settings . Eventually the serve should also normalize as the initial- de-duplication faces come to an end . Regardless , optimal settings for your system should be in place to avoid performance issues . 
@@45151335 @5151335/ 55329 @qwx465329 <h> Category Archive : Processes <p> Exchange 2010 store.exe service Store.exe is a service that is part of Microsoft Exchange server 2010 and is found on your mailbox server . Many people come across this process as it is known to use a lot of memory from the host machine . If you are running an Exchange server on your machine you <p> What is csrss.exe ? Many people while running through the Task Manager come across csrss.exe . With the amount of spyware , trojans and other malicious code running around the Internet it is always good to do a check every now and then that your computer is okay . Csrss.exe actually stands for Client/Server Runtime Subsystem and it is <p> Short description : Microsoft Search Component for SQL and Exchange Servers Long description : A process that is used by SQL server and Exchange server for the creation on indexes to hasten search . On my server it looks like it consumes a lot of memory and I 've read high CPU use is also likely specially if there 
@@45151336 @5151336/ 55329 @qwx465329 <h> Category Archive : PHP <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got it <p> Resolved : Ubuntu php5-fpm throws error " unknown instance " on service reload Rather short post this time around . It turns out there is a known issue that when you perform a service reload on php5-fpm you might encounter yourself with a bit of a problem . The use of reload is meant to result in a reloading of 
@@45151337 @5151337/ <h> What are Hard Faults per Second ? <p> As anyone who has taken a look at the Task Manager and then into the Performance Monitor , one of the two key metrics " Hard Faults per Second " is an important one to consider . I-ve never been too familiar with this term so I finally decided to research what it really meant ( the name has a- connotation- that the least the better you are ) . <p> Apparently Page faults are nowadays known in Windows as Hard faults . A page fault is when a memory address is no longer in the main memory and Windows needs to get it from the Hard drive rather than from the RAM . This is normal as usually computers use more memory than that which is physically available and it swaps the information around to the hard drive based on what it needs to access more often , etc . So a hard fault is simply the- occurrence- in which the OS has to access secondary memory to retrieve information ( this obviously has a higher- performance- cost than @ @ @ @ @ @ @ @ @ @ have no idea what amount of hard faults start being considered a performance concern . Obviously if you are looking for good performance you want to have 0 hard faults per second as much as possible . This is usually the case when you have a decent % of RAM left available . You can opt for limiting the amount of RAM being used by applications that allow it or physically adding more . As one would suspect , a computer with plenty of RAM will generally exhibit no page faults . A way to possibly measure the performance impact this page faults are having on your computer is to take a look at the performance monitor . There if you observe the hard drive section , take a look to see how much activity the page file is generating . If this one is one of your most read/write files- consistently- then you have a- potentially- considerable benefit of upgrading your RAM . In my case because of the heavy hard drive use by the applications I strive to maintain enough RAM to avoid page faults . 
@@45151342 @5151342/ <h> How to : Prevent CrashPlan Pro from shutting down abruptly <p> Weve been using- CrashPlan Pro as our backup solution for offsite backups . Because of the time it takes to upload the files over the internet we have been doing backups of different folders based on their importance . Recently we ran into the issue that when performing a backup the Crashplan- application would just close itself . When launched it would remain- open anywhere from a few seconds to a few minutes . Obviously we did n't  experience this behavior since day 1 . All the backups were performed- as expected but we started having issues once we started programming those larger backups with big VHDs- files . So something with the larger backup was causing CrashPlan- to terminate- abruptly . The CrashPlan service would still be running but the backup jobs would actually be stopped potentially creating the issue that if left unattended no backups would be performed . <h> The cause <p> After reaching out to CrashPlan- support their agent was very polite and in a few minutes of troubleshooting suggested that the memory @ @ @ @ @ @ @ @ @ @ Their recommendation is about 1gb of RAM- per terabyte of storage backup you need . With 3.5 TB needed to be backed up I guess it was obvious what the issue was here . <h> The solution <p> Running Notepad or any other text-editor- as an Admin , edit the CrashPlan engines CrashPlanService.ini file to allow it to use more java memory : <p> Go to File &gt; Open , and navigate to C : Program **34;2172;TOOLONG ( note that you will have to change the option from . txt- files to all files in the file selection screen in order to see the CrashPlanService.ini ) <p> Find the following line in the file : <p> -Xmx512m <p> Edit to something larger such as 640 , 768 , 896 , or 1024 . E.g. : <p> -Xmx1024m <p> This sets the maximum- amount of memory that CrashPlan- can use . CrashPlan will not use that much until it needs it . I would recommend starting out setting it to 1024 , and go higher only if you continue experiencing problems . You can set it as high as 2048 on @ @ @ @ @ @ @ @ @ @ <h> Shmuli <p> wow , I have been racking my brain looking at ways to stop it from crashing , thinking it was my hard drive that has been acting up , but after changing the number to 2048 in the . ini file it has so far not crashed ! Hope that solves my problem . 
@@45151344 @5151344/ <h> What is : Circular Logging in Microsoft Exchange Server <p> I had some issues with an Exchange Server and had to perform a restore operation . While doing so I noticed that even though the user mailboxes were not huge , the size of the mailbox store was . As I read more about it , Exchange maintains the Mailbox Database file and additional files which are the log files of everything that goes on . Simply put , you could almost recreate the database from the log files if it got corrupted , etc. if you perform a backup with a tool that does support Exchange 2013 , it will take care of removing the log files so the space used on the hard drive does not grow out of control . If you are using a less fancy tool then you are probably backing up twice the data you should be . So here is where the whole notion of circular logging finally comes in : <p> The Microsoft Exchange transactional logging as explained above logs every single transaction that is performed on the database . @ @ @ @ @ @ @ @ @ @ used to conserve hard disk space by overwriting individual log files keeping the transactional log to a minimum . By default this option is disabled and there is no limit as to how large the mailbox database can get . If you enable it , it sets a limit to the size of the log and once that limit has been reached the first log file is overwritten . Think of it as the name implies , you have a circle of logs which are empty and you go around until you get back to were you started and begin overwriting the oldest logs , hence the name circular logging . <p> Now , when you perform a backup it clears the logs . The rationale behind this is that if you were to perform a restore you will need to rely only on the logs that were created after the backup to perform a hard restore of the database . <p> So thus far seems really cool how the logging works right ? So why would someone turn on circular logging ? It turns out Circular logging has been @ @ @ @ @ @ @ @ @ @ disk space was hard to come by so you might want to conserve disk space and take a chance . Another possibility is that your backup engine does not have an Exchange plugin so those logs keep growing and growing . By enabling circular logging you take a risk as you do n't  know when you backed up and how that maps to the logging files , but if you are comfortable with your backup infrastructure and savvy you could deal with it . 
@@45151345 @5151345/ 55329 @qwx465329 <h> Tag Archive : SQL <p> SQL Server 2012 : How to copy a database from one server to another using the Copy Database Wizard I was provisioning my SQL box when I found I didn-t need an already provisioned Windows Server box I had used for Exchange 2013 . Lazy enough I installed SQL Server 2012 on my already installed Windows box . <p> How to : Resolve error The EXECUTE permission was denied on the object procputObjectTVP , database SharePointConfig when using SharePoint 2013 My saga with SharePoint is still far from over apparently . The latest and greatest error message found on the event log- reads " How to : Resolve error The EXECUTE permission was denied- on the object procputObjectTVP , database SharePointConfig when <p> How to Configure TMG logging to use a central SQL Server store The default log settings for Forefront TMG are set to a local Microsoft SQL Server Express 2008 SP1 database . During the Forefront TMG installation a local Microsoft SQL Server Express database will be installed . If you want to change this local SQL Server <p> @ @ @ @ @ @ @ @ @ @ : " A Restart from a Previous Installation is Pending " or- " There are pending actions from a previous install " or " Setup can not continue because a restart from a previous installation or update is pending " keeps showing up . In occasions you 'll find yourself unable to install software receiving an 
@@45151347 @5151347/ <h> What is : " Wasted Memory " ? ( Zend OpCache &amp; Memcached ) <p> As I have been trying to fine tune all the performance settings on our new Linux box there was a new term we came across : " Wasted Memory " . Zend OpCache has a setting that will cause the restart of your OpCache if the wasted memory exceeds a threshold . I am no expert but that would result in losing all your cached information in OpCache . <p> So , what is " Wasted Memory " ? From what I have read it is memory that has been allocated to the service but is not in use . As your memory needs increase so does the memory allocated to you . However , once you release information from the cache that memory is still in use . Wasted Memory is therefore memory you used in the past that right now contains information that is of no use therefore it is considered wasted . <p> Same goes for Memcache. - Wasted memory is memory allocated in RAM so it can not be @ @ @ @ @ @ @ @ @ @ for Memcache although not used at the moment . <p> So why would there be a function to restart the service you ask ? Well , say you had limited memory , so by restarting the service you would free all the memory again . The downside is that your currently cached information is lost but the advantage is that you start fresh . Theoretically this would not happen often . You can adjust the value of % of Wasted Memory to better suit your needs , perhaps you want to be more tolerant than the 5% that comes out of the box or perhaps you do n't  want to use it at all . Its all up to you ! <h> Observed Behavior <p> I decided to play a little with OpCache to see if a restart is such a bad thing or not . Apparently about 60% of the cache I had got re-established in an instant and another 20% in the next few seconds . Probably it tries to preload as much as possible when it starts , not sure , but this is a good sign @ @ @ @ @ @ @ @ @ @ waste memory by default but a restart seems not a bad thing now . I am going to see how it behaves once there is no free memory and it needs to start using the wasted memory . <p> UPDATE 1 : I already managed to deplete the available memory but still no dice . Wasted memory continues to slowly increase while used memory is decreasing which I assume is not a good sign . To further indicate this might be a problem the % of hits has gone down . I went ahead and further restricted the amount of memory available to OpCache . The Memory quickly filled and the waste memory slowly grew . <p> I researched a little bit more and found the following information : <p> memoryusage array ; contains information about Optimizer+ memory usage , with the following keys:usedmemory integer ; bytes of memory usedfreememory integer ; bytes of memory available for cachewastedmemory integer ; bytes of memory used by invalid or outdated code Wasted memory is reclaimed when the accelerator is reset , or when the percentage of wasted memory reaches the value of @ @ @ @ @ @ @ @ @ @ cache memory out of total memory available <p> As noted- Wasted memory is reclaimed when the accelerator is reset , or when the percentage of wasted memory reaches the value of the maxwastedpercentage directive ( which triggers a reset ) . I do n't  understand why it works that way but who am I to argue . Back to **27;2242;TOOLONG = 5. - - Also another thing I noted is the- opcache.usecwd ( default " 1 " ) setting . I recommend leaving it enabled as having multiple WordPress installations on your server would result in collisions ( or multiple anything on server as the file names would be the same . ) I 've included below the most common directives , their definitions and default values : <p> Configuration Directives ------------------------ opcache.enable ( default " 1 " ) OPcache On/Off switch . When set to Off , code is not optimized and cached. opcache.enablecli ( default " 0 " ) Enables the OPcache for the CLI version of PHP . It 's mostly for testing and debugging . **25;2271;TOOLONG ( default " 64 " ) The OPcache shared memory storage @ @ @ @ @ @ @ @ @ @ code in Mbytes. **29;2298;TOOLONG ( default " 4 " ) The amount of memory for interned strings in Mbytes. **27;2329;TOOLONG ( default " 2000 " ) The maximum number of keys ( scripts ) in the OPcache hash table . The number is actually the first one in the following set of prime numbers that is bigger than the one supplied : 223 , 463 , 983 , 1979 , 3907 , 7963 , 16229 , 32531 , 65407 , 130987 , 262237 , 524521 , 1048793 . Only numbers between 200 and 1000000 are allowed . **27;2358;TOOLONG ( default " 5 " ) The maximum percentage of " wasted " memory until a restart is scheduled . opcache.usecwd ( default " 1 " ) When this directive is enabled , the OPcache appends the current working directory to the script key , thus eliminating possible collisions between files with the same name ( basename ) . Disabling the directive improves performance , but may break existing applications . **26;2387;TOOLONG ( default " 1 " ) When disabled , you must reset the OPcache manually or restart the webserver for @ @ @ @ @ @ @ @ @ @ of the check is controlled by the directive " opcache.revalidatefreq " . opcache.revalidatefreq ( default " 2 " ) How often ( in seconds ) to check file timestamps for changes to the shared memory storage allocation . ( " 1 " means validate once per second , but only once per request . " 0 " means always validate ) **28;2415;TOOLONG ( default " 2 " ) Prevents caching files that are less than this number of seconds old . It protects from caching of incompletely updated files . In case all file updates on your site are atomic , you may increase performance setting it to " 0 " . opcache.revalidatepath ( default " 0 " ) Enables or disables file search in includepath optimization If the file search is disabled and a cached file is found that uses the same includepath , the file is not searched again . Thus , if a file with the same name appears somewhere else in includepath , it wo n't be found . Enable this directive if this optimization has an effect on your applications . The default for this @ @ @ @ @ @ @ @ @ @ . opcache.savecomments ( default " 1 " ) If disabled , all PHPDoc comments are dropped from the code to reduce the size of the optimized code . Disabling " Doc Comments " may break some existing applications and frameworks ( e.g. Doctrine , ZF2 , PHPUnit ) opcache.loadcomments ( default " 1 " ) If disabled , PHPDoc comments are not loaded from SHM , so " Doc Comments " may be always stored ( savecomments=1 ) , but not loaded by applications that do n't need them anyway . opcache.fastshutdown ( default " 0 " ) If enabled , a fast shutdown sequence is used for the accelerated code The fast shutdown sequence does n't free each allocated block , but let 's the Zend Engine Memory Manager do the work . **26;2445;TOOLONG ( default " 0 " ) Allow file existence override ( fileexists , etc. ) performance feature . **25;2473;TOOLONG ( default " 0xffffffff " ) A bitmask , where each bit enables or disables the appropriate OPcache passes opcache.inheritedhack ( default " 1 " ) Enable this hack as a workaround for " ca n't redeclare @ @ @ @ @ @ @ @ @ @ DECLARECLASS opcodes use inheritance ( These are the only opcodes that can be executed by PHP , but which may not be executed because the parent class is missing due to optimization ) . When the file is loaded , OPcache tries to bind the inherited classes by using the current environment . The problem with this scenario is that , while the DECLARECLASS opcode may not be needed for the current script , if the script requires that the opcode at least be defined , it may not run . The default for this directive is disabled , which means that optimization is active . In php-5.3 and above this hack is not needed anymore and this setting has no effect . opcache.dupsfix ( default " 0 " ) Enable this hack as a workaround for " Can not redeclare class " errors . **25;2500;TOOLONG The location of the OPcache blacklist file ( wildcards allowed ) . Each OPcache blacklist file is a text file that holds the names of files that should not be accelerated . The file format is to add each filename to a new line @ @ @ @ @ @ @ @ @ @ a file prefix ( i.e. , /var/www/x blacklists all the files and directories in /var/www that start with ' x ' ) . Line starting with a ; are ignored ( comments ) . Files are usually triggered by one of the following three reasons : 1 ) Directories that contain auto generated code , like Smarty or ZFW cache . 2 ) Code that does not work well when accelerated , due to some delayed compile time evaluation . 3 ) Code that triggers an OPcache bug . opcache.maxfilesize ( default " 0 " ) Allows exclusion of large files from being cached . By default all files are cached. **25;2527;TOOLONG ( default " 0 " ) Check the cache checksum each N requests . The default value of " 0 " means that the checks are disabled . Because calculating the checksum impairs performance , this directive should be enabled only as part of a debugging process . **27;2554;TOOLONG ( default " 180 " ) How long to wait ( in seconds ) for a scheduled restart to begin if the cache is not being accessed . The @ @ @ @ @ @ @ @ @ @ may be a problem with a process . After this time period has passed , the OPcache assumes that something has happened and starts killing the processes that still hold the locks that are preventing a restart . If the log level is 3 or above , a " killed locker " error is recorded in the Apache logs when this happens . opcache.errorlog OPcache errorlog file name . Empty string assumes " stderr " . **25;2583;TOOLONG ( default " 1 " ) All OPcache errors go to the Web server log . By default , only fatal errors ( level 0 ) or errors ( level 1 ) are logged . You can also enable warnings ( level 2 ) , info messages ( level 3 ) or debug messages ( level 4 ) . **28;2610;TOOLONG Preferred Shared Memory back-end . Leave empty and let the system decide . opcache.protectmemory ( default " 0 " ) Protect the shared memory from unexpected writing during script execution . Useful for internal debugging only . opcache.restrictapi ( default " " ) Allows calling OPcache API functions only from PHP scripts which @ @ @ @ @ @ @ @ @ @ " means no restriction . opcache.mmapbase Mapping base of shared memory segments ( for Windows only ) . All the PHP processes have to map shared memory into the same address space . This directive allows to manually fix the " Unable to reattach to base address " errors . 
@@45151348 @5151348/ <p> The Joule , Dallas 1530 Main Street , - Dallas , - TX- 75201 Phone : - ( 214 ) 748-1300 Site : - http : **25;2640;TOOLONG My Rating : 4 1/2 stars Category : 4 Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to <p> Cane Rosso Category : - Pizza 2612 Commerce St Dallas , - TX- 75226 Neighborhood : Deep Ellum ( 214 ) 741-1188 www.ilcanerosso.com Menu Yelp : - http : **36;2667;TOOLONG My Rating : 4 1/2 stars Summary : One of the best pizza place Ive been in . Great variety and different from the ordinary , great quality of ingredients and superior taste . It is not expensive or cheap but their rating makes it <p> Earn 50 bonus miles per check in ( per day ) via Facebook or Foursquare , within 2 miles of approved airports within the 50 states ( http : @ @ @ @ @ @ @ @ @ @ confirmation message to the social media chosen , containing information about nearby retailers " you can opt out of these messages if you 'd like <p> After recently visiting Disneyland there were a couple of noteworthy tips to take into consideration when visiting this attraction in Anaheim , CA . First of all I would highly recommend staying in a hotel within the property . Not only are the usually nicer but the- convenience- they offer being right in the park as well as some added 
@@45151353 @5151353/ 55329 @qwx465329 <h> How to : Create a generic method constraining T to an Enumeration <p> Although I believe that as part of the . Net framework version 4 you can specify the Enumeration type that you need when you are creating a generic method , this is not available in the . Net framework 3.5 ( or in previous releases for that matter ) . The problem then becomes that you do n't want to have a method that allows any type of object to be passed to it ( the main reason why you 're choosing generics right ? ) so how can you restrict it as much as possible to guarantee you 're getting the enumeration you want ? <p> What I found is that you can restrict the generic type to be of type struct ' . This of course is not perfect as it could not only be any kind of enumeration , but it also could be a value type ( int , bool , etc. ) or a custom structure that uses struct . Further research reveals that Enum implements an interface @ @ @ @ @ @ @ @ @ @ further restricts the object that can be passed to the generic method/class almost guaranteeing an enumeration . <p> At this point I 'm happy but for those who want to take it a step further you can perform a check within the method like the one below as a final gate : <p> if ( ! typeof(T).IsEnum ) throw new ArgumentException ( " T must be an enum type " ) ; <p> This should help you get the job done . Unfortunately at this point you can receive any given enum and not one of a particular type , but again , you can run code similar to the one above and then throw an exception or return a default value ' if it is not what you were expecting . Again , not ideal but it 's the best I have at this point . So what we have thus far looks more or less like this : <p> public T MyMethod&lt;T&gt; ( ) where T : struct , IConvertible if ( ! typeof(T).IsEnum ) throw new ArgumentException ( " T must be an enum type " ) ; 
@@45151357 @5151357/ 55329 @qwx465329 <h> Category Archive : Networking <p> What is 1e100.net ? Perhaps many of you have come across the domain 1e100.net . As I was working with custom search I noticed my firewall was blocking traffic so the results were not coming up . A little bit of digging revealed the following : Denied Connection Log type : Web Proxy ( Forward ) Status : 12227 The name on the <p> The reserved IP ranges for private networks are the following : 10.0.0.1/8 through 10.255.255.254/8 172.16.0.1/12 through 172.31.255.254/12 192.168.0.1/16 through 192.168.255.254/16 This information is very useful specially when working with routing tables as you may want to know what subnet mask to use when and the size of your private network based on its address class , etc. 
@@45151358 @5151358/ 55329 @qwx465329 <h> What is www.msftncsi.com ? <p> If you are running a firewall one of the most common sites you 'll run across is www.mstfncsi.com . This is because the url www.msftncsi.com is generally used by Windows machines to verity that there is network connectivity . A Windows machine ( Windows 8 , Windows 7 and even Windows Vista I believe ) will try to get a file from a web server to determine if there is network connectivity . There are two tests that a Microsoft Windows Client will perform to determine if there is outside connectivity to the Internet . The first one of such tests is being able to resolve msftncsi.com successfully and then retrieve a text time . The second test is more basic and it tries to retry said file directly via an IP address instead of using your DNS server to resolve those addresses . This way Windows can determine if you are having issues with your DNS , router , etc . If it determines successfully that there is network connectivity you 'll see on your network card status indications if there is @ @ @ @ @ @ @ @ @ @ , I have only IPv4 Connectivity as my Internet Service Provider ( ISP ) does not support IPv6 yet . <p> Below are a few details of the URLs that are used . As you might imagine there is an ipv6 and an ipv4 address : 
@@45151360 @5151360/ <h> How to : Uninstall the New Relic agent for a Web App <h> How to : Uninstall the New Relic agent for a Web App <p> The New Relic agent is a powerful - lightweight agent that helps you monitor the performance of your Web Application . Thanks to it you can identify potential problems , measure performance , and deliver a better user experience . So why would you want to uninstall it ? Well , in my case I was playing with a bunch of plugins that created a bunch of " apps " out of my main web app . Long term this is desirable as I can measure the performance of my overall deployment but also subsite by subsite I can see how the server is behaving . The only issues is that as I was playing with this functionality I switch the naming of the applications and different plugins created different apps . In one case each subsite had 3 sites : the regular one , the admin one , and I believe an ajax one too much for me . So at @ @ @ @ @ @ @ @ @ @ flavors of each . a nightmare . Now , New Relic wont let you delete an application that is active and it wo n't merge sites so the only option is to uninstall the New Relic plugin for like an hour so that their site let 's you delete your apps . <h> Uninstalling the agent <p> Note : - Make sure your applications are not reporting any data ( their " traffic light " status is gray ) before uninstalling the agent . 
@@45151361 @5151361/ <h> How to : Enable Shadow Copy or Previous Version in Windows 2012 R2 ? <h> How to : Enable Shadow Copy or Previous Version in Windows 2012 R2 ? <p> I recently had a bit of an issue with a program . It uses an access database and one of the employees modified it but we needed to revert the changes . Seemed simple enough , just reach out to pick a previous version from the server share and call it a day . I unfortunately realized that we had not activated Shadow Copy on the server . Enabling Shadow Copy allows you to configure how often the server takes " snapshots " of the files and allows you to go back in time and see the versions as they are modified . It is a pretty nifty feature that you need to activate manually . Word of caution , it does consume disk space . <h> So , moving on to the How To : <p> In the console tree , right-click- Shared Folders , click- All Tasks , and then click- Configure Shadow Copies . <p> @ @ @ @ @ @ @ @ @ @ want to enable Shadow Copies of Shared Folders for , and then click- Enable . <p> You will see an alert that Windows will create a shadow copy now with the current settings and that the settings might not be appropriate for servers with high I/O loads . Click- Yesif you want to continue or- No if you want to select a different volume or settings . <p> To make changes to the default schedule and storage area , click- Settings . <p> As easy as that ! I suggest you only enable Shadow Copies of Shared Folders for User shares . It is better to do a backup of a SQL database than having it perform Shadow Copies , this is really not a backup solution . Also , Shadow Copies is not recommended for IO intensive loads . 
@@45151362 @5151362/ 55329 @qwx465329 <h> Category Archive : Crashplan <p> How to : Speed up CrashPlan by Reassigning Cache Folder to a Different Directory I 'm guessing if you 've bumped into this article is because you have deployed CrashPlan and you 're looking up for ways to speed it up . I have been a customer for several years now and I am happy with the solution from a <p> How to : Prevent CrashPlan Pro from shutting down abruptly Weve been using- CrashPlan Pro as our backup solution for offsite backups . Because of the time it takes to upload the files over the internet we have been doing backups of different folders based on their importance . Recently we ran into the issue that when performing a 
@@45151363 @5151363/ <h> How to write an Xpath Query for the Event Viewer in the new versions of Windows ? <p> So today I finally decided to take a look at the event logs on my computer . My roommate used my desktop somehow and I am trying to uncover how he did it . After looking through the security logs I found that there was a remote desktop connection from another computer I left unlocked during the time he used it so I guess that explains the situation . After that I was curious to find out if he accessed any other computer so I was trying to look at other logs . It is a pain to go through the entire security log so I was looking for a way to filter the results . Is there a way to filter all those and only display the logon and logoff attempts via Terminal Services ? 
@@45151365 @5151365/ <h> How to : Configure Swappiness in Ubuntu <h> How to : Configure Swappiness in Ubuntu <p> In one of my previous articles on how to setup virtual memory / swap memory in Ubuntu I covered how to install a swap space for a Windows Azure VM . I deployed swap mostly as a precaution against running out of memory which could result on applications unable to work properly and the site could go down . To avoid that you deploy swap which most likely decrease the speed of your services but at least they wont crash . It effectively buys you time to resolve your memory issues at the expense of using the HD resulting in added latencies . So thus far setting up your swap memory seems genius , but beware , if your swappiness is not set up properly you will face unecessary latencies . In my case , I had still over a gigabyte of memory available but I was already swapping a lot . There is no point in swapping when there is plenty of RAM left ; You are just moving information in @ @ @ @ @ @ @ @ @ @ memory . Swappiness then controls how aggressively should the operating system use the swap space . <h> What is Swappiness ? <p> The swappiness parameter in Ubuntu controls the tendency of the OS kernel to move memory ( information ) used by processes out of physical memory and onto the swap disk . As mentioned above using a hard drive is supposed to be much slower than using RAM by nature . This results in applications taking longer to respond , particularly those who are being swapped to disk ( the others suffer indirectly because of the added IO operations but at least they are present in RAM already ) <p> There are some important things to know about how to use this parameter : <p> Accepted Swappiness values range from 0 to 100 inclusively . <p> A value of 0 indicates the kernel to avoid swapping at all costs . <p> A value of 100 indicates it should aggressively swap information on memory . <p> So thus far you must be thinking after reading my thoughts on swappiness that 0 is the way to go : be careful with @ @ @ @ @ @ @ @ @ @ including your applications , the use and capacity of the system . For example , assume you use a value of 0 and memory is almost full . If you try to launch a large application there will be a delay until the system is able to free enough memory by swapping processes before you can use your application . Take the other extreme of 100 : You should at all times have plenty of RAM to launch large processes but your overall response rate of your machine will take a performance hit . <p> Ubuntu has determined by default that a value of 60 is ideal for most users . It allows for memory to be available to launch new applications while is not that aggressive as to cause a huge performance hit . But as I mentioned the best value for your machine depends on the use you give the system . For example , I am running a server that eventually launches processes to service incoming requests but their size is relatively small and for the most part I always have plenty of RAM to handle all @ @ @ @ @ @ @ @ @ @ a swappiness value of 10. - You should think about your general RAM usage patterns and feel free to experiment . I expect boundary values ( 0 and 100 ) are too extreme but you can see what is the difference between 25 and 75 for example . <h> How do I configure Swappiness ? <p> This is rather easy . There is a file ( /proc/sys/vm/swappiness ) that it is used during operation that indicates the current swappiness the system is running under . Changing this value would result on the swappiness of the system being changed for the duration of the kernel ( i.e. until you reboot / shutdown &amp; turn back on ) . <p> If you want to see the value I recommend using cat over nano to avoid accidentally editing the file : <p> $ cat /proc/sys/vm/swappiness 60 <p> You 'll probably see the default value of 60 unless it was modified already . As I mentioned you could modify the file , swapoff and back on to have the change take place only during your current session . If you modify the value via the @ @ @ @ @ @ @ @ @ @ edit the /etc/sysctl.conf- file directly to avoid accidentally breaking something else ) . Said change would be take place only after you reboot the machine . Suppose you want to use a swappiness level of 15 : <p> $ sysctl vm.swappiness=15 <p> If you want you can execute swapoff -a and swapon -a to get your new swappiness value applied to the system ( instead of having to reboot this could be an attractive option ) . <p> Do n't  forget to check your memory usage with free -m to see the swap size and usage . <h> What swappiness value do you recommend ? <p> This is my personal opinion and you should monitor your swap usage and performance to fine tune it . I believe this is a good starting point but use your own criteria , experiment and decide what is best for you . <p> Recommended Swappiness <p> Memory generally available <p> Relative size of new processes <p> Recommended Swappiness <p> A lot <p> Small <p> &lt; 10 <p> A lot <p> Medium <p> 10 &lt; x &lt; 20 <p> A lot <p> Large <p> 15 @ @ @ @ @ @ @ @ @ @ &lt; 15 <p> Medium <p> Medium <p> 15 &lt; x &lt; 25 <p> Medium <p> Large <p> 25 &lt; x &lt; 45 <p> Low <p> Small <p> 35 &lt; x &lt; 60 <p> Low <p> Medium <p> 50 &lt; x &lt; 70 <p> Low <p> Large <p> 65 &lt; x &lt; 90 <p> It all becomes relative but you get the general idea : The more memory you generally have free or available the lower the swappiness you can use . Also , the larger the new processes you launch the larger the swappiness you 'll need so they can properly fit in memory if need be . Consider the following scenario : You have a machine with 16gb of RAM and you generally only use 2gb . You have never used more than 13gb and do n't  expect to . So even if new processes use a lot of RAM you never really run out of memory so a value at 10 or below would be recommended ( even 0 ) . <p> If you want feel free to provide via the comments your systems specs , memory usage patterns @ @ @ @ @ @ @ @ @ @ play with ! 
@@45151370 @5151370/ 55329 @qwx465329 <h> Tag Archive : Apache <p> How to : Add or Remove Symbolic links in Ubuntu Unlike Windows , in Linux based systems symbolic links are use quite frequently . The advantage of using symbolic links is that they allow you to have the same file in multiple locations , but the content to remain the same across all of them regardless of where you <p> How to : Secure a site using Apache If I had to , I would probably say that most people who use Apache do it to publish sites that are publicly accessible and/or have built-in security ( say WordPress which manages itself access to the application and data ) . So now that I wanted to publish a site that <p> Which Web Server to use : Apache vs NGINX If you are looking around there are a lot of people using NGINX over Apache nowadays . NGINX for what I have read provides a lot of performance improvements that a site with a lot of visitors could take advantage of . I found a great article about the <p> How @ @ @ @ @ @ @ @ @ @ Option MultiViews not allowed here I turned on some . htaccess rules in order for the cache to perform faster but unfortunately it came with some unintended consequences : A bunch of- 500 error- pages and in the log files one of the errors was Core:Alert / .. /. htaccess:Option MultiViews not allowed here . This one in particular happened when <p> How to solve : - Apache- error- . htaccess : ExpiresActive not allowed here , referrer : http : //Technology.Bauzas.com/ I turned on some . htaccess rules in order for the cache to perform faster but unfortunately it came with some unintended consequences : A bunch of- 500 error- pages and in the log files one of the errors was Core:Alert / .. /. htaccess : ExpiresActive not allowed here , referrer : http : //Technology.Bauzas.com/ The reason <p> How to solve : Apache error . htaccess : RewriteEngine not allowed here , referrer : http : //Technology.Bauzas.com/ I turned on some . htaccess rules in order for the cache to perform faster but unfortunately it came with some unintended consequences : A bunch of- 500 error- pages @ @ @ @ @ @ @ @ @ @ Core:Alert / .. /. htaccess : - RewriteEngine not allowed here , referrer : http : //Technology.Bauzas.com/ 
@@45151371 @5151371/ 55329 @qwx465329 <h> Category Archive : Third Party <p> If you are looking for a way to programatically hide rows you do n't  wish to show from your data source whether by triggering an event or by a condition you can do it . Its actually pretty simple once you figure out the method exposed for you to use . Below is an example of what I <p> The grouping functionality of the Ajax control RadGrid from Telerik is probably one of its most attractive features . However , many users might encounter themselves in a situation where you can take action on any given element and you wish to update the information being displayed on-screen to correspond to the action you 've taken on the 
@@45151372 @5151372/ 55329 @qwx465329 <h> Category Archive : Office <p> How to : Create Web query files for use with Excel for Mac Web queries allow you to query data from a specific World Wide Web , Internet , or intranet site and retrieve the information directly into a Microsoft Excel worksheet . Microsoft Excel includes some sample Web queries . Definition of a Web Query File A Web query <p> Resolved : Where and How to use Name Management function in Excel 2011 for Mac So this is one of those things I keep referring back to . One of the key things I do when I work in Excel is control the values that can be entered in certain fields . I am clearly not good at <p> How to : Enable the " Connect to Outlook " grayed out/disabled button on SharePoint 2013 to synchronize your content The short answer : just hit F5 ( reload the page ) . I had been terribly confused as to why the " Connect to Outlook " button on the SharePoint 2013 ribbon would be grayed out most of the time @ @ @ @ @ @ @ @ @ @ provide scripting support . The feature is not available " when using Microsoft Office in a Terminal Services environment As the message indicates scripting support is not available in Microsoft Outlook while using it in a terminal server environment . There could be several reasons ; in some cases it is just a matter of whatever <p> How to obtain stock quotes in Excel I was working on creating a spreadsheet to calculate profits and losses on options positions but did n't  know how to populate excel with stock quotes . Back in the day there used to be an interface to get stock quotes with the MSN Money site but it is not 
@@45151373 @5151373/ <p> As you probably know , Dynamic memory is not supported or recommended by Microsoft- to be used on any Exchange Server . What gives ? Who does n't  love Dynamic Memory ? Well , Exchange was not designed with Dynamic memory in mind , but rather , it was designed- to use all available memory it could get its hands on to improve performance . <h> So how come Dynamic memory is messing up with my Exchange this time ? <p> I am not expert but the reason seems to be- your initial memory allocated to your VM . Most likely Exchange considering it has a fixed amount of memory will initialize its counters and thresholds using a constant amount of memory . Given that scenario , it is no wonder that if you start your VM with 512 mb you can eat up your private memory quite fast even if your physical server has gigs and gigs of RAM . The issue went away when I started using normal ( static ) allocation of memory rather than- dynamic . I also went as far as to play @ @ @ @ @ @ @ @ @ @ a " high " amount of dynamic memory to start the system with and then dynamic memory takes some of it away if it is IDLE . This has worked fine with me but I would recommend following Microsofts recommendations and using static memory . I kind of like living on the " edge " lol . <h> Related <h> 2 comments <p> Thanks so much JCarlos for your solution about the Dynamic Memory , Weve been struggling with this error on our new Hosted Exchange environment for the last couple of months . We 've tried assigning more ram and everything but it just keeps complaining . Right now its complaining about not having enough Private Bytes even though its been dynamically assigned 5GB of Ram and is only using 3.5GB . I will change the Startup Ram to 4GB and leave it still on Dynamic memory so that I can actually increase the memory usage as needed . <p> P.S. Youve got a load of information on this site , how do you find the time to write it all and where do you get all your ideas . @ @ @ @ @ @ @ @ @ @ , I learned the hard way as well to listen to Microsoft when they advice not do so something . I might suggest if you still want to use dynamic memory assigning a starting memory close to the average RAM your VM uses . The purpose is twofold : first as you point out the private bytes issue but secondly dynamic memory allocation begins well after your computer has booted so having plenty of resources to complete the start up tasks helps a lot . Later on Dynamic Memory will remove the excess RAM according to the usage of it . I suggest this for all the VMs you may have ( many leave the 512 mb. that comes by default and that causes sometimes issues and for sure longer boot times than it should ) . And if it is going to use 5 gb anyway , might as well let it start with that much . That way Exchange also anticipates to have that much available . <p> Sadly is not that I have the time but that I need to document the issues I encounter and solutions @ @ @ @ @ @ @ @ @ @ . Searching all over the place is more time consuming that writing about it . Also , I figured others might benefit from that information as well so that 's where most of it comes from . Currently I do some IT work , but once the systems are stable I focus on areas of strategy and operations for the business ( as we are a small business ) . Glad to know this post helped you ! 
@@45151374 @5151374/ <h> How to : Cache static HTML with CloudFlare ? <h> How to : Cache static HTML with CloudFlare ? <p> So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another point of failure and it slowed down the response times . I am sure at one point CloudFlare with its Always Online feature took care of my site but now I am not sure . So the question came to my mind how do I cache my web servers responses that are HTML . I do run a WordPress blog so saying I am caching static HTML could technically be incorrect but 90%+ of the content- once published does not change . Surely there could be some false/positives ( if I may call them that ) like mobile and desktop clients get the same flavor of the site , or incorrect behavior when publishing comments , etc . But @ @ @ @ @ @ @ @ @ @ 10% of the visitors and comments are rare , so more users are impacted by our web server going offline than by getting a desktop theme on a mobile device . So the lesser of two evils , we had to go with CloudFlare caching . The question then became why is it not caching html ? <h> What we found out <p> We needed to understand CloudFlares caching offerings to figure out how to achieve what we wanted ( if possible ) and we came up with this information : <h> What are CloudFlares caching levels ? <p> June 02 , 2016 08:58 <h> You can set CloudFlare 's CDN to cache static content according to these levels : <p> No Query String / Basic : Only delivers resources from cache when there is no query string . <p> Ignore Query String / Simple : Delivers the same resource to everyone independent of the query string ( note : this will also remove the query string from the request to your origin ) . <p> Standard / Aggressive : Delivers a different resource each time the query string changes @ @ @ @ @ @ @ @ @ @ <p> If your server ever goes offline , CloudFlare will serve a limited copy of your cached website to keep it online for your visitors . CloudFlare builds the Always Online version of your website , so your most popular pages are represented . CloudFlare is caching pages when you see the crawler in your logs . <h> Crawling frequency <p> sounds like it should cache HTML otherwise how could it keep my site offline every time my server goes down ? But somehow it does n't  and that was really bugging me . Thankfully , the previous information we recovered ( Note : CloudFlare , by default , does not cache HTML content . You need to write a- Page Rule to cache static HTML content. ) gave us an answer : Page Rules . <h> How to : Cache static HTML content using Page Rules <p> Well , you can cache static and non-static HTML content the problem is that non-static content will be a snapshot in time so when it changes because it is cached the site visitors will get always the same page . So it @ @ @ @ @ @ @ @ @ @ suggests we do this : <h> How do I cache static HTML ? <p> The first step is creating a pattern and then applying a rule to that pattern . You 'll need to find or create a way to differentiate static versus dynamic content by the URL . Some possibilities could be creating a directory for static content , appending a unique file extension to static pages , or adding a query parameter to mark content as static . Here are three examples of patterns you could create for each of those options : <p> You 'll want to design the pattern to only describe pages you know are static . <p> Click Cache everything- in the Custom caching dropdown menu . <p> Click Add rule . <p> If you see the HTML is not being cached , despite the cache everything rule , it means you need to override the origin cache directive with an " Edge Cache TTL " setting . <p> If the Cache-Control header is set to " private " , " no-store " , " no-cache " , - or " max-age=0 " , or if @ @ @ @ @ @ @ @ @ @ will not cache the resource , unless a Page Rule is set to cache everything and an Edge Cache TTL is set . <p> So , if you are lazy and your site permits it , just example.com/ the whole thing to get it cached . Keep the Cache-Control header set to no-cache when you are doing a dynamic page and make sure the Edge Cache TTL is not set and you 're golden ! <p> Another idea is to again , cache everything , but then with another rule lower the cache level on the parts of the site that are dynamic ( the opposite approach . ) So as you can see , you can get creative with it and figure what approach works best for you . 
@@45151377 @5151377/ <p> User accounts can be manually deleted or deleted through a script by using Windows Azure Active Directory Module for Windows PowerShell . <p> User accounts can be manually deleted by using Exchange Control Panel in Exchange Online . <p> User accounts can be deleted through a synchronized deletion if directory synchronization filtering changes exclude the on-premises Active Directory user object from the synchronization set . ( Directory synchronization filtering changes are also known as scoping . ) <p> User accounts can be deleted through a synchronized deletion if the on-premises user object was deleted from the on-premises Active Directory schema . <h> Before you start <p> When a user object is deleted , its not immediately and completely removed from Windows Azure Active Directory ( Windows Azure AD ) authentication system . The user object is put in a deleted state and no longer appears in the ordinary user listing . However , its present in the Windows Azure AD database and can be recovered for an organization within 30 days . To determine whether a user object is eligible to be recovered from a deleted state , @ @ @ @ @ @ @ @ @ @ , look up user accounts that were deleted through the portal . To do this , follow these steps : <p> To recover deleted user accounts , make sure that directory synchronization filtering is set in such a way that the scope includes the objects that you want to recover . For more information , go to the following Microsoft website : <p> The Active Directory recycle bin is available only with the functional level of Windows 2008 R2 or later versions . <p> For the Active Directory recycle bin to be useful in recovering an item , it must be enabled before the item is deleted . <p> If the Active Directory recycle bin is unavailable , or if the object in question is no longer in the recycle bin , try to recover the deleted item by using the AdRestore tool . To do this , follow these steps : <p> Install the AdRestore tool from the following Microsoft TechNet website : <p> Use AdRestore together with a search filter to locate the deleted on-premises user object.The following is an example of how to use AdRestore to enumerate @ @ @ @ @ @ @ @ @ @ in their name : <p> Enable the user object in Active Directory . When the object is restored , its disabled at first . Therefore , you have to enable it . If its necessary , first reset the user object 's password , and then enable the user object in Active Directory Users and Computers.To enable the user object in Active Directory Users and Computers , follow these steps : <p> In Active Directory Users and Computers , right-click the user , and then click- Reset Password . <p> Enter a new password in the- New password- and- Confirm password- boxes , and then click- OK . <p> Windows can not enable object &lt;MailboxName&gt; because : Unable to update the password . The value provided for the new password does not meet the length , complexity , or history requirements of the domain . <p> After you receive this error message , reset the users password in Active Directory Users and Computers . <p> Configure the user logon name . The user logon name ( also known as the user principal name , or UPN ) is n't set from @ @ @ @ @ @ @ @ @ @ user logon name , especially if the user is a federated account.To configure the user logon name , follow these steps : <p> In Active Directory Users and Computers , right-click the user , and then click- Properties . <p> Click- Account , enter a name in the- User logon name- box , and then click- OK . <p> Finally , if you cant recover the deleted user account through the Active Directory recycle bin or by using the AdRestore tool , perform an authoritative restore of the deleted user objects in Active Directory . <p> Warning- Make sure that only the user objects that you want to restore are marked as authoritative . Active Directory objects that are marked as authoritative in the restore process may cause many Active Directory service issues . <p> For more information about how to perform an authoritative restore of Active Directory objects , go to the following Microsoft website : <p> Warning- Objects that are restored by using resolution 3 may not have all service attributes ( Exchange Online , Lync Online , and so on ) automatically repopulated after the restore action @ @ @ @ @ @ @ @ @ @ on-premises object attributes before the next directory synchronization interval propagates the restore action to the cloud directory . <p> For example , for an object that was formerly mail-enabled in Exchange Online , you may want to use Windows PowerShell cmdlets to repopulate the Exchange Online attributes . In the following example , the User1 object is repopulated with Exchange Online attributes for the contoso.onmicrosoft.com tenant : <p> Changes that are made to a domain or to a user object in Office 365 after a user account is deleted and before the user account is recovered can affect the user experience after recovery . After user deletion and before user recovery , the following events may occur : <p> A new user is created who uses a unique user I 'd value that was formerly assigned to the deleted user . <p> A new user is created who uses a unique email address value that was formerly assigned to the deleted user . <p> When these conflicts occur , conflicting attributes must be updated to remove the conflict before user recovery can be completed . If a conflict occurs during user @ @ @ @ @ @ @ @ @ @ symptoms when a user recovery is tried : <p> Windows PowerShell returns one of the following error messages:Error message 1 <p> Restore-MsolUser : The specified user account can not be restored because of the following error : Error Type UserPrincipalName <p> Error message 2 <p> Restore-MsolUser : The specified user account can not be restored because of the following error : Error Type proxyAddress <p> To restore users who are in this state , you can correct the conflict by using the following parameters when you run theRestore-MSOLUser- cmdlet : <p> **27;2738;TOOLONG <p> NewUserPrincipalName <p> Note- When you use the- **28;2767;TOOLONG parameter , any conflicting email addresses are removed from the deleted user before you continue with the recovery process . <p> The Office 365 portal shows the equivalent error messages in the form of the Windows PowerShell error states that were mentioned earlier . For example , you see the following : <p> To restore users who are in this state , complete the form , and make sure that you specify how you want to correct the conflict . 
@@45151378 @5151378/ 55329 @qwx465329 <h> Tag Archive : Segmentation fault <p> Changes with- NginX- 1.4.7 Below are the list of changes from the 1.4.6 release of NginX to version 1.4.7 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update addresses a bug fix and a security issue . Because of the security issue we rate this update of high priority . Changes with nginx 1.4.7 18 Mar <p> Changes with- NginX- 1.4.6 Below are the list of changes from the 1.4.5 release of NginX to version 1.4.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update mainly provides- bug-fixes which are not critical . Changes with nginx 1.4.6 04 Mar 2014 * ) Bugfix : the " clientmaxbodysize " directive might not work when reading a request body <p> Changes with NginX 1.4.5 Below are the list of changes from the 1.4.4 release of NginX to stable version 1.4.5 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.5 11 Feb 2014 * @ @ @ @ @ @ @ @ @ @ instead of just a session i 'd . Thanks to <p> Changes with- NginX- 1.5.9 Below are the list of changes from the 1.5.8 release of NginX to version 1.5.9 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.9 22 Jan 2014 * ) Change : now nginx expects escaped URIs in " X-Accel-Redirect " headers. * ) Feature : the " sslbuffersize " directive . * ) Feature : the " limitrate " directive 
@@45151380 @5151380/ <h> " Microsoft outlook can not provide scripting support . The feature is not available " when using Microsoft Office in a Terminal Services environment <p> As the message indicates scripting support is not available in Microsoft Outlook while using it in a terminal server environment . There could be several reasons ; in some cases it is just a matter of whatever third party application or setup modifier was provided did n't  include that feature . The second part of this message though is a bit more unique : " The feature is not available " . This generally indicates in my experience that the feature was not included as it could not be installed on your OS . In the case of any machine running terminal services ( including Windows Multipoint ) you will encounter this problem as for some reason Office does not install scripting support . In order to install it you have a number of options ( 1 &amp; 2 are for installations by third party components and 3 is for a machine running Terminal Services ) . <p> " Microsoft Outlook can not @ @ @ @ @ @ @ @ @ @ through the use of Visual Basic scripts that generally third party components leverage . The solutions provided below are ways to add the Outlook VisualBasic Script library to your existing installation . <h> Method 1 : Copy the Outlvbs.dll file to existing installations <p> To do this , follow these steps : <p> Copy the Outlvbs.dll file from a computer that is not running Microsoft Outlook on a Terminal Server . <p> On the last available page of the Custom Installation Wizard , click- Modify Setup Properties , and then add the following property name and value:Property Name : WTSDISABLED Value : MsInfoFiles , PPTCastCoreFilesNote- Both the property name and the value are case sensitive . <h> Method 3 : Use the office installer to deploy Outlook VB script <p> To enable VB Script support for Outlook forms when running Outlook 2007 on a Terminal Server computer , do the following : <p> Install Outlook 2007 as a standalone application or as part of a 2007 Office suite . <p> Determine the globally unique identifier ( GUID ) for the product that you just installed . If you installed Outlook @ @ @ @ @ @ @ @ @ @ Enterprise2007 , use that product GUID . For more information about how to determine the GUID of the product that you installed , click the following article number to view the article in the Microsoft Knowledge Base : <p> Note- In this command , - &lt;Product Code GUID of SKUNameWW.msi&gt;- is a placeholder for your product GUID . For example , if you have Office Enterprise 2007 installed , your product GUID would be the following : <h> Kevin <p> On terminal servers if the the GUID follows the users then if i make the change and copy the same GUID to all servers after a user has already got a GUID of some kind from one of the servers previously wont they still have the wrong GUID ? <p> I 'm not entirely sure I follow what you are saying but the GUID being referenced here is an application GUID . What this means is that if you look through different computers the same GUID is used and it is user agnostic . Now , the GUID is version specific hence you need to check on an existing machine what @ @ @ @ @ @ @ @ @ @ you can go to other machines and see that for the same version of Office even with other users on the same machine that GUID is the same . I hope this helps ! <p> 1 Minute Ago I found a solution for Outlook 2010 and Outlook 2007 . You just need to run the installer with a special parameter to force it to install VBscript for outlook . Details can be found here : How to : Resolve message " Microsoft outlook can not provide scripting support . The feature is not 
@@45151382 @5151382/ 55329 @qwx465329 <h> What is : urs.microsoft.com <h> What is : urs.microsoft.com <p> The URLhttp : //urs.microsoft.com- might appear on your firewall or similar network monitoring/protection device quite often as it is the URL used by the Microsoft SmartScreen Filter Services . Generally when you see a Microsoft URL you know some part of the Windows- system is trying to communicate with some sort of Microsoft web service , but the real question is that if all of it is necessary and the impact it may have on your organization and systems . <p> In my case I find the SmartScreen- Filter Services to be quite helpful and recommend organization to enable their client machines to communicate with Microsofts- servers . Another important reason why to leave this communication available to your client machines is that Windows 8- and above when they visit a website that is in a IE zone with SmartScreen Filter enabled the lack of connectivity with Microsoft will cause a performance hit . <p> Consider the following scenarios : <p> A firewall or network device is blocking communication between your Internet Explorer- browser and the @ @ @ @ @ @ @ @ @ @ could be your home router if you are crazy about security or your corporate firewall like Microsofts ISA or TMG if you have tightened your security . <p> ( OR ) <p> You are browsing the website from within a network that does not have an active connection to the Internet . <p> In this scenarios , Internet Explorer may appear- to stop responding . The SmartScreen- Filter Services are provided through an active Internet connection to http : //urs.microsoft.com- and connectivity is needed- for it to validate- the site you are visiting . At this point I recommend you enable communication to the Microsoft URL on your firewall . You could alternatively add- sites to an IE zone that does n't  require- validation by the SmartScreen- Filter Services or worst case turn SmartScreen Filter Services off all together . 
@@45151383 @5151383/ <h> How to : Manage vault certificates in Windows Azure Backup <h> How to : Manage vault certificates in Windows Azure Backup <p> Windows Azure Recovery Services encompasses a set of Windows Azure vaults that help to protect your organization from data loss , and aid in continuity of operations . Vaults are used to store and protect information that is specified as part of your recovery services configuration . <p> If you are using Windows Azure Backup you will create backup vaults to store protected items from the servers you register for your organization . <p> If you are using Windows Azure Hyper-V Recovery Manager you will create Hyper-V Recovery Manager vaults to orchestrate failover and recovery for virtual machines managed by System Center 2012 " Virtual Machine Manager ( VMM ) . You configure and store information about registered VMM servers , protected clouds , networks , and virtual machines enabled for protection in a source location ; and about VMM servers , clouds , networks , and virtual machines that are used for failover and recovery in a target location . You can create recovery plans @ @ @ @ @ @ @ @ @ @ , and customize these plans to run additional scripts or manual actions . <p> You can configure both backup vaults and Hyper-V Recovery Manager vaults as appropriate . <p> The management certificate uploaded to a Recovery Services vault requires the following : <p> You can use any valid SSL certificate that is issued by a Certification Authority ( CA ) that is trusted by Microsoft ( and whose root certificates are distributed via the Microsoft Root Certificate Program ) . For more information , see Microsoft article- 931125 . Alternatively you can use a self-signed certificate that you create using the Makecert.exe tool . <p> The certificate should be an x.509 v3 certificate . <p> The key length should be at least 2048 bits <p> The certificate must have a valid ClientAuthentication EKU . <p> The certificate must be currently validity with a validity period that does not exceed three years . You must specify an expiry date , otherwise a default setting that is valid for more than three years will be used . <p> The certificate should reside in the- Personal- certificate store of your Local Computer . @ @ @ @ @ @ @ @ @ @ the certificate . <p> To upload to the certificate to the portal , you must export it as a . cer format file that contains the public key . <p> Each vault only has a single . certificate associated with it at any one time . You can upload a certificate to overwrite the current certificate associated with the vault at any time . <p> Use any valid SSL certificate issued by a CA trusted by Microsoft , whose root certificates are distributed via the Microsoft Root Certificate Program . For more information about this program , see Microsoft article- Windows Root Certificate Programmembers . <p> Export a certificate ( . pfx ) " On the server on which the certificate was created , you export the . cer file as a . pfx file ( containing the private key ) . This . pfx file will be uploaded to VMM servers when you install the Hyper-V Recovery Manager provider on those servers , and is used to register the servers with the vault . <p> Import the certificate ( . pfx ) " After export of the . pfx @ @ @ @ @ @ @ @ @ @ certificate store on each VMM server that contains virtual machines you want to protect . <p> Obtain the Makecert tool as described in- MakeCert . Note that when installing the Windows SDK , you can limit the installation to install makecert.exe only by selecting the option- Tools- under- . Net Development- and leaving everything else unchecked . <p> After exporting the server , copy it to the server you want to register , and then import it as follows . Note that you do not need to import the certificate on the server that was used to run MakeCert.exe . <p> Copy the private-key ( . pfx ) certificate files to a location on the local server . <p> From the- Start- screen , type- mmc.exe , and then press- Enter- to open the Microsoft Management Console . <p> In the Microsoft Management Console , expand- Certificates , right-click- Personal , point to- All Tasks , and then click- Import- to start the- Certificate Import Wizard . <p> On the- Certificate Import Wizard Welcome- page , click- Next . <p> On the- File to Import- page , click- Browse- and @ @ @ @ @ @ @ @ @ @ that contains the certificate that you want to import . Select the appropriate file , and then click- Open <p> On the- Password- page , in the- Password- box , type the password for the private-key file that you specified in the previous procedure and then click- Next . 
@@45151384 @5151384/ 55329 @qwx465329 <h> How to : Move your NginX website to HTTPs / SSL <h> How to : Move your NginX website to HTTPs- / SSL <p> It comes at no surprise that a lot of people are looking into moving their sites to HTTPs due to recent events : Googles decision to give ranking points to sites that use SSL / HTTPs and eavesdropping by governments world wide . There are a number of considerations before taking this step , specially for people who have not yet deployed HTTPs / SSL on their web servers . <p> There are a number of things you need to start doing and fixing in order to have a functional site that performs well . Because- most sites were not thought to use SSL to begin with , you 'll find that there might be problems when accessing your site from https instead of http . Below are a few tips to consider although it is not an exhaustive list : <p> Internal links : If you are using WordPress you will find a lot of the links you have not only do @ @ @ @ @ @ @ @ @ @ . This is a problem for two reasons : 1 ) you are hard coding the transport ( in this case http ) so even if you use a different transport it will force the browser to take the hard coded one and 2 ) You are hard coding your sites URL . Imagine you change your domain name now you have to change all your URLs . For these reasons I generally recommend people use relative URLs ( they start with / vs the transport or domain name ) . <p> Infrastructure concerns : Make sure your CDN supports SSL as well as any other part of your infrastructure ( reverse proxies , firewalls , etc ) . Consider using SPDY when using SSL for added performance benefits . I really like using- http : //spdycheck.org/- to check for SPDY readiness . It let 's you know if you have deployed SPDY and other tips to get the most out of it . We will go over the checklist for this in a minute <p> NginX specific configuration : We will cover the required configuration needed to address the implementation @ @ @ @ @ @ @ @ @ @ <p> So let 's get to it ! <h> I. Use SSL cache <p> As you can imagine hosting a site using SSL requires additional work on the CPU . We need to make sure we do n't  saturate the CPU otherwise our site will be loading much slower . For example , I once tried using a 16k bit certificate my CPU would die depending on the number of requests it had to handle . For that reason I 've joined the 99% of people in using a 2k bit certificate for my site . For that same reason we need to turn on SSL session cache on NginX to save on CPU resources : <p> sslsessioncache shared:SSL:20m ; <p> sslsessiontimeout 10m ; <h> II . Use the right cyphers <p> This was also a surprise to me , but you can configure the cyphers NginX uses . This has two major impacts : 1 ) Certain cyphers are weak or vulnerabilities have been discovered . Pretty much using them does not guarantee much security or worse case , compromises the security of your entire site . 2 ) Certain cyphers @ @ @ @ @ @ @ @ @ @ to be helpful for the poor guy who uses that strange CPU intensive cypher but you 're just asking for trouble down the road . I wrote an article on CloudFlare recommended configuration here : - How to : Improve SSL performance on NginX- which you can also refer to as well as this article : Hardening Your Web Server 's SSL- Ciphers . <p> Below is the recommended configuration as of the date of publishing of this post : <h> f ) Use HTTP Strict Transport Security ( HSTS ) <p> All this does is indicate your client that for future requests for your site it should use SSL . I set up the max age to one day as I am starting to test site wide SSL deployment . If you feel very comfortable with this I have observed most people use a max age of one year . Be as it may the important thing is to enable it and like cache use a reasonable time in case you are forced to go back to a non-SSL site . <p> # This forces every request after this one @ @ @ @ @ @ @ @ @ @ max-age=86400 ; includeSubdomains " ; <p> Do n't  forget that the includeSubdomains should only be used if you are deploying Strict Transport Security to your subdomains as well . You can remove it if you do n't  want your subdomains to use HSTS . <p> UPDATE : SSLLabs recommends using at least 180 days for your HSTS . So you should use- 15552000 for max age once your done testing and find the performance of your site acceptable . <p> g ) Enable- OCSP Stapling <p> If you are not sure what OCSP Stapling is , I recommend reading the CloudFlare article : - OCSP Stapling : How CloudFlare Just Made SSL 30% Faster . Pretty much what this does is remove a large portion of the SSL overhead by doing some work on your server . This is good as your visitors will enjoy a faster experience on your site . <p> First you need to make sure your certificate includes the entire certificate chain . Ill look up a previous post I had on how to concatenate certificates to achieve this ( pretty much you have your certificate @ @ @ @ @ @ @ @ @ @ authority above it . ) <p> Here is the code needed on NginX : <p> sslstapling on ; <p> sslstaplingverify on ; <p> resolver 8.8.8.8 8.8.4.4 valid=300s ; <p> resolvertimeout 10s ; <p> Do keep in mind we are using Googles resolvers for this . You can use other DNS resolvers if you prefer . <p> Here is a sample report from SPDYCheck.org : <h> Report Details <p> Network Server on 443Nice , this host has a network service listening on port 443 . SPDY works over SSL/TLS which usually listens on port 443 . <p> SSL/TLS DetectedGood , this host is speaking SSL/TLS . SPDY piggybacks on top of SSL/TLS , so a website needs SSL/TLS to use SPDY . <p> Valid X.509 CertificateThis website is responding with a valid X.509 certificate . X.509 certificate errors can cause the browser to display warning messages and to stop speaking with the website , so using a valid certificate is an essential step to supporting SPDY . <p> ServerHello Contains NPN ExtensionNice , this server including the NPN Entension during the SSL/TLS handshake . The NPN Extension is an additional part @ @ @ @ @ @ @ @ @ @ to tell browser it supports additional protocols , like SPDY . <p> Success ! SPDY is Enabled ! Hurray , this website is using SPDY ! The following protocols are supported : <p> spdy/3.1 <p> http/1.1 <p> HTTP Fallback DetectedThis website is using SPDY , but it also supports traditional HTTP over SSL . This ensures that older web browsers can still access this site using HTTP <p> HTTP Redirects to SPDYPretty Sexy ! Accessing this website via HTTP automatically redirects the user to access the website via SSL/TLS and SPDY . This means all of websites visitors that can browse the site with SPDY , do browse the site using SPDY . <p> **25;2824;TOOLONG SupportededExcellent ! This website is using HSTS , also known as Strict Transport Security . This tells the browser to always use SSL when talking to this website , allows more of your visitors the opportunity to both be secure and to use SPDY . The server is sending the **31;2851;TOOLONG : max-age=86400 ; includeSubdomains which tells the web browser to always use SSL to access this website for the next 1 days . 
@@45151385 @5151385/ <h> Author 's posts listings <p> Packages are manually installed via the dpkg command ( Debian Package Management System ) . dpkg is the backend to commands like apt-get and aptitude , which in turn are the backend for GUI install apps like the Software Center and Synaptic . Something along the lines of : dpkg &gt; apt-get , aptitude &gt; Synaptic , Software Center But of course the <p> How to : Assign multiple IP addresses to one interface in Ubuntu using the Command Line Interface ( CLI ) ? I have been working with Ubuntu more lately and ran into the need to direct traffic going to one server ( via IP ) to go to a new server but I could n't change the clients configuration . Because part of <p> UniFi How to further customize USG configuration with config.gateway.json Obtained from : - LONG ... Overview The file- config.gateway.json- is used for advanced configuration of the USG . This file allows you to make customizations persistent across provisions . When making customizations via the config.gateway.json file it is best to extract only the customizations that ca n't @ @ @ @ @ @ @ @ @ @ NginX- 1.4.7 Below are the list of changes from the 1.4.6 release of NginX to version 1.4.7 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update addresses a bug fix and a security issue . Because of the security issue we rate this update of high priority . Changes with nginx 1.4.7 18 Mar <p> Changes with- NginX- 1.4.6 Below are the list of changes from the 1.4.5 release of NginX to version 1.4.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update mainly provides- bug-fixes which are not critical . Changes with nginx 1.4.6 04 Mar 2014 * ) Bugfix : the " clientmaxbodysize " directive might not work when reading a request body <p> Changes with- NginX- 1.6 Below are the list of changes from the 1.5.13 release of NginX to version 1.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update makes the development / mainline version 1.5.13 part of the stable branch. - Because of this several new features @ @ @ @ @ @ @ @ @ @ Changes with- NginX- 1.7 Below are the list of changes from the 1.5.13 release of NginX to version 1.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several new Features- so updating to this version is not considered urgent/critical . Changes with nginx 1.7.0 24 Apr 2014 * ) Feature : backend SSL certificate verification . * ) <p> Changes with- NginX- 1.5.13 Below are the list of changes from the 1.5.12 release of NginX to version 1.5.13 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several bugfixes but no critical- security updates . Changes with nginx 1.5.13 08 Apr 2014 * ) Change : improved hash table handling ; the default values of the " variableshashmaxsize " <p> Changes with- NginX- 1.5.12 Below are the list of changes from the 1.5.11 release of NginX to version 1.5.12 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes a security fix so it is considered an @ @ @ @ @ @ @ @ @ @ Mar 2014 * ) Security : a heap memory 
@@45151388 @5151388/ <h> Google IP address ranges <h> Google IP address ranges <p> Recently I decided to try the Google Compute Engine Cloud , and one of the things I liked right from the beginning was the ability to connect via SSH to your Ubuntu box . I am going to have to read a bit more about it but basically it seems like it generates a keypair , sets it up on your server and then opens a terminal connection via ssh on your web browser how cool is that ? ! In my case I wanted to increase the security by limiting the IP- addresses that can access the SSH port . Here is where this post comes along . If you look at the documentation basically it says you need to allow all Google IP addresses because the web client may connect from any Google IP I know , a lot of IPs to authorize but I guess it is better than opening the port to all IP addresses in the world . I am probably going to enable and disable the rule manually but open it to @ @ @ @ @ @ @ @ @ @ client to initiate an ssh connection . <h> The ranges <p> Well , the IP ranges that Google owns/manages vary throughout time . Fortunately , there are ways to obtain the lates versions at any given point . Unfortunately , I am not sure how can you dynamically update them in the Google Cloud Engines firewall rules so I am manually adding them . In order to identify them I am relying on the spf record google offers : - spf.google.com <p> So there you have it . The first block is called netblocks which contains all the blocks in the IPv4 space , then you have netblocks2 which contains all the blocks in the IPv6 space which leaves netblocks3 which contains an IPv4 address range but I am not sure why not simply include it in netblocks . If anyone knows the reason or has a good guess please share ! <p> If you wish to query this information anytime yourself , you can do it querying the Google DNS servers like so : <p> nslookup -q=TXT spf.google.com 8.8.8.8 <p> nslookup -q=TXT netblocks.google.com 8.8.8.8 <p> nslookup -q=TXT netblocks2.google.com 8.8.8.8 @ @ @ @ @ @ @ @ @ @ to automate the update of the ranges via a shell script for example or an API call but for me I think it is not necessary seeing updates shouldnt happen too often and if there is an issue I can always update as required . 
@@45151391 @5151391/ <h> How to : Erase a log file in Ubuntu <h> How to : Erase a log file in Ubuntu <p> Ever had this huge error log , full of nightmares and bad memories ? Well , once you 're done fixing the problems then you 're stuck with megs if not gigs worth of bad memories that perhaps you wish to get rid off . Well , in my case one of my errors sent so much information to the log file it was on the gigs arena . This obviously becomes an issue as much of that information is not important to me any more and it can have a performance impact on the system . <p> Usually I would move the original file to a backup location and create a new one . The issue with this approach is that the permissions or attributes of the original file need to be copied over to the new one more work basically . So I did some research and it is possible to simply truncate your log file so that all you 're doing is getting rid of the data within @ @ @ @ @ @ @ @ @ @ <p> There are a couple of ways to do this : You can do some fancy output redirection or you could use the truncate command . Either way the result is the same and all it takes is one command line . Here it goes : <h> Option 1 : Redirect output to the file to clear it : <p> If you simply redirect null output to the file , basically you 're wiping it clean . Simply do the following : <p> &gt;error.log <h> Option 2 : Use the truncate command : <p> For those who feel more at home with a traditional command instead of ninja magic : 
@@45151392 @5151392/ <p> The Joule , Dallas 1530 Main Street , - Dallas , - TX- 75201 Phone : - ( 214 ) 748-1300 Site : - http : **25;2884;TOOLONG My Rating : 4 1/2 stars Category : 4 Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to <p> Cane Rosso Category : - Pizza 2612 Commerce St Dallas , - TX- 75226 Neighborhood : Deep Ellum ( 214 ) 741-1188 www.ilcanerosso.com Menu Yelp : - http : **36;2911;TOOLONG My Rating : 4 1/2 stars Summary : One of the best pizza place Ive been in . Great variety and different from the ordinary , great quality of ingredients and superior taste . It is not expensive or cheap but their rating makes it 
@@45151393 @5151393/ <p> The Joule , Dallas 1530 Main Street , - Dallas , - TX- 75201 Phone : - ( 214 ) 748-1300 Site : - http : **25;2949;TOOLONG My Rating : 4 1/2 stars Category : 4 Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to <p> Cane Rosso Category : - Pizza 2612 Commerce St Dallas , - TX- 75226 Neighborhood : Deep Ellum ( 214 ) 741-1188 www.ilcanerosso.com Menu Yelp : - http : **36;2976;TOOLONG My Rating : 4 1/2 stars Summary : One of the best pizza place Ive been in . Great variety and different from the ordinary , great quality of ingredients and superior taste . It is not expensive or cheap but their rating makes it <p> Earn 50 bonus miles per check in ( per day ) via Facebook or Foursquare , within 2 miles of approved airports within the 50 states ( http : @ @ @ @ @ @ @ @ @ @ confirmation message to the social media chosen , containing information about nearby retailers " you can opt out of these messages if you 'd like <p> After recently visiting Disneyland there were a couple of noteworthy tips to take into consideration when visiting this attraction in Anaheim , CA . First of all I would highly recommend staying in a hotel within the property . Not only are the usually nicer but the- convenience- they offer being right in the park as well as some added 
@@45151394 @5151394/ 55329 @qwx465329 <h> What is Google Sync ? <h> What is Google Sync ? <p> Google Sync- was designed- to allow access to Google Mail , Calendar and Contacts via the Microsoft Exchange ActiveSync- protocol . With the recent launch of CardDAV , Google now offers similar access via IMAP , CalDAV- and CardDAV , making it possible to build a seamless sync experience using open protocols . For that reason Google has decided to stop offering Google Sync and forcing new users to use the open protocols to sync their information . <p> Personally Google Sync was a great feature as it offered what is known- as- " Push technology " instead of the standard " check my email every x minutes " which consumes a lot of power/bandwidth and you have to wait until the next sync to get your information on your device very old school . Also , Google Sync offered to sync your mail , calendar and contacts on one go versus- now having to set up- 3 different accounts to get your email ( IMAP ) , Calendars ( CalDAV ) and Contacts @ @ @ @ @ @ @ @ @ @ issues of having to sync to get your email , CalDAV- I hear is still poorly implemented and some users have struggled vs Google Sync but with time theyll iron that out , and the CardDAV implementation seems to work fine but still lacks the ability to get those contact groups over to your device . <p> Whatever you chose to do , I would recommend setting up your Google sync on all your devices before the end of January when they stop letting people do that . That way you can use Google sync or an alternative method later on but you still have the choice of ActiveSync . Remember , you can turn off any of the 3 sync types on ActiveSync and use an alternative account to sync your Contacts for example . <p> To read more about Google Sync end of Life you can visit Google.com or read the following article I found on their site : <h> Google Sync End of Life <p> Google Sync- was designed to allow access to Gmail , Google Calendar , and Contacts via the Microsoft- Exchange ActiveSync- protocol . @ @ @ @ @ @ @ @ @ @ similar access via IMAP , CalDAV , and CardDAV , making it possible to build a seamless sync experience using open protocols . Starting January 30 , 2013 , consumers wo n't be- able to set up new devices using Google Sync , however , existing Google Sync connections will continue to function. - Google Sync- will continue to be- fully supported for Google Apps for Business , Government and Education users who are unaffected by this announcement . <p> What do I need to do if I 'm already using Google Sync ? <p> Nothing ! Existing users can continue to use Google Sync on their current devices . Starting January 30 , 2013 , users , other than paid Google Apps users , wont be able to set up new devices using Google Sync and should see our sync site for instructions . You can also consult with your device carrier or manufacturer for how they recommend to sync with Gmail , Google Calendar , and Contacts . Google Apps for Business , Education , and Government customers can continue to set up new devices with Google Sync after @ @ @ @ @ @ @ @ @ @ are also reaching their end of life , and will no longer be supported by Google for any users : Google Calendar Sync , Google Sync for Nokia S60 , and SyncML . See below for more details . <p> How do I know if I 'm affected by this announcement ? <p> Google Calendar Sync : The Google Calendar Sync- download link will be removed on December 14 , 2012 . Existing users can continue to use it to sync with Microsoft Outlook Calendar , but it will no longer be supported by Google as of December 14 , 2012 . Google Apps for Business , Education , and Government customers can still use Google Apps Sync for Microsoft Outlook- . <p> Google Sync for Nokia S60:Google Sync for Nokia S60 will no longer be supported by Google on January 30 , 2013 . If you have an older Nokia device , check to see if you are syncing with Google via Mail for Exchange on your device . If so , we recommend you consult Nokia 's Support Site for how to sync with Google on your device @ @ @ @ @ @ @ @ @ @ with a small number of older Nokia and Sony Ericsson- devices , will no longer be supported and stop syncing on January 30 , 2013 . This service is used- to sync contacts between your device with your Google account . We recommend you to download your contacts and uninstall SyncML . <p> What will happen if I keep using Google Sync after January 30 , 2013 ? <p> The service will continue to work for existing Google Sync devices . Google Sync will continue to work , be supported , and take new sign-ups for Google Apps for Business , Education , and Government customers . <p> What will happen to Google Calendar Sync after December 14 , 2012 ? <p> The service will continue to work for existing Google Sync devices . Google Sync will continue to work , be supported , and take new sign-ups for Google Apps for Business , Education , and Government customers . <p> What will happen to Google Sync for Nokia S60 and SyncML after January 30 , 2013 ? <p> Google Sync for Nokia S60 and SyncML will no longer be @ @ @ @ @ @ @ @ @ @ consult Nokia 's Support site for the best alternative for your phone . <p> Who does this affect ? <p> Were ending support for Google Sync for all users , other than paid Google Apps users . Google Apps for Business , Education , and Government customers are not affected by this announcement . We 're ending support for Google Calendar Sync for all users on December 14 , 2012 . And we 're ending support for Google Sync for Nokia S60 , and SyncML for all users , starting January 30 , 2013. 
@@45151397 @5151397/ <h> NAME <h> SYNOPSIS <h> DESCRIPTION <p> In the 1st form , create a link to TARGET with the name LINKNAME . In the 2nd form , create a link to TARGET in the current directory . In the 3rd and 4th forms , create links to each TARGET in DIRECTORY . Create hard links by default , symbolic links with --symbolic . By default , each destination ( name of new link ) should not already exist . When creating hard links , each TARGET must exist . Symbolic links can hold arbitrary text ; if later resolved , a relative link is interpreted in relation to its parent directory . Mandatory arguments to long options are mandatory for short options too . --backup=CONTROL make a backup of each existing destination file -b like --backup but does not accept an argument -d , -F , --directory allow the superuser to attempt to hard link directories ( note : will probably fail due to system restrictions , even for the superuser ) -f , --force remove existing destination files -i , --interactive prompt whether to remove destinations -L , @ @ @ @ @ @ @ @ @ @ treat LINKNAME as a normal file if it is a symbolic link to a directory -P , --physical make hard links directly to symbolic links -r , --relative create symbolic links relative to link location -s , --symbolic make symbolic links instead of hard links -S , --suffix=SUFFIX override the usual backup suffix -t , **28;3047;TOOLONG specify the DIRECTORY in which to create the links -T , --no-target-directory treat LINKNAME as a normal file always -v , --verbose print name of each linked file --help display this help and exit --version output version information and exit The backup suffix is ' ' , unless set with --suffix or SIMPLEBACKUPSUFFIX . The version control method may be selected via the --backup option or through the VERSIONCONTROL environment variable . Here are the values : none , off never make backups ( even if --backup is given ) numbered , t make numbered backups existing , nil numbered if numbered backups exist , simple otherwise simple , never always make simple backups Using -s ignores -L and -P . Otherwise , the last option specified controls behavior when a TARGET is a @ @ @ @ @ @ @ @ @ @ <p> link(2) , symlink(2) The full documentation for ln is maintained as a Texinfo manual . If the info and ln programs are properly installed at your site , the command **26;3077;TOOLONG ' should give you access to the complete manual . 
@@45151398 @5151398/ 55329 @qwx465329 <h> Category Archive : Entity Framework <p> After writing code using entity framework for sometime and seeing the code base continue to grow I realized the importance of coding with performance in mind . There are things that come to mind like eager loading , but that alone wont do much as your application continues to grow . Below are some key best practices I <p> While working on a project I came across the need to identify what the entity set name is so I can use that to add an updated entity to the set . In order to identify the plural I came up with two options : Use the Pluralization service to figure out the entity set name : The <p> So currently I 'm working on an entity model in which there are a lot of parent child relationships and for obvious reasons the entity framework is a great choice to build that entity model . However , I 've ran into the issue that although in the database several of this relationships already exist , when trying to <p> When using the entity @ @ @ @ @ @ @ @ @ @ this : " LINQ to Entities does not recognize the method getItem(Int32) method , and this method can not be translated into a store expression . " After doing some research the root cause of this issue was that in Linq I was trying to resolve a 
@@45151401 @5151401/ <h> Author 's posts listings <p> Which Web Server to use : Apache vs NGINX If you are looking around there are a lot of people using NGINX over Apache nowadays . NGINX for what I have read provides a lot of performance improvements that a site with a lot of visitors could take advantage of . I found a great article about the <p> How to : Send emails from WordPress in a Linux host WordPress uses Php libraries in order to send emails . What this means is that if your php is not set up correctly it wo n't know how to send emails . I was hoping all it would take was just a few entries with an SMTP server <p> Cloud Hosting Providers There are a number of Cloud Hosting providers out there . I just thought I would keep a list of the most interesting ones for reference here : Cloud Hosting Providers Provider Name Provider URL Brief Description Digital Ocean https : **30;3105;TOOLONG Never used them , but they offer attractive pricing and their features look highly competitive . <p> How to : @ @ @ @ @ @ @ @ @ @ occurred attempting to join the domain : The requested resource is in use . " One of my latest missteps was not completing the migration of all servers to the new domain name when I performed an Active Directory Domain Name change . Well , one of the unintended consequences <p> How to : Prevent Jetpacks mobile theme or WPtouch to get cached by W3 Total Cache Honestly the problem statement is more like : " How to prevent W3 Total Cache from serving the mobile theme to all my clients " . The issue arises when the first client that visits a page is a mobile client . W3 Total Cache <p> How to : Remove an Application from the New Relic Applications Dashboard The New Relic agent is a powerful - lightweight agent that helps you monitor the performance of your Web Application . Thanks to it you can identify potential problems , measure performance , and deliver a better user experience . Now , New Relic wont let you delete an application <p> How to : Uninstall the New Relic agent for a Web App The New @ @ @ @ @ @ @ @ @ @ you monitor the performance of your Web Application . Thanks to it you can identify potential problems , measure performance , and deliver a better user experience . So why would you want to uninstall it ? Well , <p> How to solve : - Apache- error- . htaccess : - Option MultiViews not allowed here I turned on some . htaccess rules in order for the cache to perform faster but unfortunately it came with some unintended consequences : A bunch of- 500 error- pages and in the log files one of the errors was Core:Alert / .. /. htaccess:Option MultiViews not allowed here . This one in particular happened when <p> How to solve : - Apache- error- . htaccess : ExpiresActive not allowed here , referrer : http : //Technology.Bauzas.com/ I turned on some . htaccess rules in order for the cache to perform faster but unfortunately it came with some unintended consequences : A bunch of- 500 error- pages and in the log files one of the errors was Core:Alert / .. /. htaccess : ExpiresActive not allowed here , referrer : http : //Technology.Bauzas.com/ The @ @ @ @ @ @ @ @ @ @ : RewriteEngine not allowed here , referrer : http : //Technology.Bauzas.com/ I turned on some . htaccess rules in order for the cache to perform faster but unfortunately it came with some unintended consequences : A bunch of- 500 error- pages and in the log files one of the errors was Core:Alert / .. /. htaccess : - RewriteEngine not allowed here , referrer : http : //Technology.Bauzas.com/ 
@@45151403 @5151403/ 55329 @qwx465329 <h> How to : Manage Windows Azure with PowerShell <h> - How to : Manage Windows Azure with PowerShell <p> You can manage your Windows Azure account via PowerShell with an array of commands at your disposal . Below is a quick reference guide of the commands available . <p> You can connect to Microsofts Azure Cloud using the Azure Powershell SDK . After you have installed the SDK you should be able to execute the commands in the reference guide to manage your **25;3137;TOOLONG . First you will need to connect to your account via the following instructions : 
@@45151405 @5151405/ <h> How to select the best RAID Level configuration <h> Selecting the Best RAID Level Configuration <p> Selecting an appropriate RAID level used to be a simple matter , but with time newer options are available and thus it becomes more difficult to select the most appropriate option . Several factors like the need for redundancy , budget , performance of read and write operations , availability of built-in hot spare and number of drives impact the final decision anyone makes . Below is a table that should help individuals better select the RAID level that is most adequate for the logical drives available on the storage space . Using your requirements for performance and reliability you can make a more educated decision on which level to chose . <p> - <p> RAID Level <p> Redundancy <p> Disk Drive <p> Usage <p> Read Performance <p> Write Performance <p> Built-in Hot-Spare <p> Minimum <p> Disk Drives <p> RAID 0 <p> No <p> 100% <p> www <p> www <p> No <p> 2 <p> RAID 1 <p> Yes <p> 50% <p> ww <p> ww <p> No <p> 2 <p> RAID 1E <p> @ @ @ @ @ @ @ @ @ @ 3 <p> RAID 10 <p> Yes <p> 50% <p> ww <p> ww <p> No <p> 4 <p> RAID 5 <p> Yes <p> 67 94% <p> www <p> w <p> No <p> 3 <p> RAID 5EE <p> Yes <p> 50 88% <p> www <p> w <p> Yes <p> 4 <p> RAID 50 <p> Yes <p> 67 94% <p> www <p> w <p> No <p> 6 <p> RAID 6 <p> Yes <p> 50 88% <p> ww <p> w <p> No <p> 4 <p> RAID 60 <p> Yes <p> 50 88% <p> ww <p> w <p> No <p> 8 <p> In general the more hard drives you have available in the array the better the performance . However , one must be careful nowadays as with increased storage space comes higher risks . Hard drives are rated and provide a number of errors per number of reads . Obviously the more space available the more likely a read/write error is to happen . Now , the more drives you have the higher that probability of an error occurring across the array . For that reason I recommend having a professional RAID controller that @ @ @ @ @ @ @ @ @ @ integrated ones before and whenever they come across a minor error the entire array becomes un-operational and lost . <p> Another important aspect to keep in mind is that certain RAID levels as RAID 5 and 6 use distributed parity , which means that the controller has to be constantly computing the parity information and that introduces a performance hit and utilization of the controller . Raid 1 or 10 in contrast have not such parity operations . 
@@45151406 @5151406/ <h> How to : delete default admin user in multisite <h> How to : delete default admin user in Multisite <p> As a Multisite user I have struggled with removing the admin rights of the first admin/user created as well as to delete/remove that user . Unfortunately WordPress does not allow you to change your username , and in case of the user Admin everyone trying to hack into your site is going to try that right off the bat . So then what are your options ? One would be deleting that user so that your new super admin has a name no one knows of ( word of advice , never publish something with that username otherwise they will figure that out ) or option two : Limit that users rights as much as possible so in case it is compromised they are simply a subscriber and cant do any harm on your site . <p> So getting back on topic How can you delete the default admin in a Multisite installation ? <p> Well , for some people seems that option two is their only real @ @ @ @ @ @ @ @ @ @ this user but you need to follow a set of steps to ensure it will allow you and you wont lock yourself out of WordPress : <p> Create a backup ! Db and Filesystem , but mostly- the- database . You are sort of about to- mess with WordPress and you know it does n't  like it . 
@@45151407 @5151407/ <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got @ @ @ @ @ @ @ @ @ @ ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <h> Exchange Server 2013 : HTTP 500 Errors for ECP and OWA ( Fresh Install ) <p> Sometime ago I changed my domain name ( you can read more about it here : - How to : Rename an Active Directory Domain Name ) to better address some business needs but unfortunately I had my Exchange Server offline as we are mostly using Exchange Online now . The issue is that if you have the servers offline and you complete the domain name change your old servers do n't  get the new changes . This ended up resulting on us being unable to bring that server back to fully functional status and losing our ability to manage our Exchange Hybrid configuration . Long story short I needed a new server to manage Exchange Online . I decided to grab a @ @ @ @ @ @ @ @ @ @ encountered a blank HTTP 500 page after providing my credentials to enter Exchange Server 2013 ECP . Below is a brief story of this long saga : <p> As many of you probably know by now , HTTP 500 error messages are generally attributed to either Certificate misconfiguration or Authentication misconfiguration of OWA and ECP . <p> Certificate problems arise when the certificates you are using on the backend are corrupted or there might be another issue like validation , etc . This generally can be easily resolved by assigning a new certificate to your Exchange WebServer . You can do this via Exchange Shell or the not so recommended way : directly on IIS . <p> Authentication problems arise when you are not using the same authentication methods on your front and backend Exchange WebSites . The default recommended approach is Forms Based Authentication . The recommendation at this point is to use the Exchange Management Shell to establish both the OWA and ECP sites to accept Forms Based Authentication . <p> Finally , there are some weird issues that might require you to completely remove OWA and ECP @ @ @ @ @ @ @ @ @ @ Management Shell ) and recreate them . By default Forms Based Authentication should be enabled but feel free to double check . <p> So that pretty much covers 99% of the problems out there referring to HTTP 500 error when accessing ECP . But in my case it was something much worse . <p> I had tried the three solutions described above with no avail and proceeded to upgrade to Cumulative Update 3 and the most recent Service Pack 1 in hopes that would do the trick . After many failures I came across a post that narrated a similar situation : <p> User had installed an Exchange Server from scratch to the letter ( per Microsofts instructions ) <p> User got Error 500 regardless <p> So he finally got Microsoft on the line and after much troubleshooting they determined a setting on AD was the culprit . I obviously tried deleting those entries and had no luck but it pointed me in the right direction : Something got really messed up on the backend that I could n't use Exchange ECP . I could had spent some time on figuring @ @ @ @ @ @ @ @ @ @ decided there were too many variables and too many configurations I was better off starting from scratch . As I mentioned , we had migrated to Exchange Online so wiping all local settings was not a big deal for us . <p> Once you have wiped all the Exchange Server Configuration information from Active Directory you are ready to proceed with the installation . You can follow this simple instructions to quickly deploy it : How to Install Exchange Server 2013 SP1 . You 'll notice that this time around there is no HTTP 500 blank page after you install Exchange and everything works as it should ! <p> Fortunately it seems that the hybrid configuration is also stored online or somewhere else . After re-installing Exchange 2013 SP1 ( with a previous CU I did have some issues ) it picked up my hybrid deployment and from elsewhere in AD my users and groups . Because of this I only did a Hybrid sync to reconfigure my connections between onPremise and Cloud Exchanges , assigned a certificate and establish my externals URLs and I 'm all good and working again ! @ @ @ @ @ @ @ @ @ @ traces of Exchange Server 2013 from your Active Directory then follow this simple instructions . This has worked thus far for me but perhaps I missed something so feel free to provide any feedback you may have . <p> Removing Exchange from AD is not something you might want to do carelessly . It should be obvious that wiping this information should be equivalent to starting your whole Exchange Infrastructure from scratch . This may result on losing mailboxes , emails , settings , email deliverability , etc . If you are familiar with the risks and understand what you are doing that 's fine , but if any of this sounds new to you then you should be most careful and seek professional help . <p> We are going to use the ADSIEdit tool which is used to edit Active Directory Metadata/Schema/etc . This is probably more delicate that messing with the Windows Registry so please proceed with extreme caution . <p> You 'll find this method is usually a last resort in some extreme cases like : <p> a ) The uninstaller failed halfway and there is no power that @ @ @ @ @ @ @ @ @ @ enterprise installation and the installer failed halfway and now you have an unclean installation from which you cant proceed ) <p> or <p> b ) You have a server that is kaput so you cant run the uninstaller and remove it from AD ( which in this case you might just want to remove some entries not entire subtrees like well do here . ) <h> I. Remove the entire Microsoft Exchange Configuration <p> First , open ADSIEdit ( which you can find on your start menu . Once opened go to Action -&gt; Connect to and there select Configuration like shown here : <p> Navigate to this path to delete the following two Exchange Subtrees : <p> CN=Configuration , DC=DOMAIN , DC=LOCAL <p> CN=Services <p> CN=Microsoft Exchange ( DELETE ) <p> CN=Microsoft Exchange Autodiscover ( DELETE ) <p> Once you are done open the connection but this time to the " Default Naming Context " in order to delete the Exchange security groups and objects : <p> CN=Default naming context , DC=DOMAIN , DC=LOCAL <p> CN=Microsoft Exchange Security Groups- ( DELETE ) <p> CN=Microsoft Exchange Security Objects- ( DELETE @ @ @ @ @ @ @ @ @ @ <p> There are a few Active Directory users that are generated automatically by Exchange . Some serve as Discovery services , others are used to monitor the health of the system . Regardless these will no longer be needed if you have permanently removed Exchange from your organization : <p> DiscoverySearch MailboxGUID <p> Exchange **25;3164;TOOLONG <p> FederatedEmail.GUID <p> Migration.GUID <p> *SystemMailboxGUID <p> *HealthMailboxGUID <h> III . Remove settings from a server <p> If you have access to your Exchange server you can delete a few things to leave it almost in pre-Exchange state . I strongly suggest you simply start from a fresh Windows Installation as it is hard to truly leave a server in a pre-exchange state . <h> How to : Install Exchange Server 2013 SP1 <p> As usual every time you try to install a new version of Exchange Server there are new caveats and what you expect will take 30 minutes takes longer . This time around was no exception . I am still hoping the day will come the installer decides it is going to do what it takes to do everything automatically instead of @ @ @ @ @ @ @ @ @ @ supposed to install on your own . Though there are good news with Exchange 2013 SP1 : The Edge Transport Server is back ! Surely lots of people out there are very excited about this ; Fortunately for me I do n't  care anymore . We are moving our entire infrastructure to the cloud as maintaining a server for just a few users is not worth the time and effort . Regardless we needed to deploy a server OnPremise to modify our hybrid configuration so we are including those details here . <h> I. Readiness Checks <p> As I mentioned there are new caveats or shall I say , goodies you need installed for Exchange Server to do its wonders . If you try to deploy it as is you might encounter a Prerequisite Analysis that shows a bunch of errors : <p> Fear not , most of them can easily go away with the simple execution of a Powershell command . I used to go line by line clicking on each link figuring out what I needed to do really a huge waste of time . <p> First off @ @ @ @ @ @ @ @ @ @ schema you will need the Remote Tools Administration Pack , If you do n't  you will encounter issues like : <p> " was run : " There was an error while running ldifde.exe to import the schema file C : **29;3191;TOOLONG **31;3222;TOOLONG . The error code is : 8224 . More details can be found in the error file : LONG ... The Exchange Server setup operation did n't  complete . More details can be found in ExchangeSetup.log located in the &lt;SystemDrive&gt; : ExchangeSetupLogs folder . <p> Entry DN : LONG ... Add error on entry starting on line 1 : Operations Error The server side error is : 0x21a2 The FSMO role ownership could not be verified because its directory partition has not replicated successfully with at least one replication partner . The extended server error is : 000021A2 : SvcErr : DSID-030A0B6B , problem 5012 ( DIRERROR ) , data 8610 <h> Nginx : How to correctly use Time and Size postfix / parameters <p> I was struggling on figuring out why my cache size was not growing as intended . I had setup my configuration so that @ @ @ @ @ @ @ @ @ @ refreshed often . The idea being able to cache my entire site but at the same time having NginX frequently checking for new updates . If the backed or upstream server was down , then most likely a " stale " copy of the page would be available and served to the client . That way visitors would suffer of reduced downtime . But the problem I was having is that the cache was growing up to a point and then started to decrease and oscillated around a few mb . Long hours checking headers and debugging until I finally realized I was n't using the Time parameters correctly . I was indicating " m " in the configuration thinking that stood for months but then it hit me What is the postfix for minute ? So there you have it , my cache was n't growing because it was getting deleted shortly after it was saved . Quite the undesired behavior and result of my lack of attention . <p> Regardless if you make silly mistakes like I do , I always find myself in need of reference material to figure @ @ @ @ @ @ @ @ @ @ time parameters in NginX . Because of that I have prepared two tables to help anyone who is also in need of a quick reference when configuring Nginx : <h> Size <p> By default when you specify a parameter of type size it is defaulted to bytes . If you wish to indicate something more practical you could use the postfix m or k . As fas as I am concerned g for gigabyte also works but then again I could not find it in the documentation . I have used it in my configuration file with nginx -t not complaining and the behavior looks correct ( but truly I have not come close to the limit I set so I cant say for sure ) <p> Syntax <p> Description <p> k , K <p> kilobytes <p> m , M <p> megabytes <h> Time <p> There is more variety with the time parameters than with the sizes ( doubtful many would need fore than g ) . Some might expect the default might be in milliseconds as that is the smaller unit that nginx recognizes , but it actually defaults @ @ @ @ @ @ @ @ @ @ milliseconds <p> s <p> seconds <p> m <p> minutes <p> h <p> hours <p> d <p> days <p> w <p> weeks <p> M <p> Months ( 30 days ) <p> y <p> Years ( 365 days ) <p> The cool thing with time is that you could combine different units . For example : " 1h 30m " is used for one hour and thirty minutes . Obviously this is far cooler than having to type 90m . If you want to combine postfixes , you need to order them from most to least significant ( 30m 1h ) would not work . <p> One of the main reasons why I had left my admin panel open to server non-ssl requests was that Zemanta Editorial Assistant would not work properly behind HTTPS . I did some looking around and found that the same scripts were being sent over to the browser so I was at a loss of what was causing the issue . Pretty much everytime I worked on a post the " Content Recommendations " , " Related Articles " , " In-Text Links " and the " @ @ @ @ @ @ @ @ @ @ on screen but have no content on them . This pretty much render them useless . Because of that I could n't force myself to move to an SSL only WordPress backend . <p> Finally though I got tired of having to log-in every time I switched between the HTTPs and HTTP version of the site . I decided it was time to resolve this issue . I went in and digged through the HTML trying to understand what was going on : Was Zemanta not using https ? Why is this not working . <p> To my dismay the answer was much simpler than I could had imagined . It turns out because Zemanta runs a script that is not part of this domain and does not travel via HTTPs , most browsers will block it . Take a look at Chrome for example : <p> Blocking this script made Zemanta unable to operate . This resulted as I mentioned on the different sections being present on the screen but unable to get content from the Zemanta servers . <p> The good news is just clicking on the Load unsafe @ @ @ @ @ @ @ @ @ @ as far as I know let you chose which " unsafe " scripts to run . This could potentially present a security risk . I run my own WP install so I am assuming all the scripts I have are safe , but if you visit other sites you might have your doubts . It is even possible your site has been compromised and truly unsafe scripts are also being executed with scripts you want like Zemanta and there is no way to execute some and not all . <p> As far as I can see the Zemanta scripts are being blocked not because they do n't  travel via HTTPs but because they are not part of the domain the website is on . I see their own domain name and Amazons domain as well . At this point there is little you can do if you want the message not to show up . One alternative is hosting your own versions of the scripts therefore having them load from your own server / domain avoiding chrome to consider them unsafe . This however could go against the terms of @ @ @ @ @ @ @ @ @ @ consult a lawyer . But if this is possible Google has a PageSpeed module for Apache and NginX that allows for third party components to be rewritten for your own domain with the aim of lowering DNS lookups and connections to more domains speeding up the site . You will see there they will also have a disclaimer that doing so could go against the terms of use of said code . <h> How to : Configure memcache to use a unix socket <p> I have a server which is going to have its own memcached instance installed on itself so I was thinking using a unix socket instead of tcp should improve the overall performace at least a little . I had tried doing the same with MySQL and I was pleased on the improved speed if only a few ms ( it adds up I think ) . Well , with no further due here it is : <p> First , you need to edit the main configuration file : - # nano /etc/memcached.conf <p> # Start with a cap of 64 megs of memory . Its reasonable @ @ @ @ @ @ @ @ @ @ will grow to this size , but does not start out holding this much # memory -m 64 <p> # Default connection port is 11211-p 11211 <p> # Run the daemon as root . The start-memcached will default to running as root if no # -u command is present in this config file -u memcache <p> # Specify which IP address to listen on . The default is to listen on all IP addresses # This parameter is one of the only security measures that memcached has , so make sure # its listening on a firewalled interface. -l 127.0.0.1 <p> # Limit the number of simultaneous incoming connections . The daemon default is 1024 # -c 1024 <p> # Lock down all paged memory . Consult with the README and homepage before you do this # -k <p> The main aspects we are going to look is the port , listening port , socket , and octal file mode : <p> Comment out the default port like so : <p> # -p 11211 <p> Comment out the IP address option : <p> # -l 127.0.0.1 <p> Add the following @ @ @ @ @ @ @ @ @ @ <p> -s /var/run/memcached.sock <p> Optionally you could configure the octal file mode of the socket using the -a paramter : <p> -a 0766 <p> Note that the two TPC options -l ( listening ip ) and -p ( port ) were commented out and we introduced socket specific options -s ( socket ) and -a ( socket access file mode ) . <p> Furthermost note that we used the /var/run folder to avoid worrying about creating a special directory for memcache . It is though a good idea to establish a dedicated space to separate all memcache sockets and use a name that indicates the instance on the configuration and socket files . This though requires a little more work . After deciding on your nomenclature and directory we are going to configure the init.d script to create said folder if it is not available . Edit the memcached init.d script and find the following lines were you will insert the bold code : <h> General Commands Manual : dphys-swapfile <h> NAME <h> SYNOPSIS <h> DESCRIPTION <p> dphys-swapfile computes the size for an optimal swap file ( and resizes an @ @ @ @ @ @ @ @ @ @ file , unmounts it , and and delete it if not wanted any more . <h> OPTIONS <p> There is only one parameter , an command , which can be either of these : setup Tells dphys-swapfile to compute the optimal swap file size and ( re- ) generate an fitting swap file . Default it 2 times RAM size . This can be called at boot time , so the file allways stays the right size for current RAM , or run by hand whenever RAM size has changed . swapon and swapoff These run the swapon and swapoff commands on the swapfile . Note that direct swapon/off from /etc/fstab is not possible , as that is ( at least on Debian ) done in the same script that mounts /var ( which is where the swap file most likely resides ) . And we need to do our setup between those actions . So pass up /etc/fstab , and do our own swapon/off. uninstall Gets rid of an unwanted swap file , reclaiming the disk space . <h> CONFIG <p> The config file /etc/dphys-swapfile allows the user to @ @ @ @ @ @ @ @ @ @ file is a sh script fragment full of assignments , which is sourced . Standard sh syntax rules apply . Assignments are : CONFSWAPFILE Set where the swap file should be placed . Defaults to /var/swap . It is unlikely that you will need to change this , unless you have very strange partitioning , and then you will most likely be using an swap partition anyway . CONFSWAPSIZE Force file size to this . Default is 2*RAM size . This is unlikely to be needed , unless in strange diskspace situations . Note that swap enabled and smaller than RAM causes kernal-internal VM trouble on random systems . CONFSWAPFACTOR Set the relation between RAM and swap size . Must be an integer . Defaults to 2 which means swap size = 2 * RAM size CONFMAXSWAP Set maximum size of the swap file in MBytes . Defaults to 2048 which was the former kernel limit for the swapfile size and is now a limit to prevent unusual big swap files on systems with a lot of RAM . <h> FILES <h> EXAMPLES <p> dphys-swapfile is usually run at system @ @ @ @ @ @ @ @ @ @ script , such as this ( minimal ) one : # ! /bin/sh # **26;3255;TOOLONG - automatically set up an swapfile # author franklin , last modification 2004.06.04 # This script is copyright ETH Zuerich Physics Departement , # use under either modified/non-advertising BSD or GPL license case " $1 " in start ) /sbin/dphys-swapfile setup /sbin/dphys-swapfile swapon ; ; stop ) /sbin/dphys-swapfile swapoff ; ; esac exit 0 If an sysadmin wants to have his swapfile in annother place , say /var/run/swap , he can use : In /etc/dphys-swapfile : **26;3283;TOOLONG <p> This is a rather unhelpful message , as it does not specify what requirements it is missing . This is specially considering that their Installation instructions states LONG ... <h> System requirements <p> Before you start using the Google Publisher Plugin , make sure you meet the following minimum requirements : <p> PHP version : minimum 5.2.0 <p> WordPress : minimum 3.0 <p> Ensure that you can install third-party WordPress plugins . <p> So what am I missing ? Pretty sure I am running version 5.5.3 and WordPress is version 3.8.1 <p> I decided I needed to @ @ @ @ @ @ @ @ @ @ doing and there it was in the Utils.php file : <p> If you look at the code you will see all the- **30;3311;TOOLONG function will show the error message if any of the two conditions are met : <p> The filter , json , pcre and SPL extensions are loaded <p> The PHP version is 5.2 or above <p> So now that we know what we need how do we get it ? <p> Well , Filter , PCRE and SPL have been added by default over time to PHP and by version 5.5.3 you can count on them . At the time of writting 5.5.3 is the version that - comes when you install php using apt-get install php5 on Ubuntu . <p> So that leaves us with json . To get it installed and loaded simply do the following : <p> Install the extension via repository:apt-get install php5-json <p> Restart your php service : <p> service php5-fpm restart <p> And you 're done ! If you are using a different version that does n't  come with the other 3 extensions you could edit the utils.php file to show the @ @ @ @ @ @ @ @ @ @ How to : Configure Swappiness in Ubuntu <p> In one of my previous articles on how to setup virtual memory / swap memory in Ubuntu I covered how to install a swap space for a Windows Azure VM . I deployed swap mostly as a precaution against running out of memory which could result on applications unable to work properly and the site could go down . To avoid that you deploy swap which most likely decrease the speed of your services but at least they wont crash . It effectively buys you time to resolve your memory issues at the expense of using the HD resulting in added latencies . So thus far setting up your swap memory seems genius , but beware , if your swappiness is not set up properly you will face unecessary latencies . In my case , I had still over a gigabyte of memory available but I was already swapping a lot . There is no point in swapping when there is plenty of RAM left ; You are just moving information in and out of memory that could easily fit all in @ @ @ @ @ @ @ @ @ @ system use the swap space . <h> What is Swappiness ? <p> The swappiness parameter in Ubuntu controls the tendency of the OS kernel to move memory ( information ) used by processes out of physical memory and onto the swap disk . As mentioned above using a hard drive is supposed to be much slower than using RAM by nature . This results in applications taking longer to respond , particularly those who are being swapped to disk ( the others suffer indirectly because of the added IO operations but at least they are present in RAM already ) <p> There are some important things to know about how to use this parameter : <p> Accepted Swappiness values range from 0 to 100 inclusively . <p> A value of 0 indicates the kernel to avoid swapping at all costs . <p> A value of 100 indicates it should aggressively swap information on memory . <p> So thus far you must be thinking after reading my thoughts on swappiness that 0 is the way to go : be careful with that choice . The correct answer depends on many things @ @ @ @ @ @ @ @ @ @ system . For example , assume you use a value of 0 and memory is almost full . If you try to launch a large application there will be a delay until the system is able to free enough memory by swapping processes before you can use your application . Take the other extreme of 100 : You should at all times have plenty of RAM to launch large processes but your overall response rate of your machine will take a performance hit . <p> Ubuntu has determined by default that a value of 60 is ideal for most users . It allows for memory to be available to launch new applications while is not that aggressive as to cause a huge performance hit . But as I mentioned the best value for your machine depends on the use you give the system . For example , I am running a server that eventually launches processes to service incoming requests but their size is relatively small and for the most part I always have plenty of RAM to handle all the services needed . In my case I like using @ @ @ @ @ @ @ @ @ @ your general RAM usage patterns and feel free to experiment . I expect boundary values ( 0 and 100 ) are too extreme but you can see what is the difference between 25 and 75 for example . <h> How do I configure Swappiness ? <p> This is rather easy . There is a file ( /proc/sys/vm/swappiness ) that it is used during operation that indicates the current swappiness the system is running under . Changing this value would result on the swappiness of the system being changed for the duration of the kernel ( i.e. until you reboot / shutdown &amp; turn back on ) . <p> If you want to see the value I recommend using cat over nano to avoid accidentally editing the file : <p> $ cat /proc/sys/vm/swappiness 60 <p> You 'll probably see the default value of 60 unless it was modified already . As I mentioned you could modify the file , swapoff and back on to have the change take place only during your current session . If you modify the value via the sysctl command ( I recommend not editing the do not @ @ @ @ @ @ @ @ @ @ else ) . Said change would be take place only after you reboot the machine . Suppose you want to use a swappiness level of 15 : <p> $ sysctl vm.swappiness=15 <p> If you want you can execute swapoff -a and swapon -a to get your new swappiness value applied to the system ( instead of having to reboot this could be an attractive option ) . <p> Do n't  forget to check your memory usage with free -m to see the swap size and usage . <h> What swappiness value do you recommend ? <p> This is my personal opinion and you should monitor your swap usage and performance to fine tune it . I believe this is a good starting point but use your own criteria , experiment and decide what is best for you . <p> Recommended Swappiness <p> Memory generally available <p> Relative size of new processes <p> Recommended Swappiness <p> A lot <p> Small <p> &lt; 10 <p> A lot <p> Medium <p> 10 &lt; x &lt; 20 <p> A lot <p> Large <p> 15 &lt; x &lt; 30 <p> Medium <p> Small <p> 10 @ @ @ @ @ @ @ @ @ @ &lt; 25 <p> Medium <p> Large <p> 25 &lt; x &lt; 45 <p> Low <p> Small <p> 35 &lt; x &lt; 60 <p> Low <p> Medium <p> 50 &lt; x &lt; 70 <p> Low <p> Large <p> 65 &lt; x &lt; 90 <p> It all becomes relative but you get the general idea : The more memory you generally have free or available the lower the swappiness you can use . Also , the larger the new processes you launch the larger the swappiness you 'll need so they can properly fit in memory if need be . Consider the following scenario : You have a machine with 16gb of RAM and you generally only use 2gb . You have never used more than 13gb and do n't  expect to . So even if new processes use a lot of RAM you never really run out of memory so a value at 10 or below would be recommended ( even 0 ) . <p> If you want feel free to provide via the comments your systems specs , memory usage patterns and I can help you pick a starting value to @ @ @ @ @ @ @ @ @ @ VMs and moving their storage files from one location to another . I do n't  recall exactly what I did that caused this issue but I was unable to start my VM . I would get every time the following error message : <p> Hyper-V Manager <p> An error occurred while attempting to start the selected virtual machine(s) . <h> The Problem <p> Not sure what originates this issue , but as the error message indicates Hyper-V does not have enough permissions ( /sufficient priviledge ) to open the attached hard drive . If I open a Virtual Machine Disk of a VM that works I can see there is an account on the Security settings tab that looks like virtual machine I 'd . When I go into the VHDX file for the one that is not working I can see it only has the standard accounts that are inherited by the parent folder . Clearly at some point the file permissions were lost , hence it can not be opened . <h> The Solution <p> There are many things you could do at this point . The idea at @ @ @ @ @ @ @ @ @ @ account the rights it needs to open the hard drive . <p> The most tempting solution is to simply go to the security tab and add the account . Set the permissions like those of other VMs ( At the very least read and write permissions . I fear there is a lot of room for error on this approach so it is not my favorite . There is always the possibility you missed a setting and something might not work as intended . This brings me to my second and favorite solution : <p> Use Hyper-V manager to remove the disk from the Virtual Machine and then go back in to add it again . Apparently every time you add a virtual hard disk it will execute the required commands to give that Virtual Machine account the required permissions to the attachments . I say apparently because I cant imagine how this did n't  happen in my case and I ended up getting the error message . Regardless , doing this did solve my issue and it is the Hyper-V manager fixing the permissions so I feel more confident @ @ @ @ @ @ @ @ @ @ many other ways to modify file permissions that you could use . Most importantly , there is always PowerShell . There is a " AddAccessRule " command you could apply to a VirtualMachine.HardDrive that will , of course , add the required access rules so that the VM can access said attached Virtual Hard Drive . Pretty neat right ? <p> There is one said script that you could use found here : - LONG ... I have n't tried it but it looks as it would work . I only had one VM to fix so it is much faster to do the Hyper-V manager operation than trying to download and then validate the script before executing it . But if you want to do this for a number of VMs you could use this to run it against 
@@45151410 @5151410/ <p> The Joule , Dallas 1530 Main Street , - Dallas , - TX- 75201 Phone : - ( 214 ) 748-1300 Site : - http : **25;3343;TOOLONG My Rating : 4 1/2 stars Category : 4 Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to <p> Cane Rosso Category : - Pizza 2612 Commerce St Dallas , - TX- 75226 Neighborhood : Deep Ellum ( 214 ) 741-1188 www.ilcanerosso.com Menu Yelp : - http : **36;3370;TOOLONG My Rating : 4 1/2 stars Summary : One of the best pizza place Ive been in . Great variety and different from the ordinary , great quality of ingredients and superior taste . It is not expensive or cheap but their rating makes it 
@@45151411 @5151411/ 55329 @qwx465329 <h> Custom NginX Distribution Available Packages <h> Custom NginX Distribution Available Packages <p> As part of the custom NginX distribution available on this site , there are a few packages you can chose from depending on your needs that can be deployed . Below is the list of packages , additional information and description : <h> Different packages available : <p> Package : - nginx <p> Architecture : all <p> Depends : nginx-full nginx-light , <p> This is a dependency package to install either nginx-full ( by default ) or- nginx-light . <p> Package : - nginx-doc <p> Architecture : all <p> Section : doc <p> Depends : lsb-base ( &gt;= 3.2-14 ) , <p> This package provides extra documentation to help unleash the power of Nginx . <p> Package : - nginx-common <p> Architecture : all <p> Depends : lsb-base ( &gt;= 3.2-14 ) , <p> Replaces : nginx ( &lt;&lt; 0.8.54-4 ) , <p> nginx-extras ( &lt;&lt; 0.8.54-4 ) , <p> nginx-full ( &lt;&lt; 0.8.54-4 ) , <p> nginx-light ( &lt;&lt; 0.8.54-4 ) <p> Breaks : nginx ( &lt;&lt; 0.8.54-4 ) , <p> nginx-extras ( @ @ @ @ @ @ @ @ @ @ , <p> nginx-light ( &lt;&lt; 0.8.54-4 ) <p> Suggests : nginx-doc , fcgiwrap <p> This package contains base configuration files used by all versions of- nginx . <p> Package : - nginx-full <p> Architecture : any <p> Depends : nginx-common ( = $source:Version ) , <p> Provides : httpd , - nginx <p> Conflicts : nginx-extras , - nginx-light , - nginx-naxsi <p> Suggests : nginx-doc ( = $source:Version ) <p> This package provides a version of nginx with the complete set of- standard modules included ( but omitting some of those included in- nginx-extra ) . <p> It includes an interceptor ( listening on TCP port 8080 ) , which monitors- HTTP requests from naxsi , and an extractor ( running on TCP port 8081 ) , - which reads the database and prints reports about blocked requests . <h> 1 ping <p> right ? Well , now it is still easy but you need to decide which package to install . If you read our Custom NginX Distribution post you can find out more about the different packages and what they contain . Sometimes we 
@@45151412 @5151412/ 55329 @qwx465329 <h> Category Archive : Servers <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker I am going to have to start with a confession , " Resolved " might be a bit of a stretch . For starters , the best case scenario looks like recovering the information @ @ @ @ @ @ @ @ @ @ , <p> How to : " Change product key " in Windows 8 or in Windows Server 2012 I 've learned over the years to not activate a Windows product until I am confident it is stable . I say that because I used to activate Windows right after installation and either because of third party update , malfunctioning hardware , etc . I had <p> How to : Enable Shadow Copy or Previous Version in Windows 2012 R2 ? I recently had a bit of an issue with a program . It uses an access database and one of the employees modified it but we needed to revert the changes . Seemed simple enough , just reach out to pick a previous version from the <p> Ubuntu 14.04 and above in a Generation 2 Hyper-V Virtual Machine ( VM ) As most of you know , a Generation 2 Hyper-V Virtual Machine is generally reserved for Windows 2012 or 64 bit versions of Windows 8 as the New virtual Machine Wizard specifies : Generation 2 This virtual machine generation provides support for features such as <p> How to : Configure @ @ @ @ @ @ @ @ @ @ TMG ) 2010 Microsofts Thread Management Gateway is a solution that aims at providing advanced firewall and reverse proxy capabilities to the enterprise . Unfortunately Microsoft wont be releasing new versions in the future so I have n't been writing much about it lately ; However , many <p> How to : Configure your environment to host Hyper-V virtual machines on Shared Storage ( SMB 3.0 ) Well , as embarrassed- as I am , I 'll confess I wish I knew what I know now or found a post like this long ago . When I read you could now host your Hyper-V virtual machines on shared storage I got super <p> Resolved : Passwords must meet complexity requirements Recently we were working on synchronizing passwords between an onPremise AD deployment and Azure Directory Services . As part of that we had to enable a strong password policy ( You can configure this security setting by opening the appropriate policy and expanding the console tree : Computer ConfigurationWindows SettingsSecurity SettingsAccount PoliciesPassword 
@@45151414 @5151414/ <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <h> United Airlines : Update Priority <h> United Airlines : Update Priority <p> This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it ca n't be secured at the time it is applied to a reservation , it gives you preference on the waitlist over others who are doing the standard complimentary upgrade . I would advice checking with- United as they change their policies from time to time . <p> Upgrade priority . <p> Upgrade waitlists will be prioritized by status , fare class and time of request . Starting at five days ( 120 hours ) before departure , select flights will also be eligible for Complimentary Premier Upgrades . The confirmation windows will be : 
@@45151415 @5151415/ <h> Which Web Server to use : Apache vs NGINX <p> If you are looking around there are a lot of people using NGINX over Apache nowadays . NGINX for what I have read provides a lot of performance improvements that a site with a lot of visitors could take advantage of . I found a great article about the pros and cons of each web server ( http : **35;3408;TOOLONG ) and I thought I could do a quick re-cap of the most important points : <h> Performance : NGINX <p> nginx is faster at serving static files and consumes much less memory for concurrent requests because Nginx is event-based it does n't  need to spawn new processes or threads for each request , so its memory usage is very low <p> WordPress.com has found nginx to be the only load balancer able to handle 8000 live traffic requests per second . <h> Features : Apache <p> So the decision at the end boils down to what is more important . If you love . htaccess then probably Apache is the way to go . If you are @ @ @ @ @ @ @ @ @ @ the way to go . You could always find workarounds , after all NGINX is very popular even among WordPress installations . There are converters to assist you in migrating over form an . htaccess environment like : - http : //winginx.com/htaccess 
@@45151416 @5151416/ <p> I am writing this post mostly because there is in my opinion a much better way to install MySQLTuner on your machine that the usual apt-get install MySQLTuner . As many of you would probably already know MySQLTuner is a well known and widely use tool . It gives you a very nice summary of how your configuration is performing and points you were you can make improvements to get even better performance out of your database server . I generally recommend people to install Percona ( which is a drop-in replacement for MySQL ) . Percona is based on MySQL code but its cooler ( they add more performance features you might only find in the commercial MySQL version and they do a better job of keeping up to date than Oracle does ) . Because of that I even wrote on the article : - How to : Install MySQL in an Ubuntu Server- instructions on how to install Percona instead of MySQL at the end as a suggested alternative . <p> Regardless , as I mentioned earlier the main reason for this article is to @ @ @ @ @ @ @ @ @ @ And why is that ? Well , let 's see what running that command has to say on one of my server box : <p> As highlighted on bold , the installer will try to remove Percona server , will install instead mysql and a bunch of other libraries . Most of the time this is fine but many times you 'll find you only want the tool and you 're happy with Percona as I am . So what to do ? <p> Fortunately MySQLTuner is a tool written on perl and only requires client libraries to connect to mysql which is not a big deal ( percona client is available ) . Simply go to : - https : **34;3445;TOOLONG and download the mysqltuner.pl file to get the latest and greatest ( do a wget- LONG ... Once you have downloaded the script , all you have to do is go to the location where you saved it and execute perl mysqltuner.pl and voila ! <p> Now , if you are looking for some tips on how to use this tool and recommended changes here are a few highlights : <h> @ @ @ @ @ @ @ @ @ @ ) then definitively change the waittimeout . <p> By default it is 8 hours but if for some reason the application does n't  close the connection it might stay there hanging for up to 8 hours ( so you start getting a lot of sleeping open connections which consume resources . <p> waittimeout = 30- - - - #default is- 28800 <p> You can leave the interactive timeout alone . This is generally a higher value than waittimeout and as you might guess it requires interaction for it to remain on wait state . <h> II . Change the max number of connections . <p> On my installation it was 500 ! Each connection uses a bit of resources as well even if not in use . MySQLTuner can help you see over time what is the maximum number of connections you end up using so you can adjust accordingly . For example , if you are using PHP-FPM you will most likely not use more connections than the max number of child processes you have . Consider that and examine the output of MySQLTuner to decide what 's the best @ @ @ @ @ @ @ @ @ @ having MySQL running for at least 24 hours , preferably 7 days so you get metrics for an entire week ) at about 80% ( so increase the maximum usage by 25% when setting up this variable ) <p> As I mentioned earlier , you want to run this against your server at least 24 hours after you have started MySQL so the recommendations are most accurate ( I would recommend running it 7 days , 30 days , etc. again as you might get more insight the more use the server gets ) . But please do n't  feel you wo n't get any important information if you run it say an hour or immediately after starting . Immediately after starting you can see if you screwed something up with memory . While after an hour might have valuable insight if you are just starting to optimize your setup . For example , here I already got to use all the size for my temporary tables so this error most likely wont be going away no matter how long I run MySQL . With time you 'll learn which recommendations you @ @ @ @ @ @ @ @ @ @ right away . <p> Making good use of that recommendation also note the previous one " When making adjustments , make tmptablesize and maxheaptablesize- equal " . <p> All changes to your /etc/mysql/my.cnf usually require a service restart . I have tried the reload parameter but mysqltuner does not seem to note the change . <p> My final piece of advice is to focus on performance metrics and the recommendations section : <p> If you are sharing your server box with other applications try to have the Maximum possible memory usage within your estimates of how much RAM you are willing to commit to MySQL <p> As I mentioned make sure your Maximum Number of Connections is at about 80% of your available ones . Run MySQL for a while and add 25% more connections to the maximum you get . <p> Configure Slow Queries . Indicate how many seconds constitute a slow query and see if you get a few or too many . <p> Make sure all the buffers have enough RAM . <p> Configure your swappiness to a low value . Swapping RAM does take a bit @ @ @ @ @ @ @ @ @ @ are using most of these buffers / cache to avoid having to go to the disk to get information . For more information on configuring how aggressively- to swap look at this post : - How to : Configure Swappiness in Ubuntu <p> Keep calm and continue improving your configuration ; ) . You 'll find right out of the box no server is the same . With time and looking at the metrics you 'll see if you primarily write or read , your activity and memory needs and will size the server better and adjust parameters to the load you are receiving . Percona has a tool to help you come up with some starting values at : - https : **26;3481;TOOLONG . Keep in mind you should understand them and apply them perhaps individually instead of just copy pasting the entire thing . If your server has been used before then some of the new settings might make it crash as it created tables with different settings . Also , using some parameters might break applications like WordPress. 
@@45151418 @5151418/ 55329 @qwx465329 <h> Category Archive : Excel <p> How to : Create Web query files for use with Excel for Mac Web queries allow you to query data from a specific World Wide Web , Internet , or intranet site and retrieve the information directly into a Microsoft Excel worksheet . Microsoft Excel includes some sample Web queries . Definition of a Web Query File A Web query <p> Resolved : Where and How to use Name Management function in Excel 2011 for Mac So this is one of those things I keep referring back to . One of the key things I do when I work in Excel is control the values that can be entered in certain fields . I am clearly not good at <p> How to obtain stock quotes in Excel I was working on creating a spreadsheet to calculate profits and losses on options positions but did n't  know how to populate excel with stock quotes . Back in the day there used to be an interface to get stock quotes with the MSN Money site but it is not 
@@45151419 @5151419/ <p> So as I was working in moving a site from one installation to another using the WordPress export/import utility I noticed something : There was so much data going around that the server was erroring out . I opened the- 30mb- XML export file- ( which is ridiculous for only 300 posts ) and found myself with lots of duplicate post meta data including lots of empty entries from Yoast WordPress SEO . I proceeded to manually delete duplicates the best I could with regular expressions but found myself in need of a better way to keep my database lean . <p> I developed some SQL queries to address the empty values I wanted to remove from the database but as of the latest release of Yoast WP SEO they offered the functionality of not persisting empty values and a method to clean- the database in a future release . The author also released a list of SQL queries that are more complete than mine ( after all , he does know all the meta data he persists while I was just going through the db seeing which @ @ @ @ @ @ @ @ @ @ below is his list of - queries to clean your database . Keep in mind you are going to have to change your table name to match your WP prefix and Blog I 'd if you use multisite : 
@@45151420 @5151420/ 55329 @qwx465329 <h> Tag Archive : Web Server <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got it <p> How to : Cache static HTML with CloudFlare ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <p> Resolved : Blank pages when using NginX with php-fpm As of late I have been a bit busy so keeping up with updates to the web server has been pretty much neglected . Because of that I decided to switch to the nginx.org supported distribution to get the latest updates although that means a distribution without any <p> Changes @ @ @ @ @ @ @ @ @ @ the 1.5.13 release of NginX to version 1.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update makes the development / mainline version 1.5.13 part of the stable branch. - Because of this several new features are included and we highly suggest people upgrade to <p> How to : Install Exchange Server 2013 SP1 As usual every time you try to install a new version of Exchange Server there are new caveats and what you expect will take 30 minutes takes longer . This time around was no exception . I am still hoping the day will come the installer decides it is going <p> Nginx : How to correctly use Time and Size postfix / parameters I was struggling on figuring out why my cache size was not growing as intended . I had setup my configuration so that pages would be cached for a really long time but refreshed often . The idea being able to cache my entire site but <p> How to : Install Ruby in an Ubuntu Server to use it with NewRelics Plugins : Memcached @ @ @ @ @ @ @ @ @ @ New Relic I find myself trying to figure out how to perform each of the steps . Also , most of them require other applications to be installed as well like Ruby <p> How to : Install Ruby on Rails using RVM This is just a quick complement of the article How to : Install the NginX Agent for NewRelic in an Ubuntu server . As I mentioned there if you need Ruby for something other than running a plugin agent like in a development web server RVM is the way <p> How to : Install the NginX Agent for NewRelic in an Ubuntu server Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . For example , take this instructions on how to install the NginX agent to get monitoring on your web server : <p> Resolved : 400 Bad Request Request header or Cookie too large when using NginX One of the issues I ran into when using NginX was : " 400 Bad Request Request header or Cookie too large . " @ @ @ @ @ @ @ @ @ @ working properly and this behavior was only experienced when doing 
@@45151421 @5151421/ 55329 @qwx465329 <h> Category Archive : MySQL <p> How to : Install and use MySQLTuner to Optimize your MySQL configuration / performance I am writing this post mostly because there is in my opinion a much better way to install MySQLTuner on your machine that the usual apt-get install MySQLTuner . As many of you would probably already know MySQLTuner is a well known and <p> How to : Get the size of your Databases in MySQL Query Browser Imagine you want to transfer data from one server to another and you need to know the size of the databases you are thinking of transfering . Unlike Microsoft SQL Server which graphical interface allows you to see the size of the database files <p> MySQL : How to list all available tables in the system Because sometimes you just need to know ( the available tables are out there so you can use that list in a script ) . If you are running MySQL Workbench you can execute the following SQL Query and export the results to a file : select tablename from <p> How to : Install @ @ @ @ @ @ @ @ @ @ deploying MySQL as part of a " LAMP " installation or whatever flavors you like best , Ubuntu is a good OS to do so . I say that because they make the installation super easy from the command line with- apt-get , it does n't  get much simpler <p> How to : Reset the Auto Increment value for a MySQL table Although this is something most people wo n't need to do , there are some scenarios where you want to reset the I 'd value you have in a column . In my case for example , I am working with WordPress and the blog I 'd is set based 
@@45151422 @5151422/ <p> How to : Create an SSH connection using Terminal on Mac OS X &amp; save the configuration for later use / shortcut So now that I bought myself a new Mac I decided I would try to avoid by all means installing Windows on it . I use a lot of applications ( and some games ) that either <p> What is a privileged port on a Mac / Darwin ? Recently I was trying to connect to a remote SSH server using my MacBook . At one point when trying to configure a Tunnel I got an error saying : Privileged ports can only be forwarded by root My first instinct was to make sure I had <p> Why does my Mac OSX say " update needed " on every boot ? If you recently reinstalled your Mac with OS X you might notice that every time you reboot when you try to login instead of showing the username of the first user account- created it shows an icon with the text " Update Needed " . You can of <p> How to : Setup your iOS @ @ @ @ @ @ @ @ @ @ a proxy server for its internet connection If you are working inside a corporate network- you might need to use a proxy server in order to- successfully connect to the internet . Proxy servers have several advantages , as the resolution of the web address takes place at <p> How to : Sync my Google Contacts with my iPhone or any other iOS device One of the weirdest issues I-ve faced as of late is that my Google Contacts were not being brought in correctly to my iPhone . What was really going on behind the scenes was that all the contacts instead of being imported <p> Deploying Microsoft Lync 2010 mobility services for iPad and iPhone After much struggle to get my iPad to work with Lync server ( I tried everything from certificates to configuring the edge server , etc . ) I found out Microsoft let 's those mobile devices connect via web services but you need to install them . The installation surprisingly seems <p> Sometimes you want to clear your history due to a number of reasons , like when you type a password and now @ @ @ @ @ @ @ @ @ @ read this information is stored in your profile as a History file , and many people go as far as suggesting you should delete it . However , <p> Lync 2010 : Ca n't verify the certificate from the server when logging in from the iPad / iPhone When trying to log in from an iPad or iPhone you may run across the error message : " Ca n't verify the certificate from the server . Please contact your support team . " Something I 've noticed through experience working with anything from <p> Sometimes you need to run certain commands on the terminal with elevated credentials otherwise you wont be able to make modifications to the operating system , etc . In order to run such a command you need to use sudo to elevate your current session . For example , you can run " sudo -s " and that will ask for 
@@45151423 @5151423/ <h> Category Archive : WordPress <p> Resolved : Your server may not be able to connect to sites running on it . Error message : SSL certificate problem : self signed certificate So recently I ran across an error while performing a WordPress Update . Usually after you perform an update , you are taken to a screen to individually update all your subsites ( I am guessing <p> Resolved : hphp Warning : Parameter 1 to W3PluginTotalCache : : obcallback() expected to be a reference , value given in **26;3509;TOOLONG on line 3282 As you probably know ( otherwise you would probably not be here ) , after deciding to move form PHP to HHVM , if you are running a WordPress site and use W3 Total Cache you will encounter an endless <p> How to : Increase the memory being allocated to PHP As part of a series of posts regarding how to improve performance when using WordPress , this particular one will focus on the key aspect of memory available to PHP . As most readers would agree , available memory is an important aspect of @ @ @ @ @ @ @ @ @ @ Editorial Assistant issues with https/SSL One of the main reasons why I had left my admin panel open to server non-ssl requests was that Zemanta Editorial Assistant would not work properly behind HTTPS . I did some looking around and found that the same scripts were being sent over to the browser so I <p> How to : Have WordPress communicate with MySQL via Socket As I keep looking into how to improve the performance of website one of the recurrent points mentioned is to use Linux sockets where possible . I really do n't  have much experience and I can see how avoiding the TCP stack might help but I figured at <p> How to : Move the header widget down in Graphene to place your Ads closer to the post If you 've visited our site before you might remember how our Google Add for the header was all the way up overlapping the good old banner . If not then you are like most users ; Advertising that is not <p> Quick Fix : WordPress SEO by Yoast sitemapindex.xml giving blank page After the latest somewhat recent update @ @ @ @ @ @ @ @ @ @ my sitemap . If I do a view source on the blank page that shows up now when I open my sitemap I can see the XML and it is <p> Does JetPacks Photon damage Image SEO ? Thus far I have n't been able to find concrete proof of this but it probably does have an impact . Why is that ? Because the URL being served is from a different site than yours which results in Google or other search engines associating that picture to them not you . <p> How to : Clean your WordPress database from all the empty Yoast WordPress SEO meta data So as I was working in moving a site from one installation to another using the WordPress export/import utility I noticed something : There was so much data going around that the server was erroring out . I opened the- 30mb- XML export 
@@45151426 @5151426/ 55329 @qwx465329 <h> Category Archive : Microsoft <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker I am going to have to start with a confession , " Resolved " might be a bit of a stretch . For starters , the best case scenario looks like recovering the information @ @ @ @ @ @ @ @ @ @ , <p> How to : " Change product key " in Windows 8 or in Windows Server 2012 I 've learned over the years to not activate a Windows product until I am confident it is stable . I say that because I used to activate Windows right after installation and either because of third party update , malfunctioning hardware , etc . I had <p> How to : Enable Shadow Copy or Previous Version in Windows 2012 R2 ? I recently had a bit of an issue with a program . It uses an access database and one of the employees modified it but we needed to revert the changes . Seemed simple enough , just reach out to pick a previous version from the <p> How to : Place a program-s shortcut for all users desktop or Start Menu All users desktop : C : UsersPublicDesktop All users start menu : C : **32;3537;TOOLONG Menu However , the folder is hidden by defaultdoes n't show the public desktop folder by default .. you must show the hidden files and folders . <p> How to : Create Web query files @ @ @ @ @ @ @ @ @ @ to query data from a specific World Wide Web , Internet , or intranet site and retrieve the information directly into a Microsoft Excel worksheet . Microsoft Excel includes some sample Web queries . Definition of a Web Query File A Web query 
@@45151427 @5151427/ <h> How to : Synchronize content between SharePoint 2013 and Outlook <p> One of the great features of SharePoint- 2013 is its ability to synchronize a variety of information with Outlook . You can connect a SharePoint- calendar , library , contact list , or other type of list with Outlook . I think by far the calendar and contact list are the most used ones and we have taken this approach for all our shared contacts and calendars . The big advantage here is the web interface anyone can access- and the portal features of SP . In order to add any SharePoint- list on to your Outlook follow these simple steps : ( all the information below was obtained from the Microsoft site , if you are having issues having the Connect to Outlook button not be grayed out consult the related article on how to resolve this . ) <p> Go to the calendar , library , or list that you want to connect to Outlook . <p> Click Calendar , Library , or List . ( The name of the tab varies depending on which @ @ @ @ @ @ @ @ @ @ . ) <p> Click Connect to Outlook. - - Note- - - In a tasks list , the name of the command appears as Sync to Outlook , and does additional- synchronization of your tasks between SharePoint and Outlook . <p> In the dialog boxes that appear click Allow or Yes- as appropriate . If your site is a part of an Office365- subscription , you may be required to use your subscription user name and password . <p> When you 're finished , your calendar will appear in Outlook under Other Calendars . Contacts will appear under Other Contacts . Libraries , such as document libraries or picture libraries , appear under SharePoint Lists . The files in the library behave similar to e-mail messages . For Office files , click to preview the file in the Reading Pane or double-click the file to open it . Right-click the file for more options . <h> Working with a synchronized document library <p> You can synchronize an entire SharePoint- library with Outlook , or particular folders of documents within the library . If the library has subfolders- and you want @ @ @ @ @ @ @ @ @ @ , go to the library at its top level- and then connect it to Outlook . If you are in a subfolder- of a library , and then connect the library to Outlook , only the contents of that subfolder- are connected . <p> To connect only the contents of a single folder in Outlook , there are two options . If the folder is not open , click the folder to open it , and then click Connect to Outlook . If the folder that you want to connect is already open , follow the steps to connect a library . <p> After you synchronize a library , the library appears as a folder in the Outlook Navigation Pane under SharePoint Lists . You can invite other team members to connect to the library . Right-click the library name in Outlook , and then click Share This Folder . <p> When you use Instant Search , Outlook searches all the synchronized SharePoint- documents . Document previews in the Outlook Reading Pane enable you to easily browse through search results . You may find opening and saving changes to SharePoint @ @ @ @ @ @ @ @ @ @ files from the library . You can then preview , edit , and manage documents in the library from within Outlook . You can also take the libraries offline , work with the Office files in the library , bring them back online , and then synchronize the changes . <p> This synchronization is one-way or unidirectional for Office files . Changes made to files in the SharePoint- library can be downloaded to Outlook , but you can not make changes to non-Office files in Outlook and upload them back to the library . If you work with Outlook offline , you can edit Office files , such as Word documents , Excel spreadsheets , and PowerPoint presentations . When you come back online , you can upload your changes to the SharePoint- library . However , Outlook does not track changes made to these Office files from the SharePoint library . For best results , check out these files first , if you intend to make offline changes to them in Outlook . <h> Working with a synchronized contact list <p> You can synchronize a SharePoint- contact list between @ @ @ @ @ @ @ @ @ @ share , and manage SharePoint- contacts more efficiently in Outlook . You can also take the contact list offline , work with the contacts , bring them back online , and then synchronize them either from Outlook or SharePoint . <p> From Outlook , the synchronized SharePoint contact list works just like other Outlook contact folders . You can view , edit , print , and even call these contacts by using Microsoft Lync . You can send them e-mail messages and meeting requests , use color categories , store multiple phone numbers and e-mail addresses , and include contact photos , Electronic Business Cards , as well as birthday and anniversary information . <p> This contact list synchronization is two-way or bi-directional . Changes made in Outlook to these contacts are automatically synchronized with the SharePoint- contact list . Changes made to the SharePoint contact list are automatically synchronized with the contacts in Outlook . <p> When you open a SharePoint- contact in Outlook or open the contact list in SharePoint , you see the latest changes. - If changes are made- to the contact in Outlook while the @ @ @ @ @ @ @ @ @ @ the browser to see the latest changes in the SharePoint- list . If changes are made- to the SharePoint- contact list while the contact is opened in Outlook , you may need to use the Send/Receive command in Outlook to see the latest changes to the Outlook contact . <h> Working with a synchronized task list <p> You can work with your tasks from your SharePoint- site- similar to the way you work with your Outlook tasks . You can drag or copy tasks back and forth between the folders for Outlook and the SharePoint- site . However , recurring tasks and task requests from Outlook are converted- to regular tasks on the SharePoint site . You can also take a task list offline , work with the tasks in the list , bring them back online , and then synchronize them from Outlook . <p> - Tip- - - After you connect a task list to Outlook , you can send a sharing message to other team members that invite them to connect to the tasks . Right-click the list name in Outlook , and then click Share tasks @ @ @ @ @ @ @ @ @ @ a link and a command to connect to the list . <p> The contents of the synchronized task list appear under Other Tasks- in the Navigation Pane and are consolidated- in the To-Do Bar . To track the status , progress , and history of your projects , you and team members can create , assign , update , respond to , and delete- tasks when working in your respective Outlook folders . <p> You can also apply flags and categories to your synchronized tasks as you do with your Outlook tasks . The flags and categories do not appear- on the SharePoint- site , but the flags and categories are available when you work with your synchronized tasks in Outlook . SharePoint task notifications are also fully integrated with tasks and calendars , and you can choose settings to automatically generate notification e-mail messages and reminders . <p> This task list synchronization is two-way or bi-directional . Changes made in Outlook to these tasks are automatically synchronized with the SharePoint- task list . Changes made to the SharePoint- task list are automatically synchronized with the tasks in Outlook . @ @ @ @ @ @ @ @ @ @ the task list in SharePoint , you see the latest changes . <p> If changes are made- to the task in Outlook while the SharePoint- task list is opened , you need to refresh the browser to see the latest changes in the list . If changes are made- to the SharePoint- task list while the task is opened in Outlook , you may need to use the Send/Receive command to see the latest changes in the Outlook task . 
@@45151429 @5151429/ <h> How to : Move the header widget down in Graphene to place your Ads closer to the post <h> How to : Move the header widget down in Graphene to place your Ads closer to the post <p> If you 've visited our site before you might remember how our Google Add for the header was all the way up overlapping the good old banner . If not then you are like most users ; Advertising that is not close to the content gets lost , specially if it is above the header . This is why Google has recommended to : Move your ad unit below the header . <h> What 's the issue with ads at the very top of the page ? <p> Ads placed above the header are often hidden , and accordingly have lower CTRs than ads further down the page . Move ads at the top of your page down and you could earn more . <p> So keeping that in mind and to please Google ( who after all just cares the site getting more clicks so they make more money ) @ @ @ @ @ @ @ @ @ @ The problem is that Graphenes point of view is that the header widget should be place at the header obviously . But if you are trying to place advertising on the top of your post via widgets this presents a challenge . Regardless of how you look at it , editing the code behind is necessary ( and if it is not please correct me ! ) The options include : <p> Create a new Widget Area for your advertising <p> Find a plugin that could insert that advertising in for you <p> Move the header widget to a more friendly area . <p> On this post we will focus on option number 3 : Moving the existing widget area to another location that should work better than the current one . For this you need to edit the header.php file of the theme . You could do this via a number of ways : Your WordPress admin site , FTP into your web host , or if you have a VPN just log in and edit the file . The key here is to move the Header Widget Area @ @ @ @ @ @ @ @ @ @ this section of the code , you are going to cut and paste it so go ahead and cut the text . Now you need to find the area were you need to paste this code . Below are the two lines you need to find were you will paste this code inbetween them as shown here : <p> Once you have done this , you will notice that your widget area no longer resides at the header but now below it somewhere between your navigation and your posts . This makes it so that your ad does n't  get lost as easily . Also , if you were as lazy as I was your ad was overlapping your banner making it look hideous and out of place much better now ! <p> NOTE : <p> Every time there is a new update to the theme you are going to have to make this change . With that regards the simplest solution might just be to find a plugin to insert that advertising unit above the post . I have n't found a great one so I am sticking to making this change every update . 
@@45151430 @5151430/ <h> Author 's posts listings <p> Sometimes you need to run certain commands on the terminal with elevated credentials otherwise you wont be able to make modifications to the operating system , etc . In order to run such a command you need to use sudo to elevate your current session . For example , you can run " sudo -s " and that will ask for <p> How to : Clear or Flush DNS cache on a Mac OS X computer Most operating systems- implement a local DNS cache for a number of reasons : lower the load on DNS servers , faster domain name resolution , etc . This is great for a number of reasons , but it also poses a problem from time to time . Most <p> After writing code using entity framework for sometime and seeing the code base continue to grow I realized the importance of coding with performance in mind . There are things that come to mind like eager loading , but that alone wont do much as your application continues to grow . Below are some key best practices @ @ @ @ @ @ @ @ @ @ a string stored in the database into an enumeration in our application . I was n't really excited about having to write a switch statement nor having to maintain that code . Fortunately there is a command in . Net that allows you to parse that string into any enumeration which <p> How to : Take a screenshot on a Mac ( either to the clipboard or to a file on the desktop ) We all have from time to time the need to share a screenshot of something on the screen . If it is not to show someone else something on our screen it could be to capture information <p> Apparently after some research there is no way to directly print your screen on a Mac . What I recommend is simply taking a screenshot and saving it to the desktop and then printing that document . You can learn how to take a screenshot on this article . <p> The Jetpack server was unable to communicate with your site : Unable to successfully activate Jetpack on a WordPress installation . Im having issues activating Jetpack on my WordPress @ @ @ @ @ @ @ @ @ @ website needs to be publicly accessible to use Jetpack : - siteinaccessible Error Details : The Jetpack server was unable to communicate with your <p> I 've recently been working on a data migration and I constantly been needing to delete all the tables in the db and start over . After doing some research I came across this hidden stored procedure in SQL Server that allows you to delete each table in the database you are connected to . Use this extremely 
@@45151432 @5151432/ <h> Category Archive : Internet Information Services ( IIS ) <p> How to : Improve the performance / speed of WordPress running in IIS To be entirely honest I am still struggling with this topic . However I have identified a few points that should help improve the performance / speed of a WordPress site running in IIS . Now , if a distinction should be made it would be <p> How to : Configure Central Certificate Store ( CCS ) with IIS 8 ( Windows Server 2012 ) There are a few things you might run across as you try to develop a web farm that has to share SSL Certificates , one of them is the Central Certificate Store . What is CCS ? Central Certificate Store or Centralized SSL Certificate Support <p> How to : Resolve error " The Module DLL **35;3571;TOOLONG failed to load . - The data is the error . " This error might be caused for a number of reasons , but generally speaking the resolution is pretty simple . LogCust.Dll is a library used to log custom errors . There are @ @ @ @ @ @ @ @ @ @ depend on <p> How to : Resolve error " Faulting application name : w3wp.exe , Faulting module name : ntdll.dll , Unable to create log file " To save people some time the error " Faulting application name : w3wp.exe " refers to an issue with IIS , while the error " Faulting module name : ntdll.dll " is incredibly generic it could be so many things , so you really need to <p> How to : Get full metrics including Bandwidth and/or Referral Information on SmarterStats By default IIS 8.5 does not provide on its log file enough information for SmarterStats to calculate bandwidth utilization and Referral information . However , enabling the logs to include this information is fairly simple , the hardest thing is knowing why you were not getting 
@@45151435 @5151435/ 55329 @qwx465329 <h> Category Archive : Outlook.com <p> Resolved : Outlook.com/People contacts not syncing to iPhone Although I have been recommending people to migrate to Outlook.com as an alternative to Gmail ( You can read more about that here : How to : Migrate from Gmail to Outlook.com ( previously Hotmail.com &amp; Live.com ) . Basically having ActiveSync and Customs Domains ( until the end of July 2014 ) as the big <p> How to : Set up multiple Administrator accounts in Microsofts Live Domains Sometimes you want to distribute the administrative tasks- of maintaining a system across a number of individuals to- provide- business continuity in- case you are not available . Fortunately if you are using Live Domains you can have multiple administrators to manage your domain ( create accounts , etc . ) Obviously each <p> How to : Add an email Alias to your Outlook.com account There are several different kinds of aliases that you can create for an Outlook.com account . The easiest way by far is to use what I call for a lack of a better name an " on demand " @ @ @ @ @ @ @ @ @ @ <p> How to : Use your own Domain name with Outlook.com as the Backend Recently I decided to move from Gmail to Outlook.com as Google took away GoogleSync for free accounts ( you can read about that here : - How to move from Gmail to Outlook.com ) . But I 've been using my own domain in Google Apps for a while now <p> How to : Clean up your Gmail Inbox by deleting large attachments ( Sort Gmail by Date/Size ) So , apparently it is possible to fill out your Google Quota on Email . I am constantly reminded by my Google Drive application that Google requires more storage space and that it is at its capacity while Gmail shows a message <p> How to : Migrate from Gmail to Outlook.com ( previously Hotmail.com &amp; Live.com ) At the end of last year Microsoft released a tool aimed at making the transition from Gmail to Outlook.com as easy as possible ( and I must say it was pretty easy ) . I never thought they day would come I would recommend people using Microsoft 
@@45151436 @5151436/ <p> The Joule , Dallas 1530 Main Street , - Dallas , - TX- 75201 Phone : - ( 214 ) 748-1300 Site : - http : **25;3608;TOOLONG My Rating : 4 1/2 stars Category : 4 Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to <p> Cane Rosso Category : - Pizza 2612 Commerce St Dallas , - TX- 75226 Neighborhood : Deep Ellum ( 214 ) 741-1188 www.ilcanerosso.com Menu Yelp : - http : **36;3635;TOOLONG My Rating : 4 1/2 stars Summary : One of the best pizza place Ive been in . Great variety and different from the ordinary , great quality of ingredients and superior taste . It is not expensive or cheap but their rating makes it 
@@45151438 @5151438/ <p> Usages with just one SRC arg and no DEST arg will list the source files instead of copying . <h> DESCRIPTION <p> Rsync is a fast and extraordinarily versatile file copying tool . It can copy locally , to/from another host over any remote shell , or to/from a remote rsync daemon . It offers a large number of options that control every aspect of its behavior and permit very flexible specification of the set of files to be copied . It is famous for its delta-transfer algorithm , which reduces the amount of data sent over the network by sending only the differences between the source files and the existing files in the destination . Rsync is widely used for backups and mirroring and as an improved copy command for everyday use . <p> Rsync finds files that need to be transferred using a " quick check " algorithm ( by default ) that looks for files that have changed in size or in last-modified time . Any changes in the other preserved attributes ( as requested by options ) are made on the destination file directly @ @ @ @ @ @ @ @ @ @ not need to be updated . <p> Some of the additional features of rsync are : <p> support for copying links , devices , owners , groups , and permissions <p> exclude and exclude-from options similar to GNU tar <p> a CVS exclude mode for ignoring the same files that CVS would ignore <p> can use any transparent remote shell , including ssh or rsh <p> does not require super-user privileges <p> pipelining of file transfers to minimize latency costs <p> support for anonymous or authenticated rsync daemons ( ideal for mirroring ) <h> GENERAL <p> Rsync copies files either to or from a remote host , or locally on the current host ( it does not support copying files between two remote hosts ) . <p> There are two different ways for rsync to contact a remote system : using a remote-shell program as the transport ( such as ssh or rsh ) or contacting an rsync daemon directly via TCP . The remote-shell transport is used whenever the source or destination path contains a single colon ( : ) separator after a host specification . Contacting an rsync @ @ @ @ @ @ @ @ @ @ a double colon ( : : ) separator after a host specification , OR when an rsync : // URL is specified ( see also the " USING RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION " section for an exception to this latter rule ) . <p> As a special case , if a single source arg is specified without a destination , the files are listed in an output format similar to " ls -l " . <p> As expected , if neither the source or destination path specify a remote host , the copy occurs locally ( see also the- list-only- option ) . <p> Rsync refers to the local side as the " client " and the remote side as the " server " . Do n't  confuse " server " with an rsync daemon a daemon is always a server , but a server can be either a daemon or a remote-shell spawned process . <h> SETUP <p> See the file README for installation instructions . <p> Once installed , you can use rsync to any machine that you can access via a remote shell ( as @ @ @ @ @ @ @ @ @ @ daemon-mode protocol ) . For remote transfers , a modern rsync uses ssh for its communications , but it may have been configured to use a different remote shell by default , such as rsh or remsh . <p> You can also specify any remote shell you like , either by using the- -e- command line option , or by setting the RSYNCRSH environment variable . <p> Note that rsync must be installed on both the source and destination machines . <h> USAGE <p> You use rsync in the same way you use rcp . You must specify a source and a destination , one of which may be remote . <p> Perhaps the best way to explain the syntax is with some examples : <p> rsync -t *. c foo:src/ <p> This would transfer all files matching the pattern *. c from the current directory to the directory src on the machine foo . If any of the files already exist on the remote system then the rsync remote-update protocol is used to update the file by sending only the differences . See the tech report for details . @ @ @ @ @ @ @ @ @ @ all files from the directory src/bar on the machine foo into the /data/tmp/bar directory on the local machine . The files are transferred in " archive " mode , which ensures that symbolic links , devices , attributes , permissions , ownerships , etc. are preserved in the transfer . Additionally , compression will be used to reduce the size of data portions of the transfer . <p> rsync -avz foo:src/bar/ /data/tmp <p> A trailing slash on the source changes this behavior to avoid creating an additional directory level at the destination . You can think of a trailing / on a source as meaning " copy the contents of this directory " as opposed to " copy the directory by name " , but in both cases the attributes of the containing directory are transferred to the containing directory on the destination . In other words , each of the following commands copies the files in the same way , including their setting of the attributes of /dest/foo : <p> rsync -av /src/foo /destrsync -av /src/foo/ /dest/foo <p> Note also that host and module references do n't  require a @ @ @ @ @ @ @ @ @ @ . For example , both of these copy the remote directorys contents into " /dest " : <p> rsync -av host : /destrsync -av host : : module /dest <p> You can also use rsync in local-only mode , where both the source and destination do n't  have a : in the name . In this case it behaves like an improved copy command . <p> Finally , you can list all the ( listable ) modules available from a particular rsync daemon by leaving off the module name : <p> rsync somehost.mydomain.com : : <p> See the following section for more details . <h> ADVANCED USAGE <p> The syntax for requesting multiple files from a remote host is done by specifying additional remote-host args in the same style as the first , or with the hostname omitted . For instance , all these work : <p> This word-splitting still works ( by default ) in the latest rsync , but is not as easy to use as the first method . <p> If you need to transfer a filename that contains whitespace , you can either specify the- protect-args- @ @ @ @ @ @ @ @ @ @ the whitespace in a way that the remote shell will understand . For instance : <p> rsync -av host : ' file name with spaces ' /dest <h> CONNECTING TO AN RSYNC DAEMON <p> It is also possible to use rsync without a remote shell as the transport . In this case you will directly connect to a remote rsync daemon , typically using TCP port 873 . ( This obviously requires the daemon to be running on the remote system , so refer to the STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS section below for information on that . ) <p> Using rsync in this way is the same as using it with a remote shell except that : <p> you either use a double colon : : instead of a single colon to separate the hostname from the path , or you use an rsync : // URL . <p> the first word of the " path " is actually a module name . <p> the remote daemon may print a message of the day when you connect . <p> if you specify no path name on the @ @ @ @ @ @ @ @ @ @ daemon will be shown . <p> if you specify no local destination then a listing of the specified files on the remote daemon is provided . <p> you must not specify the- rsh- ( -e ) option . <p> An example that copies all the files in a remote module named " src " : <p> rsync -av host : : src /dest <p> Some modules on the remote daemon may require authentication . If so , you will receive a password prompt when you connect . You can avoid the password prompt by setting the environment variable RSYNCPASSWORD to the password you want to use or using the- password-file- option . This may be useful when scripting rsync . <p> WARNING : On some systems environment variables are visible to all users . On those systems using- password-file- is recommended . <p> You may establish the connection via a web proxy by setting the environment variable RSYNCPROXY to a hostname:port pair pointing to your web proxy . Note that your web proxys configuration must support proxy connections to port 873 . <p> You may also establish a daemon connection @ @ @ @ @ @ @ @ @ @ variable RSYNCCONNECTPROG to the commands you wish to run in place of making a direct socket connection . The string may contain the escape " %H " to represent the hostname specified in the rsync command ( so use " %% " if you need a single " % " in your string ) . For example : <p> The command specified above uses ssh to run nc ( netcat ) on a proxyhost , which forwards all data to port 873 ( the rsync daemon ) on the targethost ( %H ) . <h> USING RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION <p> It is sometimes useful to use various features of an rsync daemon ( such as named modules ) without actually allowing any new socket connections into a system ( other than what is already required to allow remote-shell access ) . Rsync supports connecting to a host using a remote shell and then spawning a single-use " daemon " server that expects to read its config file in the home dir of the remote user . This can be useful if you want to encrypt a daemon-style @ @ @ @ @ @ @ @ @ @ fresh by the remote user , you may not be able to use features such as chroot or change the uid used by the daemon . ( For another way to encrypt a daemon transfer , consider using ssh to tunnel a local port to a remote machine and configure a normal rsync daemon on that remote host to only allow connections from " localhost " . ) <p> From the users perspective , a daemon transfer via a remote-shell connection uses nearly the same command-line syntax as a normal rsync-daemon transfer , with the only exception being that you must explicitly set the remote shell program on the command-line with the- rsh=COMMAND- option . ( Setting the RSYNCRSH in the environment will not turn on this functionality . ) For example : <p> rsync -av --rsh=ssh host : : module /dest <p> If you need to specify a different remote-shell user , keep in mind that the user@ prefix in front of the host is specifying the rsync-user value ( for a module that requires user-based authentication ) . This means that you must give the -l user option @ @ @ @ @ @ @ @ @ @ example that uses the short version of the- rsh- option : <p> The " ssh-user " will be used at the ssh level ; the " rsync-user " will be used to log-in to the " module " . <h> STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS <p> In order to connect to an rsync daemon , the remote system needs to have a daemon already running ( or it needs to have configured something like inetd to spawn an rsync daemon for incoming connections on a particular port ) . For full information on how to start a daemon that will handling incoming socket connections , see the- rsyncd.conf(5) man page that is the config file for the daemon , and it contains the full details for how to run the daemon ( including stand-alone and inetd configurations ) . <p> If you 're using one of the remote-shell transports for the transfer , there is no need to manually start an rsync daemon . <h> SORTED TRANSFER ORDER <p> Rsync always sorts the specified filenames into its internal transfer list . This handles the merging together of the contents @ @ @ @ @ @ @ @ @ @ duplicate filenames , and may confuse someone when the files are transferred in a different order than what was given on the command-line . <p> If you need a particular file to be transferred prior to another , either separate the files into different rsync calls , or consider using- delay-updates- ( which does n't  affect the sorted transfer order , but does make the final file-updating phase happen much more rapidly ) . <h> EXAMPLES <p> Here are some examples of how I use rsync . <p> To backup my wifes home directory , which consists of large MS Word files and mail folders , I use a cron job that runs <p> rsync -Cavz . arvidsjaur:backup <p> each night over a PPP connection to a duplicate directory on my machine " arvidsjaur " . <p> To synchronize my samba source trees I use the following Makefile targets : <p> this allows me to sync with a CVS directory at the other end of the connection . I then do CVS operations on the remote machine , which saves a lot of time as the remote CVS protocol is n't @ @ @ @ @ @ @ @ @ @ " old " and " new " ftp sites with the command : <p> rsync -az -e ssh --delete ftp/pub/samba nimbus : " ftp/pub/tridge " <p> This is launched from cron every few hours . <h> OPTIONS SUMMARY <p> Here is a short summary of the options available in rsync . Please refer to the detailed description below for a complete description . <h> OPTIONS <p> Rsync accepts both long ( double-dash + word ) and short ( single-dash + letter ) options . The full list of the available options are described below . If an option can be specified in more than one way , the choices are comma-separated . Some options only have a long variant , not a short . If the option takes a parameter , the parameter is only listed after the long variant , even though it must also be specified for the short . When specifying a parameter , you can either use the form option=param or replace the = with whitespace . The parameter may need to be quoted in some manner for it to survive the shells command-line parsing . @ @ @ @ @ @ @ @ @ @ a filename is substituted by your shell , so option=/foo will not change the tilde into your home directory ( remove the = for that ) . <p> help <p> Print a short help page describing the options available in rsync and exit . For backward-compatibility with older versions of rsync , the help will also be output if you use the- -h- option without any other args . <p> version <p> print the rsync version number and exit . <p> -v , verbose <p> This option increases the amount of information you are given during the transfer . By default , rsync works silently . A single- -v- will give you information about what files are being transferred and a brief summary at the end . Two- -v- options will give you information on what files are being skipped and slightly more information at the end . More than two- -v- options should only be used if you are debugging rsync.In a modern rsync , the- -v- option is equivalent to the setting of groups of- info- and- debug- options . You can choose to use these newer options @ @ @ @ @ @ @ @ @ @ , as any fine-grained settings override the implied settings of- -v . Both- info- and- debug- have a way to ask for help that tells you exactly what flags are set for each increase in verbosity . <p> info=FLAGS <p> This option let 's you have fine-grained control over the information output you want to see . An individual flag name may be followed by a level number , with 0 meaning to silence that output , 1 being the default output level , and higher numbers increasing the output of that flag ( for those that support higher levels ) . Use- info=help- to see all the available flag names , what they output , and what flag names are added for each increase in the verbose level . Some examples : <p> Note that- info=names output is affected by the- out-format- and- itemize-changes- ( -i ) options . See those options for more information on what is output and when . <p> This option was added to 3.1.0 , so an older rsync on the server side might reject your attempts at fine-grained control ( if one or more @ @ @ @ @ @ @ @ @ @ server was too old to understand them ) . <p> debug=FLAGS <p> This option let 's you have fine-grained control over the debug output you want to see . An individual flag name may be followed by a level number , with 0 meaning to silence that output , 1 being the default output level , and higher numbers increasing the output of that flag ( for those that support higher levels ) . Use- debug=help- to see all the available flag names , what they output , and what flag names are added for each increase in the verbose level . Some examples : <p> Note that some debug messages will only be output when- msgs2stderr- is specified , especially those pertaining to I/O and buffer debugging . <p> This option was added to 3.1.0 , so an older rsync on the server side might reject your attempts at fine-grained control ( if one or more flags needed to be send to the server and the server was too old to understand them ) . <p> msgs2stderr <p> This option changes rsync to send all its output directly to stderr @ @ @ @ @ @ @ @ @ @ the protocol ( which normally outputs info messages via stdout ) . This is mainly intended for debugging in order to avoid changing the data sent via the protocol , since the extra protocol data can change what is being tested . Keep in mind that a daemon connection does not have a stderr channel to send messages back to the client side , so if you are doing any daemon-transfer debugging using this option , you should start up a daemon using- no-detach- so that you can see the stderr output on the daemon side.This option has the side-effect of making stderr output get line-buffered so that the merging of the output of 3 programs happens in a more readable manner . <p> -q , quiet <p> This option decreases the amount of information you are given during the transfer , notably suppressing information messages from the remote server . This option is useful when invoking rsync from cron . <p> no-motd <p> This option affects the information that is output by the client at the start of a daemon transfer . This suppresses the message-of-the-day ( MOTD ) @ @ @ @ @ @ @ @ @ @ that the daemon sends in response to the " rsync host : : " request ( due to a limitation in the rsync protocol ) , so omit this option if you want to request the list of modules from the daemon . <p> -I , ignore-times <p> Normally rsync will skip any files that are already the same size and have the same modification timestamp . This option turns off this " quick check " behavior , causing all files to be updated . <p> size-only <p> This modifies rsyncs " quick check " algorithm for finding files that need to be transferred , changing it from the default of transferring files with either a changed size or a changed last-modified time to just looking for files that have changed in size . This is useful when starting to use rsync after using another mirroring system which may not preserve timestamps exactly . <p> modify-window <p> When comparing two timestamps , rsync treats the timestamps as being equal if they differ by no more than the modify-window value . This is normally 0 ( for an exact match ) @ @ @ @ @ @ @ @ @ @ to a larger value in some situations . In particular , when transferring to or from an MS Windows FAT filesystem ( which represents times with a 2-second resolution ) , - modify-window=1- is useful ( allowing times to differ by up to 1 second ) . <p> -c , checksum <p> This changes the way rsync checks if the files have been changed and are in need of a transfer . Without this option , rsync uses a " quick check " that ( by default ) checks if each files size and time of last modification match between the sender and receiver . This option changes this to compare a 128-bit checksum for each file that has a matching size . Generating the checksums means that both sides will expend a lot of disk I/O reading all the data in the files in the transfer ( and this is prior to any reading that will be done to transfer changed files ) , so this can slow things down significantly.The sending side generates its checksums while it is doing the file-system scan that builds the list of the @ @ @ @ @ @ @ @ @ @ is scanning for changed files , and will checksum any file that has the same size as the corresponding senders file : files with either a changed size or a changed checksum are selected for transfer . <p> Note that rsync always verifies that each- transferred- file was correctly reconstructed on the receiving side by checking a whole-file checksum that is generated as the file is transferred , but that automatic after-the-transfer verification has nothing to do with this options before-the-transfer " Does this file need to be updated ? " check . <p> For protocol 30 and beyond ( first supported in 3.0.0 ) , the checksum used is MD5 . For older protocols , the checksum used is MD4 . <p> -a , archive <p> This is equivalent to- -rlptgoD . It is a quick way of saying you want recursion and want to preserve almost everything ( with -H being a notable omission ) . The only exception to the above equivalence is when- files-from- is specified , in which case- -r- is not implied.Note that- -a- does not preserve hardlinks , because finding multiply-linked files is @ @ @ @ @ @ @ @ @ @ <p> You may turn off one or more implied options by prefixing the option name with " no- " . Not all options may be prefixed with a " no- " : only options that are implied by other options ( e.g.- no-D , - no-perms ) or have different defaults in various circumstances ( e.g.- no-whole-file , - no-blocking-io , - no-dirs ) . You may specify either the short or the long option name after the " no- " prefix ( e.g.- no-R- is the same as- no-relative ) . For example : if you want to use- -a- ( archive ) but do n't  want- -o- ( owner ) , instead of converting- -a- into- -rlptgD , you could specify- -a no-o- ( or- -a no-owner ) . <p> The order of the options is important : if you specify- no-r -a , the- -r- option would end up being turned on , the opposite of- -a no-r . Note also that the side-effects of the- files-from- option are NOT positional , as it affects the default state of several options and slightly changes the meaning of- @ @ @ @ @ @ @ @ @ @ . <p> -r , recursive <p> This tells rsync to copy directories recursively . See also- dirs- ( -d ) . Beginning with rsync 3.0.0 , the recursive algorithm used is now an incremental scan that uses much less memory than before and begins the transfer after the scanning of the first few directories have been completed . This incremental scan only affects our recursion algorithm , and does not change a non-recursive transfer . It is also only possible when both ends of the transfer are at least version 3.0.0 . <p> Some options require rsync to know the full file list , so these options disable the incremental recursion mode . These include : - delete-before , - delete-after , - prune-empty-dirs , and- delay-updates . Because of this , the default delete mode when you specify- delete- is now- delete-during- when both ends of the connection are at least 3.0.0 ( use- del- or- delete-during- to request this improved deletion mode explicitly ) . See also the- delete-delay- option that is a better choice than using- delete-after . <p> Incremental recursion can be disabled using the- no-inc-recursive- @ @ @ @ @ @ @ @ @ @ relative <p> Use relative paths . This means that the full path names specified on the command line are sent to the server rather than just the last parts of the filenames . This is particularly useful when you want to send several different directories at the same time . For example , if you used this command : <p> rsync -av /foo/bar/baz.c remote : /tmp/ <p> this would create a file named baz.c in /tmp/ on the remote machine . If instead you used <p> rsync -avR /foo/bar/baz.c remote : /tmp/ <p> then a file named /tmp/foo/bar/baz.c would be created on the remote machine , preserving its full path . These extra path elements are called " implied directories " ( i.e. the " foo " and the " foo/bar " directories in the above example ) . <p> Beginning with rsync 3.0.0 , rsync always sends these implied directories as real directories in the file list , even if a path element is really a symlink on the sending side . This prevents some really unexpected behaviors when copying the full path of a file that you did @ @ @ @ @ @ @ @ @ @ you want to duplicate a server-side symlink , include both the symlink via its path , and referent directory via its real path . If you 're dealing with an older rsync on the sending side , you may need to use the- no-implied-dirs- option . <p> It is also possible to limit the amount of path information that is sent as implied directories for each path you specify . With a modern rsync on the sending side ( beginning with 2.6.7 ) , you can insert a dot and a slash into the source path , like this : <p> rsync -avR /foo/. /bar/baz.c remote : /tmp/ <p> That would create /tmp/bar/baz.c on the remote machine . ( Note that the dot must be followed by a slash , so " /foo/. " would not be abbreviated . ) For older rsync versions , you would need to use a chdir to limit the source path . For example , when pushing files : <p> ( cd /foo ; rsync -avR bar/baz.c remote : /tmp/ ) <p> ( Note that the parens put the two commands into a sub-shell , @ @ @ @ @ @ @ @ @ @ in effect for future commands . ) If you 're pulling files from an older rsync , use this idiom ( but only for a non-daemon transfer ) : <p> rsync -avR --rsync-path= " cd /foo ; rsync " - remote:bar/baz.c /tmp/ <p> no-implied-dirs <p> This option affects the default behavior of the- relative- option . When it is specified , the attributes of the implied directories from the source names are not included in the transfer . This means that the corresponding path elements on the destination system are left unchanged if they exist , and any missing implied directories are created with default attributes . This even allows these implied path elements to have big differences , such as being a symlink to a directory on the receiving side.For instance , if a command-line arg or a files-from entry told rsync to transfer the file " path/foo/file " , the directories " path " and " path/foo " are implied when- relative- is used . If " path/foo " is a symlink to " bar " on the destination system , the receiving rsync would ordinarily delete " path/foo @ @ @ @ @ @ @ @ @ @ the file into the new directory . With- no-implied-dirs , the receiving rsync updates " path/foo/file " using the existing path elements , which means that the file ends up being created in " path/bar " . Another way to accomplish this link preservation is to use the- keep-dirlinks- option ( which will also affect symlinks to directories in the rest of the transfer ) . <p> When pulling files from an rsync older than 3.0.0 , you may need to use this option if the sending side has a symlink in the path you request and you wish the implied directories to be transferred as normal directories . <p> -b , backup <p> With this option , preexisting destination files are renamed as each file is transferred or deleted . You can control where the backup file goes and what ( if any ) suffix gets appended using the- backup-dir- and- suffix- options.Note that if you do n't  specify- backup-dir , ( 1 ) the- omit-dir-times- option will be implied , and ( 2 ) if- delete- is also in effect ( without- delete-excluded ) , rsync will add @ @ @ @ @ @ @ @ @ @ the end of all your existing excludes ( e.g.- -f " P * " ) . This will prevent previously backed-up files from being deleted . Note that if you are supplying your own filter rules , you may need to manually insert your own exclude/protect rule somewhere higher up in the list so that it has a high enough priority to be effective ( e.g. , if your rules specify a trailing inclusion/exclusion of * , the auto-added rule would never be reached ) . <p> backup-dir=DIR <p> In combination with the- backup- option , this tells rsync to store all backups in the specified directory on the receiving side . This can be used for incremental backups . You can additionally specify a backup suffix using the- suffix- option ( otherwise the files backed up in the specified directory will keep their original filenames ) . Note that if you specify a relative path , the backup directory will be relative to the destination directory , so you probably want to specify either an absolute path or a path that starts with " .. / " . If @ @ @ @ @ @ @ @ @ @ can not go outside the modules path hierarchy , so take extra care not to delete it or copy into it . <p> suffix=SUFFIX <p> This option allows you to override the default backup suffix used with the- backup- ( -b ) option . The default suffix is a if no -backup-dir- was specified , otherwise it is an empty string . <p> -u , update <p> This forces rsync to skip any files which exist on the destination and have a modified time that is newer than the source file . ( If an existing destination file has a modification time equal to the source files , it will be updated if the sizes are different . ) Note that this does not affect the copying of symlinks or other special files . Also , a difference of file format between the sender and receiver is always considered to be important enough for an update , no matter what date is on the objects . In other words , if the source has a directory where the destination has a file , the transfer would occur regardless of the @ @ @ @ @ @ @ @ @ @ not an exclude , so it does n't  affect the data that goes into the file-lists , and thus it does n't  affect deletions . It just limits the files that the receiver requests to be transferred . <p> inplace <p> This option changes how rsync transfers a file when its data needs to be updated : instead of the default method of creating a new copy of the file and moving it into place when it is complete , rsync instead writes the updated data directly to the destination file.This has several effects : <p> Hard links are not broken . This means the new data will be visible through other hard links to the destination file . Moreover , attempts to copy differing source files onto a multiply-linked destination file will result in a " tug of war " with the destination data changing back and forth . <p> In-use binaries can not be updated ( either the OS will prevent this from happening , or binaries that attempt to swap-in their data will misbehave or crash ) . <p> The files data will be in an inconsistent @ @ @ @ @ @ @ @ @ @ if the transfer is interrupted or if an update fails . <p> A file that rsync can not write to can not be updated . While a super user can update any file , a normal user needs to be granted write permission for the open of the file for writing to be successful . <p> The efficiency of rsyncs delta-transfer algorithm may be reduced if some data in the destination file is overwritten before it can be copied to a position later in the file . This does not apply if you use- backup , since rsync is smart enough to use the backup file as the basis file for the transfer . <p> WARNING : you should not use this option to update files that are being accessed by others , so be careful when choosing to use this for a copy . <p> This option is useful for transferring large files with block-based changes or appended data , and also on systems that are disk bound , not network bound . It can also help keep a copy-on-write filesystem snapshot from diverging the entire contents of a @ @ @ @ @ @ @ @ @ @ rsync to update a file by appending data onto the end of the file , which presumes that the data that already exists on the receiving side is identical with the start of the file on the sending side . If a file needs to be transferred and its size on the receiver is the same or longer than the size on the sender , the file is skipped . This does not interfere with the updating of a files non-content attributes ( e.g. permissions , ownership , etc. ) when the file does not need to be transferred , nor does it affect the updating of any non-regular files . Implies- inplace , but does not conflict with- sparse- ( since it is always extending a files length ) . <p> append-verify <p> This works just like the- append- option , but the existing data on the receiving side is included in the full-file checksum verification step , which will cause a file to be resent if the final verification step fails ( rsync uses a normal , non-appending- inplace- transfer for the resend ) . Note : prior @ @ @ @ @ @ @ @ @ @ , so if you are interacting with an older rsync ( or the transfer is using a protocol prior to 30 ) , specifying either append option will initiate an- append-verify- transfer . <p> -d , dirs <p> Tell the sending side to include any directories that are encountered . Unlike- recursive , a directorys contents are not copied unless the directory name specified is " . " or ends with a trailing slash ( e.g. " . " , " dir/. " , " dir/ " , etc . ) . Without this option or the- recursive- option , rsync will skip all directories it encounters ( and output a message to that effect for each one ) . If you specify both- dirs- and- recursive , - recursive- takes precedence.The- dirs- option is implied by the- files-from- option or the- list-only- option ( including an implied- list-only- usage ) if- recursive- wasnt specified ( so that directories are seen in the listing ) . Specify- no-dirs- ( or- no-d ) if you want to turn this off . <p> There is also a backward-compatibility helper option , - @ @ @ @ @ @ @ @ @ @ a hack of " -r exclude=/*/* ' " to get an older rsync to list a single directory without recursing . <p> -l , links <p> When symlinks are encountered , recreate the symlink on the destination . <p> -L , copy-links <p> When symlinks are encountered , the item that they point to ( the referent ) is copied , rather than the symlink . In older versions of rsync , this option also had the side-effect of telling the receiving side to follow symlinks , such as symlinks to directories . In a modern rsync such as this one , you 'll need to specify- keep-dirlinks- ( -K ) to get this extra behavior . The only exception is when sending files to an rsync that is too old to understand- -K- in that case , the- -L- option will still have the side-effect of- -K- on that older receiving rsync . <p> copy-unsafe-links <p> This tells rsync to copy the referent of symbolic links that point outside the copied tree . Absolute symlinks are also treated like ordinary files , and so are any symlinks in the source @ @ @ @ @ @ @ @ @ @ no additional effect if- copy-links- was also specified . <p> safe-links <p> This tells rsync to ignore any symbolic links which point outside the copied tree . All absolute symlinks are also ignored . Using this option in conjunction with- relative- may give unexpected results . <p> munge-links <p> This option tells rsync to ( 1 ) modify all symlinks on the receiving side in a way that makes them unusable but recoverable ( see below ) , or ( 2 ) to unmunge symlinks on the sending side that had been stored in a munged state . This is useful if you do n't  quite trust the source of the data to not try to slip in a symlink to a unexpected place.The way rsync disables the use of symlinks is to prefix each one with the string " /rsyncd-munged/ " . This prevents the links from being used as long as that directory does not exist . When this option is enabled , rsync will refuse to run if that path is a directory or a symlink to a directory . <p> The option only affects the client @ @ @ @ @ @ @ @ @ @ to affect the server , specify it via- remote-option . ( Note that in a local transfer , the client side is the sender . ) <p> This option has no affect on a daemon , since the daemon configures whether it wants munged symlinks via its " munge symlinks " parameter . See also the " munge-symlinks " perl script in the support directory of the source code . <p> -k , copy-dirlinks <p> This option causes the sending side to treat a symlink to a directory as though it were a real directory . This is useful if you do n't  want symlinks to non-directories to be affected , as they would be using- copy-links.Without this option , if the sending side has replaced a directory with a symlink to a directory , the receiving side will delete anything that is in the way of the new symlink , including a directory hierarchy ( as long as- force- or- delete- is in effect ) . <p> See also- keep-dirlinks- for an analogous option for the receiving side . <p> copy-dirlinks- applies to all symlinks to directories in the @ @ @ @ @ @ @ @ @ @ specified symlinks , a trick you can use is to pass them as additional source args with a trailing slash , using- relative- to make the paths match up right . For example : <p> rsync -r --relative src/. / src/. /follow-me/ dest/ <p> This works because rsync calls- lstat(2) on the source arg as given , and the trailing slash makes- lstat(2) follow the symlink , giving rise to a directory in the file-list which overrides the symlink found during the scan of " src/. / " . <p> -K , keep-dirlinks <p> This option causes the receiving side to treat a symlink to a directory as though it were a real directory , but only if it matches a real directory from the sender . Without this option , the receivers symlink would be deleted and replaced with a real directory.For example , suppose you transfer a directory " foo " that contains a file " file " , but " foo " is a symlink to directory " bar " on the receiver . Without- keep-dirlinks , the receiver deletes symlink " foo " , recreates it @ @ @ @ @ @ @ @ @ @ new directory . With- keep-dirlinks , the receiver keeps the symlink and " file " ends up in " bar " . <p> One note of caution : if you use- keep-dirlinks , you must trust all the symlinks in the copy ! If it is possible for an untrusted user to create their own symlink to any directory , the user could then ( on a subsequent copy ) replace the symlink with a real directory and affect the content of whatever directory the symlink references . For backup copies , you are better off using something like a bind mount instead of a symlink to modify your receiving hierarchy . <p> See also- copy-dirlinks- for an analogous option for the sending side . <p> -H , hard-links <p> This tells rsync to look for hard-linked files in the source and link together the corresponding files on the destination . Without this option , hard-linked files in the source are treated as though they were separate files.This option does NOT necessarily ensure that the pattern of hard links on the destination exactly matches that on the source . Cases @ @ @ @ @ @ @ @ @ @ links include the following : <p> If the destination contains extraneous hard-links ( more linking than what is present in the source file list ) , the copying algorithm will not break them explicitly . However , if one or more of the paths have content differences , the normal file-update process will break those extra links ( unless you are using the- inplace- option ) . <p> If you specify a- link-dest- directory that contains hard links , the linking of the destination files against the- link-dest- files can cause some paths in the destination to become linked together due to the- link-dest- associations . <p> Note that rsync can only detect hard links between files that are inside the transfer set . If rsync updates a file that has extra hard-link connections to files outside the transfer , that linkage will be broken . If you are tempted to use the- inplace- option to avoid this breakage , be very careful that you know how your files are being updated so that you are certain that no unintended changes happen due to lingering hard links ( and see @ @ @ @ @ @ @ @ @ @ incremental recursion is active ( see- recursive ) , rsync may transfer a missing hard-linked file before it finds that another link for that contents exists elsewhere in the hierarchy . This does not affect the accuracy of the transfer ( i.e. which files are hard-linked together ) , just its efficiency ( i.e. copying the data for a new , early copy of a hard-linked file that could have been found later in the transfer in another member of the hard-linked set of files ) . One way to avoid this inefficiency is to disable incremental recursion using the- no-inc-recursive- option . <p> -p , perms <p> This option causes the receiving rsync to set the destination permissions to be the same as the source permissions . ( See also the- chmod- option for a way to modify what rsync considers to be the source permissions . ) When this option is- off , permissions are set as follows : <p> New files get their " normal " permission bits set to the source files permissions masked with the receiving directorys default permissions ( either the receiving processs umask @ @ @ @ @ @ @ @ @ @ ACL ) , and their special permission bits disabled except in the case where a new directory inherits a setgid bit from its parent directory . <p> Thus , when- perms- and- executability- are both disabled , rsyncs behavior is the same as that of other file-copy utilities , such as- cp(1) and- tar(1) . <p> In summary : to give destination files ( both old and new ) the source permissions , use- perms . To give new files the destination-default permissions ( while leaving existing files unchanged ) , make sure that the- perms- option is off and use- chmod=ugo=rwX- ( which ensures that all non-masked bits get enabled ) . If you 'd care to make this latter behavior easier to type , you could define a popt alias for it , such as putting this line in the file /. popt ( the following defines the- -Z- option , and includes no-g to use the default group of the destination dir ) : <p> rsync alias -Z --no-p --no-g --chmod=ugo=rwX <p> You could then use this new option in a command such as this one : <p> @ @ @ @ @ @ @ @ @ @ that- -a- does not follow- -Z , or it will re-enable the two " no-* " options mentioned above . ) <p> The preservation of the destinations setgid bit on newly-created directories when- perms- is off was added in rsync 2.6.7 . Older rsync versions erroneously preserved the three special permission bits for newly-created files when- perms- was off , while overriding the destinations setgid bit setting on a newly-created directory . Default ACL observance was added to the ACL patch for rsync 2.6.7 , so older ( or non-ACL-enabled ) rsyncs use the umask even if default ACLs are present . ( Keep in mind that it is the version of the receiving rsync that affects these behaviors . ) <p> -E , executability <p> This option causes rsync to preserve the executability ( or non-executability ) of regular files when- perms- is not enabled . A regular file is considered to be executable if at least one x is turned on in its permissions . When an existing destination files executability differs from that of the corresponding source file , rsync modifies the destination files permissions as follows @ @ @ @ @ @ @ @ @ @ off all its x permissions . <p> To make a file executable , rsync turns on each x permission that has a corresponding r permission enabled . <p> If- perms- is enabled , this option is ignored . <p> -A , acls <p> This option causes rsync to update the destination ACLs to be the same as the source ACLs . The option also implies- perms.The source and destination systems must have compatible ACL entries for this option to work properly . See the- fake-super- option for a way to backup and restore ACLs that are not compatible . <p> -X , xattrs <p> This option causes rsync to update the destination extended attributes to be the same as the source ones.For systems that support extended-attribute namespaces , a copy being done by a super-user copies all namespaces except system. * . A normal user only copies the user. * namespace . To be able to backup and restore non-user namespaces as a normal user , see the- fake-super- option . <p> Note that this option does not copy rsyncs special xattr values ( e.g. those used by- fake-super ) @ @ @ @ @ @ @ @ @ @ This " copy all xattrs " mode can not be used with- fake-super . <p> chmod <p> This option tells rsync to apply one or more comma-separated " chmod " modes to the permission of the files in the transfer . The resulting value is treated as though it were the permissions that the sending side supplied for the file , which means that this option can seem to have no effect on existing files if- perms- is not enabled.In addition to the normal parsing rules specified in the- chmod(1) manpage , you can specify an item that should only apply to a directory by prefixing it with a D , or specify an item that should only apply to a file by prefixing it with a F. For example , the following will ensure that all directories get marked set-gid , that no files are other-writable , that both are user-writable and group-writable , and that both have consistent executability across all bits : <p> chmod=Dg+s , ug+w , Fo-w , +X <p> Using octal mode numbers is also allowed : <p> chmod=D2775 , F664 <p> It is also @ @ @ @ @ @ @ @ @ @ option is just appended to the list of changes to make . <p> See the- perms- and- executability- options for how the resulting permission value can be applied to the files in the transfer . <p> -o , owner <p> This option causes rsync to set the owner of the destination file to be the same as the source file , but only if the receiving rsync is being run as the super-user ( see also the- super- and- fake-super- options ) . Without this option , the owner of new and/or transferred files are set to the invoking user on the receiving side.The preservation of ownership will associate matching names by default , but may fall back to using the I 'd number in some circumstances ( see also the- numeric-ids- option for a full discussion ) . <p> -g , group <p> This option causes rsync to set the group of the destination file to be the same as the source file . If the receiving program is not running as the super-user ( or if- no-super- was specified ) , only groups that the invoking user on the @ @ @ @ @ @ @ @ @ @ Without this option , the group is set to the default group of the invoking user on the receiving side.The preservation of group information will associate matching names by default , but may fall back to using the I 'd number in some circumstances ( see also the- numeric-ids- option for a full discussion ) . <p> devices <p> This option causes rsync to transfer character and block device files to the remote system to recreate these devices . This option has no effect if the receiving rsync is not run as the super-user ( see also the- super- and- fake-super- options ) . <p> specials <p> This option causes rsync to transfer special files such as named sockets and fifos . <p> -D <p> The- -D- option is equivalent to- devices- specials . <p> -t , times <p> This tells rsync to transfer modification times along with the files and update them on the remote system . Note that if this option is not used , the optimization that excludes files that have not been modified can not be effective ; in other words , a missing- -t- or- -a- @ @ @ @ @ @ @ @ @ @ used- -I , causing all files to be updated ( though rsyncs delta-transfer algorithm will make the update fairly efficient if the files have n't actually changed , you 're much better off using- -t ) . <p> -O , omit-dir-times <p> This tells rsync to omit directories when it is preserving modification times ( see- times ) . If NFS is sharing the directories on the receiving side , it is a good idea to use- -O . This option is inferred if you use- backup- without- backup-dir . <p> -J , omit-link-times <p> This tells rsync to omit symlinks when it is preserving modification times ( see- times ) . <p> super <p> This tells the receiving side to attempt super-user activities even if the receiving rsync wasnt run by the super-user . These activities include : preserving users via the- owner- option , preserving all groups ( not just the current users groups ) via the- groups- option , and copying devices via the- devices- option . This is useful for systems that allow such activities without being the super-user , and also for ensuring that you will get @ @ @ @ @ @ @ @ @ @ super-user . To turn off super-user activities , the super-user can use- no-super . <p> fake-super <p> When this option is enabled , rsync simulates super-user activities by saving/restoring the privileged attributes via special extended attributes that are attached to each file ( as needed ) . This includes the files owner and group ( if it is not the default ) , the files device info ( device &amp; special files are created as empty text files ) , and any permission bits that we wont allow to be set on the real file ( e.g. the real file gets u-s , g-s , o-t for safety ) or that would limit the owners access ( since the real super-user can always access/change a file , the files we create can always be accessed/changed by the creating user ) . This option also handles ACLs ( if- acls- was specified ) and non-user extended attributes ( if- xattrs- was specified ) . This is a good way to backup data without using a super-user , and to store ACLs from incompatible systems . <p> The- fake-super- option only affects @ @ @ @ @ @ @ @ @ @ the remote side of a remote-shell connection , use the- remote-option- ( -M ) option : <p> rsync -av -M--fake-super /src/ host : /dest/ <p> For a local copy , this option affects both the source and the destination . If you wish a local copy to enable this option just for the destination files , specify- -Mfake-super . If you wish a local copy to enable this option just for the source files , combine- fake-super- with- -Msuper . <p> This option is overridden by both- super- and- no-super . <p> See also the " fake super " setting in the daemons rsyncd.conf file . <p> -S , sparse <p> Try to handle sparse files efficiently so they take up less space on the destination . Conflicts with- inplace- because its not possible to overwrite data in a sparse fashion . <p> preallocate <p> This tells the receiver to allocate each destination file to its eventual size before writing data to the file . Rsync will only use the real filesystem-level preallocation support provided by Linuxs- fallocate(2) system call or Cygwins- posixfallocate(3) , not the slow glibc implementation that @ @ @ @ @ @ @ @ @ @ larger files may not be entirely contiguous on the filesystem , but with this option rsync will probably copy more slowly . If the destination is not an extent-supporting filesystem ( such as ext4 , xfs , NTFS , etc. ) , this option may have no positive effect at all . <p> -n , dry-run <p> This makes rsync perform a trial run that does n't  make any changes ( and produces mostly the same output as a real run ) . It is most commonly used in combination with the- -v , verbose- and/or- -i , itemize-changes- options to see what an rsync command is going to do before one actually runs it.The output of- itemize-changes- is supposed to be exactly the same on a dry run and a subsequent real run ( barring intentional trickery and system call failures ) ; if it is n't , that 's a bug . Other output should be mostly unchanged , but may differ in some areas . Notably , a dry run does not send the actual data for file transfers , so- progress- has no effect , the " bytes @ @ @ @ @ @ @ @ @ @ data " , and " matched data " statistics are too small , and the " speedup " value is equivalent to a run where no file transfers were needed . <p> -W , whole-file <p> With this option rsyncs delta-transfer algorithm is not used and the whole file is sent as-is instead . The transfer may be faster if this option is used when the bandwidth between the source and destination machines is higher than the bandwidth to disk ( especially when the " disk " is actually a networked filesystem ) . This is the default when both the source and destination are specified as local paths , but only if no batch-writing option is in effect . <p> -x , one-file-system <p> This tells rsync to avoid crossing a filesystem boundary when recursing . This does not limit the users ability to specify items to copy from multiple filesystems , just rsyncs recursion through the hierarchy of each directory that the user specified , and also the analogous recursion on the receiving side during deletion . Also keep in mind that rsync treats a " bind " @ @ @ @ @ @ @ @ @ @ filesystem.If this option is repeated , rsync omits all mount-point directories from the copy . Otherwise , it includes an empty directory at each mount-point it encounters ( using the attributes of the mounted directory because those of the underlying mount-point directory are inaccessible ) . <p> If rsync has been told to collapse symlinks ( via- copy-links- or- copy-unsafe-links ) , a symlink to a directory on another device is treated like a mount-point . Symlinks to non-directories are unaffected by this option . <p> existing , ignore-non-existing <p> This tells rsync to skip creating files ( including directories ) that do not exist yet on the destination . If this option is combined with the- ignore-existing- option , no files will be updated ( which can be useful if all you want to do is delete extraneous files ) . This option is a transfer rule , not an exclude , so it does n't  affect the data that goes into the file-lists , and thus it does n't  affect deletions . It just limits the files that the receiver requests to be transferred . <p> ignore-existing <p> @ @ @ @ @ @ @ @ @ @ on the destination ( this does- not- ignore existing directories , or nothing would get done ) . See also- existing.This option is a transfer rule , not an exclude , so it does n't  affect the data that goes into the file-lists , and thus it does n't  affect deletions . It just limits the files that the receiver requests to be transferred . <p> This option can be useful for those doing backups using the- link-dest- option when they need to continue a backup run that got interrupted . Since a- link-dest- run is copied into a new directory hierarchy ( when it is used properly ) , using- ignore existing- will ensure that the already-handled files do n't  get tweaked ( which avoids a change in permissions on the hard-linked files ) . This does mean that this option is only looking at the existing files in the destination hierarchy itself . <p> remove-source-files <p> This tells rsync to remove from the sending side the files ( meaning non-directories ) that are a part of the transfer and have been successfully duplicated on the receiving side.Note that @ @ @ @ @ @ @ @ @ @ are quiescent . If you are using this to move files that show up in a particular directory over to another host , make sure that the finished files get renamed into the source directory , not directly written into it , so that rsync ca n't possibly transfer a file that is not yet fully written . If you cant first write the files into a different directory , you should use a naming idiom that let 's rsync avoid transferring files that are not yet finished ( e.g. name the file " foo.new " when it is written , rename it to " foo " when it is done , and then use the option- exclude=*.new- for the rsync transfer ) . <p> Starting with 3.1.0 , rsync will skip the sender-side removal ( and output an error ) if the files size or modify time has not stayed unchanged . <p> delete <p> This tells rsync to delete extraneous files from the receiving side ( ones that are n't  on the sending side ) , but only for the directories that are being synchronized . You must have asked @ @ @ @ @ @ @ @ @ @ " or " dir/ " ) without using a wildcard for the directorys contents ( e.g. " dir/* " ) since the wildcard is expanded by the shell and rsync thus gets a request to transfer individual files , not the files parent directory . Files that are excluded from the transfer are also excluded from being deleted unless you use the- delete-excluded- option or mark the rules as only matching on the sending side ( see the include/exclude modifiers in the FILTER RULES section ) . Prior to rsync 2.6.7 , this option would have no effect unless- recursive- was enabled . Beginning with 2.6.7 , deletions will also occur when- dirs- ( -d ) is enabled , but only for directories whose contents are being copied . <p> This option can be dangerous if used incorrectly ! It is a very good idea to first try a run using the- dry-run- option ( -n ) to see what files are going to be deleted . <p> If the sending side detects any I/O errors , then the deletion of any files at the destination will be automatically disabled @ @ @ @ @ @ @ @ @ @ as NFS errors ) on the sending side from causing a massive deletion of files on the destination . You can override this with the- ignore-errors- option . <p> The- delete- option may be combined with one of the delete-WHEN options without conflict , as well as- delete-excluded . However , if none of the delete-WHEN options are specified , rsync will choose the- delete-during- algorithm when talking to rsync 3.0.0 or newer , and the- delete-beforealgorithm when talking to an older rsync . See also- delete-delay- and- delete-after . <p> delete-before <p> Request that the file-deletions on the receiving side be done before the transfer starts . See- delete- ( which is implied ) for more details on file-deletion.Deleting before the transfer is helpful if the filesystem is tight for space and removing extraneous files would help to make the transfer possible . However , it does introduce a delay before the start of the transfer , and this delay might cause the transfer to timeout ( if- timeout- was specified ) . It also forces rsync to use the old , non-incremental recursion algorithm that requires rsync to @ @ @ @ @ @ @ @ @ @ once ( see- recursive ) . <p> delete-during , del <p> Request that the file-deletions on the receiving side be done incrementally as the transfer happens . The per-directory delete scan is done right before each directory is checked for updates , so it behaves like a more efficient- delete-before , including doing the deletions prior to any per-directory filter files being updated . This option was first added in rsync version 2.6.4 . See- delete- ( which is implied ) for more details on file-deletion . <p> delete-delay <p> Request that the file-deletions on the receiving side be computed during the transfer ( like- delete-during ) , and then removed after the transfer completes . This is useful when combined with- delay-updates- and/or- fuzzy , and is more efficient than using- delete-after- ( but can behave differently , since- delete-after- computes the deletions in a separate pass after all updates are done ) . If the number of removed files overflows an internal buffer , a temporary file will be created on the receiving side to hold the names ( it is removed while open , so you should n't @ @ @ @ @ @ @ @ @ @ of the temporary file fails , rsync will try to fall back to using- delete-after- ( which it can not do if- recursive- is doing an incremental scan ) . See- delete- ( which is implied ) for more details on file-deletion . <p> delete-after <p> Request that the file-deletions on the receiving side be done after the transfer has completed . This is useful if you are sending new per-directory merge files as a part of the transfer and you want their exclusions to take effect for the delete phase of the current transfer . It also forces rsync to use the old , non-incremental recursion algorithm that requires rsync to scan all the files in the transfer into memory at once ( see- recursive ) . See- delete- ( which is implied ) for more details on file-deletion . <p> delete-excluded <p> In addition to deleting the files on the receiving side that are not on the sending side , this tells rsync to also delete any files on the receiving side that are excluded ( see- exclude ) . See the FILTER RULES section for a way @ @ @ @ @ @ @ @ @ @ , and for a way to protect files from- delete-excluded . See- delete- ( which is implied ) for more details on file-deletion . <p> ignore-missing-args <p> When rsync is first processing the explicitly requested source files ( e.g. command-line arguments or- files-from- entries ) , it is normally an error if the file can not be found . This option suppresses that error , and does not try to transfer the file . This does not affect subsequent vanished-file errors if a file was initially found to be present and later is no longer there . <p> delete-missing-args <p> This option takes the behavior of ( the implied ) - ignore-missing-args- option a step farther : each missing arg will become a deletion request of the corresponding destination file on the receiving side ( should it exist ) . If the destination file is a non-empty directory , it will only be successfully deleted if force or delete are in effect . Other than that , this option is independent of any other type of delete processing.The missing source files are represented by special file-list entries which display as @ @ @ @ @ @ @ @ @ @ <p> ignore-errors <p> Tells- delete- to go ahead and delete files even when there are I/O errors . <p> force <p> This option tells rsync to delete a non-empty directory when it is to be replaced by a non-directory . This is only relevant if deletions are not active ( see- delete- for details ) . Note for older rsync versions : - force- used to still be required when using- delete-after , and it used to be non-functional unless the- recursive- option was also enabled . <p> max-delete=NUM <p> This tells rsync not to delete more than NUM files or directories . If that limit is exceeded , all further deletions are skipped through the end of the transfer . At the end , rsync outputs a warning ( including a count of the skipped deletions ) and exits with an error code of 25 ( unless some more important error condition also occurred ) . Beginning with version 3.0.0 , you may specify- max-delete=0- to be warned about any extraneous files in the destination without removing any of them . Older clients interpreted this as " unlimited " @ @ @ @ @ @ @ @ @ @ client is , you can use the less obvious- max-delete=-1- as a backward-compatible way to specify that no deletions be allowed ( though really old versions did n't  warn when the limit was exceeded ) . <p> max-size=SIZE <p> This tells rsync to avoid transferring any file that is larger than the specified SIZE . The SIZE value can be suffixed with a string to indicate a size multiplier , and may be a fractional value ( e.g. " max-size=1.5m " ) . This option is a transfer rule , not an exclude , so it does n't  affect the data that goes into the file-lists , and thus it does n't  affect deletions . It just limits the files that the receiver requests to be transferred . <p> The suffixes are as follows : " K " ( or " KiB " ) is a kibibyte ( 1024 ) , " M " ( or " MiB " ) is a mebibyte ( 1024*1024 ) , and " G " ( or " GiB " ) is a gibibyte ( 1024*1024*1024 ) . If you want the multiplier to @ @ @ @ @ @ @ @ @ @ , " MB " , or " GB " . ( Note : lower-case is also accepted for all values . ) Finally , if the suffix ends in either " +1 " or " -1 " , the value will be offset by one byte in the indicated direction . <p> Examples : max-size=1.5mb-1 is 1499999 bytes , and max-size=2g+1 is 2147483649 bytes . <p> Note that rsync versions prior to 3.1.0 did not allow- max-size=0 . <p> min-size=SIZE <p> This tells rsync to avoid transferring any file that is smaller than the specified SIZE , which can help in not transferring small , junk files . See the- max-size- option for a description of SIZE and other information.Note that rsync versions prior to 3.1.0 did not allow- min-size=0 . <p> -B , block-size=BLOCKSIZE <p> This forces the block size used in rsyncs delta-transfer algorithm to a fixed value . It is normally selected based on the size of each file being updated . See the technical report for details . <p> -e , rsh=COMMAND <p> This option allows you to choose an alternative remote shell program to use @ @ @ @ @ @ @ @ @ @ . Typically , rsync is configured to use ssh by default , but you may prefer to use rsh on a local network.If this option is used with- user@host : : module/path , then the remote shell- COMMAND- will be used to run an rsync daemon on the remote host , and all data will be transmitted through that remote shell connection , rather than through a direct socket connection to a running rsync daemon on the remote host . See the section " USING RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION " above . <p> Command-line arguments are permitted in COMMAND provided that COMMAND is presented to rsync as a single argument . You must use spaces ( not tabs or other whitespace ) to separate the command and args from each other , and you can use single- and/or double-quotes to preserve spaces in an argument ( but not backslashes ) . Note that doubling a single-quote inside a single-quoted string gives you a single-quote ; likewise for double-quotes ( though you need to pay attention to which quotes your shell is parsing and which quotes rsync is parsing @ @ @ @ @ @ @ @ @ @ the remote shell program using the RSYNCRSH environment variable , which accepts the same range of values as- -e . <p> See also the- blocking-io- option which is affected by this option . <p> rsync-path=PROGRAM <p> Use this to specify what program is to be run on the remote machine to start-up rsync . Often used when rsync is not in the default remote-shells path ( e.g. **31;3673;TOOLONG ) . Note that PROGRAM is run with the help of a shell , so it can be any program , script , or command sequence you 'd care to run , so long as it does not corrupt the standard-in &amp; standard-out that rsync is using to communicate.One tricky example is to set a different default directory on the remote machine for use with the- relative- option . For instance : <p> rsync -avR --rsync-path= " cd /a/b &amp;&amp; rsync " host:c/d /e/ <p> -M , remote-option=OPTION <p> This option is used for more advanced situations where you want certain effects to be limited to one side of the transfer only . For instance , if you want to pass- log-file=FILE- and- @ @ @ @ @ @ @ @ @ @ : <p> rsync -av -M --log-file=foo -M--fake-super src/ dest/ <p> If you want to have an option affect only the local side of a transfer when it normally affects both sides , send its negation to the remote side . Like this : <p> rsync -av -x -M--no-x src/ dest/ <p> Be cautious using this , as it is possible to toggle an option that will cause rsync to have a different idea about what data to expect next over the socket , and that will make it fail in a cryptic fashion . <p> Note that it is best to use a separate- remote-option- for each option you want to pass . This makes your useage compatible with the- protect-args- option . If that option is off , any spaces in your remote options will be split by the remote shell unless you take steps to protect them . <p> When performing a local transfer , the " local " side is the sender and the " remote " side is the receiver . <p> Note some versions of the popt option-parsing library have a bug in them that @ @ @ @ @ @ @ @ @ @ in it next to a short option letter ( e.g.- -M--log-file=/tmp/foo . If this bug affects your version of popt , you can use the version of popt that is included with rsync . <p> -C , cvs-exclude <p> This is a useful shorthand for excluding a broad range of files that you often do n't  want to transfer between systems . It uses a similar algorithm to CVS to determine if a file should be ignored.The exclude list is initialized to exclude the following items ( these initial items are marked as perishable see the FILTER RULES section ) : <p> then , files listed in a $HOME/.cvsignore are added to the list and any files listed in the CVSIGNORE environment variable ( all cvsignore names are delimited by whitespace ) . <p> Finally , any file is ignored if it is in the same directory as a . cvsignore file and matches one of the patterns listed therein . Unlike rsyncs filter/exclude files , these patterns are split on whitespace . See the- cvs(1) manual for more information . <p> If you 're combining- -C- with your own- filter- @ @ @ @ @ @ @ @ @ @ appended at the end of your own rules , regardless of where the- -C- was placed on the command-line . This makes them a lower priority than any rules you specified explicitly . If you want to control where these CVS excludes get inserted into your filter rules , you should omit the- -C- as a command-line option and use a combination of- filter=:C- and- filter=-C- ( either on your command-line or by putting the " : C " and " -C " rules into a filter file with your other rules ) . The first option turns on the per-directory scanning for the . cvsignore file . The second option does a one-time import of the CVS excludes mentioned above . <p> -f , filter=RULE <p> This option allows you to add rules to selectively exclude certain files from the list of files to be transferred . This is most useful in combination with a recursive transfer.You may use as many- filter- options on the command line as you like to build up the list of files to exclude . If the filter contains whitespace , be sure to @ @ @ @ @ @ @ @ @ @ rsync as a single argument . The text below also mentions that you can use an underscore to replace the space that separates a rule from its arg . <p> See the FILTER RULES section for detailed information on this option . <p> -F <p> The- -F- option is a shorthand for adding two- filter- rules to your command . The first time it is used is a shorthand for this rule : <p> --filter='dir-merge /. rsync-filter ' <p> This tells rsync to look for per-directory . rsync-filter files that have been sprinkled through the hierarchy and use their rules to filter the files in the transfer . If- -F- is repeated , it is a shorthand for this rule : <p> --filter='exclude . rsync-filter ' <p> This filters out the . rsync-filter files themselves from the transfer . <p> See the FILTER RULES section for detailed information on how these options work . <p> exclude=PATTERN <p> This option is a simplified form of the- filter- option that defaults to an exclude rule and does not allow the full rule-parsing syntax of normal filter rules.See the FILTER RULES section for @ @ @ @ @ @ @ @ @ @ option is related to the- exclude- option , but it specifies a FILE that contains exclude patterns ( one per line ) . Blank lines in the file and lines starting with ; or # are ignored . If- FILE- is- , the list will be read from standard input . <p> include=PATTERN <p> This option is a simplified form of the- filter- option that defaults to an include rule and does not allow the full rule-parsing syntax of normal filter rules.See the FILTER RULES section for detailed information on this option . <p> include-from=FILE <p> This option is related to the- include- option , but it specifies a FILE that contains include patterns ( one per line ) . Blank lines in the file and lines starting with ; or # are ignored . If- FILE- is- , the list will be read from standard input . <p> files-from=FILE <p> Using this option allows you to specify the exact list of files to transfer ( as read from the specified FILE or- - for standard input ) . It also tweaks the default behavior of rsync to make transferring @ @ @ @ @ @ @ @ @ @ relative- ( -R ) option is implied , which preserves the path information that is specified for each item in the file ( use- no-relative- or- no-R- if you want to turn that off ) . <p> The- dirs- ( -d ) option is implied , which will create directories specified in the list on the destination rather than noisily skipping them ( use- no-dirs- or- no-d- if you want to turn that off ) . <p> The- archive- ( -a ) options behavior does not imply- recursive- ( -r ) , so specify it explicitly , if you want it . <p> These side-effects change the default state of rsync , so the position of the- files-from- option on the command-line has no bearing on how other options are parsed ( e.g.- -a- works the same before or after- files-from , as does- no-R- and all other options ) . <p> The filenames that are read from the FILE are all relative to the source dir any leading slashes are removed and no " .. " references are allowed to go higher than the source dir . For example @ @ @ @ @ @ @ @ @ @ remote : /backup <p> If /tmp/foo contains the string " bin " ( or even " /bin " ) , the /usr/bin directory will be created as /backup/bin on the remote host . If it contains " bin/ " ( note the trailing slash ) , the immediate contents of the directory would also be sent ( without needing to be explicitly mentioned in the file this began in version 2.6.4 ) . In both cases , if the- -r- option was enabled , that dirs entire hierarchy would also be transferred ( keep in mind that- -r- needs to be specified explicitly with- files-from , since it is not implied by- -a ) . Also note that the effect of the ( enabled by default ) - relative- option is to duplicate only the path info that is read from the file it does not force the duplication of the source-spec path ( /usr in this case ) . <p> In addition , the- files-from- file can be read from the remote host instead of the local host if you specify a " host : " in front of @ @ @ @ @ @ @ @ @ @ the transfer ) . As a short-cut , you can specify just a prefix of " : " to mean " use the remote end of the transfer " . For example : <p> rsync -a --files-from= : /path/file-list src : / /tmp/copy <p> This would copy all the files specified in the /path/file-list file that was located on the remote " src " host . <p> If the- iconv- and- protect-args- options are specified and the- files-from- filenames are being sent from one host to another , the filenames will be translated from the sending hosts charset to the receiving hosts charset . <p> NOTE : sorting the list of files in the files-from input helps rsync to be more efficient , as it will avoid re-visiting the path elements that are shared between adjacent entries . If the input is not sorted , some path elements ( implied directories ) may end up being scanned multiple times , and rsync will eventually unduplicate them after they get turned into file-list elements . <p> -0 , from0 <p> This tells rsync that the rules/filenames it reads from a file @ @ @ @ @ @ @ @ @ @ not a NL , CR , or CR+LF . This affects- exclude-from , - include-from , - files-from , and any merged files specified in a- filter- rule . It does not affect- cvs-exclude- ( since all names read from a . cvsignore file are split on whitespace ) . <p> -s , protect-args <p> This option sends all filenames and most options to the remote rsync without allowing the remote shell to interpret them . This means that spaces are not split in names , and any non-wildcard special characters are not translated ( such as , $ , ; , &amp; , etc . ) . Wildcards are expanded on the remote host by rsync ( instead of the shell doing it ) . If you use this option with- iconv , the args related to the remote side will also be translated from the local to the remote character-set . The translation happens before wild-cards are expanded . See also the- files-from- option . <p> You may also control this option via the RSYNCPROTECTARGS environment variable . If this variable has a non-zero value , this option @ @ @ @ @ @ @ @ @ @ disabled by default . Either state is overridden by a manually specified positive or negative version of this option ( note thatno-s- and- no-protect-args- are the negative versions ) . Since this option was first introduced in 3.0.0 , you 'll need to make sure its disabled if you ever need to interact with a remote rsync that is older than that . <p> Rsync can also be configured ( at build time ) to have this option enabled by default ( with is overridden by both the environment and the command-line ) . This option will eventually become a new default setting at some as-yet-undetermined point in the future . <p> -T , temp-dir=DIR <p> This option instructs rsync to use DIR as a scratch directory when creating temporary copies of the files transferred on the receiving side . The default behavior is to create each temporary file in the same directory as the associated destination file.This option is most often used when the receiving disk partition does not have enough free space to hold a copy of the largest file in the transfer . In this case ( i.e. @ @ @ @ @ @ @ @ @ @ ) , rsync will not be able to rename each received temporary file over the top of the associated destination file , but instead must copy it into place . Rsync does this by copying the file over the top of the destination file , which means that the destination file will contain truncated data during this copy . If this were not done this way ( even if the destination file were first removed , the data locally copied to a temporary file in the destination directory , and then renamed into place ) it would be possible for the old file to continue taking up disk space ( if someone had it open ) , and thus there might not be enough room to fit the new version on the disk at the same time . <p> If you are using this option for reasons other than a shortage of disk space , you may wish to combine it with the- delay-updates- option , which will ensure that all copied files get put into subdirectories in the destination hierarchy , awaiting the end of the transfer . If @ @ @ @ @ @ @ @ @ @ arriving files on the destination partition , another way to tell rsync that you are n't  overly concerned about disk space is to use the- partial-dir- option with a relative path ; because this tells rsync that it is OK to stash off a copy of a single file in a subdir in the destination hierarchy , rsync will use the partial-dir as a staging area to bring over the copied file , and then rename it into place from there . ( Specifying a- partial-dir- with an absolute path does not have this side-effect . ) <p> -y , fuzzy <p> This option tells rsync that it should look for a basis file for any destination file that is missing . The current algorithm looks in the same directory as the destination file for either a file that has an identical size and modified-time , or a similarly-named file . If found , rsync uses the fuzzy basis file to try to speed up the transfer.If the option is repeated , the fuzzy scan will also be done in any matching alternate destination directories that are specified via- compare-dest @ @ @ @ @ @ @ @ @ @ the use of the- delete- option might get rid of any potential fuzzy-match files , so either use- delete-after- or specify some filename exclusions if you need to prevent this . <p> compare-dest=DIR <p> This option instructs rsync to use- DIR- on the destination machine as an additional hierarchy to compare destination files against doing transfers ( if the files are missing in the destination directory ) . If a file is found in- DIR- that is identical to the senders file , the file will NOT be transferred to the destination directory . This is useful for creating a sparse backup of just files that have changed from an earlier backup . This option is typically used to copy into an empty ( or newly created ) directory.Beginning in version 2.6.4 , multiple- compare-dest- directories may be provided , which will cause rsync to search the list in the order specified for an exact match . If a match is found that differs only in attributes , a local copy is made and the attributes updated . If a match is not found , a basis file from one @ @ @ @ @ @ @ @ @ @ up the transfer . <p> If- DIR- is a relative path , it is relative to the destination directory . See also- copy-dest- and- link-dest . <p> NOTE : beginning with version 3.1.0 , rsync will remove a file from a non-empty destination hierarchy if an exact match is found in one of the compare-dest hierarchies ( making the end result more closely match a fresh copy ) . <p> copy-dest=DIR <p> This option behaves like- compare-dest , but rsync will also copy unchanged files found in- DIR- to the destination directory using a local copy . This is useful for doing transfers to a new destination while leaving existing files intact , and then doing a flash-cutover when all files have been successfully transferred.Multiple- copy-dest- directories may be provided , which will cause rsync to search the list in the order specified for an unchanged file . If a match is not found , a basis file from one of the- DIRs will be selected to try to speed up the transfer . <p> If- DIR- is a relative path , it is relative to the destination directory . @ @ @ @ @ @ @ @ @ @ option behaves like- copy-dest , but unchanged files are hard linked from- DIR- to the destination directory . The files must be identical in all preserved attributes ( e.g. permissions , possibly ownership ) in order for the files to be linked together . An example : <p> rsync -av **25;3706;TOOLONG host:srcdir/ newdir/ <p> If files are n't  linking , double-check their attributes . Also check if some attributes are getting forced outside of rsyncs control , such a mount option that squishes root to a single user , or mounts a removable drive with generic ownership ( such as OS Xs " Ignore ownership on this volume " option ) . <p> Beginning in version 2.6.4 , multiple- link-dest- directories may be provided , which will cause rsync to search the list in the order specified for an exact match . If a match is found that differs only in attributes , a local copy is made and the attributes updated . If a match is not found , a basis file from one of theDIRs will be selected to try to speed up the transfer . <p> This @ @ @ @ @ @ @ @ @ @ , as existing files may get their attributes tweaked , and that can affect alternate destination files via hard-links . Also , itemizing of changes can get a bit muddled . Note that prior to version 3.1.0 , an alternate-directory exact match would never be found ( nor linked into the destination ) when a destination file already exists . <p> Note that if you combine this option with- ignore-times , rsync will not link any files together because it only links identical files together as a substitute for transferring the file , never as an additional check after the file is updated . <p> If- DIR- is a relative path , it is relative to the destination directory . See also- compare-dest- and- copy-dest . <p> Note that rsync versions prior to 2.6.1 had a bug that could prevent- link-dest- from working properly for a non-super-user when- -o- was specified ( or implied by- -a ) . You can work-around this bug by avoiding the- -o- option when sending to an old rsync . <p> -z , compress <p> With this option , rsync compresses the file data as @ @ @ @ @ @ @ @ @ @ the amount of data being transmitted something that is useful over a slow connection.Note that this option typically achieves better compression ratios than can be achieved by using a compressing remote shell or a compressing transport because it takes advantage of the implicit information in the matching data blocks that are not explicitly sent over the connection . <p> See the- skip-compress- option for the default list of file suffixes that will not be compressed . <p> compress-level=NUM <p> Explicitly set the compression level to use ( see- compress ) instead of letting it default . If NUM is non-zero , the- compress- option is implied . <p> skip-compress=LIST <p> Override the list of file suffixes that will not be compressed . The- LIST- should be one or more file suffixes ( without the dot ) separated by slashes ( / ) . You may specify an empty string to indicate that no file should be skipped . <p> Simple character-class matching is supported : each must consist of a list of letters inside the square brackets ( e.g. no special classes , such as " : alpha : " @ @ @ @ @ @ @ @ @ @ ) . <p> The characters asterisk ( * ) and question-mark ( ? ) have no special meaning . <p> This list will be replaced by your- skip-compress- list in all but one situation : a copy from a daemon rsync will add your skipped suffixes to its list of non-compressing files ( and its list may be configured to a different default ) . <p> numeric-ids <p> With this option rsync will transfer numeric group and user IDs rather than using user and group names and mapping them at both ends.By default rsync will use the username and groupname to determine what ownership to give files . The special uid 0 and the special group 0 are never mapped via user/group names even if the- numeric-ids- option is not specified . <p> If a user or group has no name on the source system or it has no match on the destination system , then the numeric I 'd from the source system is used instead . See also the comments on the " use chroot " setting in the rsyncd.conf manpage for information on how the chroot setting affects @ @ @ @ @ @ @ @ @ @ and groups and what you can do about it . <p> usermap=STRING , groupmap=STRING <p> These options allow you to specify users and groups that should be mapped to other values by the receiving side . The- STRING- is one or more- FROM:TO- pairs of values separated by commas . Any matching- FROM- value from the sender is replaced with a- TO- value from the receiver . You may specify usernames or user IDs for the- FROM- and- TO- values , and the- FROM- value may also be a wild-card string , which will be matched against the senders names ( wild-cards do NOT match against I 'd numbers , though see below for why a * matches everything ) . You may instead specify a range of I 'd numbers via an inclusive range : LOW-HIGH . For example : <p> --usermap=0-99:nobody , wayne:admin , *:normal --groupmap=usr:1,1:usr <p> The first match in the list is the one that is used . You should specify all your user mappings using a single- usermap- option , and/or all your group mappings using a single- groupmap- option . <p> Note that the senders name @ @ @ @ @ @ @ @ @ @ the receiver , so you should either match these values using a 0 , or use the names in effect on the receiving side ( typically " root " ) . All other- FROM- names match those in use on the sending side . All- TO- names match those in use on the receiving side . <p> Any IDs that do not have a name on the sending side are treated as having an empty name for the purpose of matching . This allows them to be matched via a " * " or using an empty name . For instance : <p> --usermap=:nobody --groupmap=*:nobody <p> When the- numeric-ids- option is used , the sender does not send any names , so all the IDs are treated as having an empty name . This means that you will need to specify numeric- FROM- values if you want to map these nameless IDs to different values . <p> For the- usermap- option to have any effect , the- -o- ( owner ) option must be used ( or implied ) , and the receiver will need to be running as a @ @ @ @ @ @ @ @ @ @ the- groupmap- option to have any effect , the- -g- ( groups ) option must be used ( or implied ) , and the receiver will need to have permissions to set that group . <p> chown=USER:GROUP <p> This option forces all files to be owned by USER with group GROUP . This is a simpler interface than using- usermap- and- groupmap- directly , but it is implemented using those options internally , so you can not mix them . If either the USER or GROUP is empty , no mapping for the omitted user/group will occur . If GROUP is empty , the trailing colon may be omitted , but if USER is empty , a leading colon must be supplied.If you specify " chown=foo:bar , this is exactly the same as specifying " usermap=*:foo groupmap=*:bar " , only easier . <p> timeout=TIMEOUT <p> This option allows you to set a maximum I/O timeout in seconds . If no data is transferred for the specified time then rsync will exit . The default is 0 , which means no timeout . <p> contimeout <p> This option allows you to @ @ @ @ @ @ @ @ @ @ its connection to an rsync daemon to succeed . If the timeout is reached , rsync exits with an error . <p> address <p> By default rsync will bind to the wildcard address when connecting to an rsync daemon . The- address- option allows you to specify a specific IP address ( or hostname ) to bind to . See also this option in the- daemon- mode section . <p> port=PORT <p> This specifies an alternate TCP port number to use rather than the default of 873 . This is only needed if you are using the double-colon ( : : ) syntax to connect with an rsync daemon ( since the URL syntax has a way to specify the port as a part of the URL ) . See also this option in the- daemon- mode section . <p> sockopts <p> This option can provide endless fun for people who like to tune their systems to the utmost degree . You can set all sorts of socket options which may make transfers faster ( or slower ! ) . Read the man page for the- setsockopt()- system call for @ @ @ @ @ @ @ @ @ @ to set . By default no special socket options are set . This only affects direct socket connections to a remote rsync daemon . This option also exists in the- daemon- mode section . <p> blocking-io <p> This tells rsync to use blocking I/O when launching a remote shell transport . If the remote shell is either rsh or remsh , rsync defaults to using blocking I/O , otherwise it defaults to using non-blocking I/O . ( Note that ssh prefers non-blocking I/O . ) <p> outbuf=MODE <p> This sets the output buffering mode . The mode can be None ( aka Unbuffered ) , Line , or Block ( aka Full ) . You may specify as little as a single letter for the mode , and use upper or lower case.The main use of this option is to change Full buffering to Line buffering when rsyncs output is going to a file or pipe . <p> -i , itemize-changes <p> Requests a simple itemized list of the changes that are being made to each file , including attribute changes . This is exactly the same as specifying- out-format=%i @ @ @ @ @ @ @ @ @ @ will also be output , but only if the receiving rsync is at least version 2.6.7 ( you can use- -vvwith older versions of rsync , but that also turns on the output of other verbose messages ) . The " %i " escape has a cryptic output that is 11 letters long . The general format is like the string- YXcstpoguax , where- Y- is replaced by the type of update being done , - X- is replaced by the file-type , and the other letters represent attributes that may be output if they are being modified . <p> The update types that replace the- Y- are as follows : <p> A- &lt;- means that a file is being transferred to the remote host ( sent ) . <p> A- &gt;- means that a file is being transferred to the local host ( received ) . <p> A- c- means that a local change/creation is occurring for the item ( such as the creation of a directory or the changing of a symlink , etc . ) . <p> A- h- means that the item is a hard link @ @ @ @ @ @ @ @ @ @ . - means that the item is not being updated ( though it might have attributes that are being modified ) . <p> A- *- means that the rest of the itemized-output area contains a message ( e.g. " deleting " ) . <p> The file-types that replace the- X- are : - f- for a file , a- d- for a directory , an- L- for a symlink , a- D- for a device , and a- S- for a special file ( e.g. named sockets and fifos ) . <p> The other letters in the string above are the actual letters that will be output if the associated attribute for the item is being updated or a " . " for no change . Three exceptions to this are : ( 1 ) a newly created item replaces each letter with a " + " , ( 2 ) an identical item replaces the dots with spaces , and ( 3 ) an unknown attribute replaces each letter with a " ? " ( this can happen when talking to an older rsync ) . <p> The attribute @ @ @ @ @ @ @ @ @ @ <p> A- c- means either that a regular file has a different checksum ( requires- checksum ) or that a symlink , device , or special file has a changed value . Note that if you are sending files to an rsync prior to 3.0.1 , this change flag will be present only for checksum-differing regular files . <p> A- s- means the size of a regular file is different and will be updated by the file transfer . <p> A- t- means the modification time is different and is being updated to the senders value ( requires- times ) . An alternate value of- T- means that the modification time will be set to the transfer time , which happens when a file/symlink/device is updated without- times- and when a symlink is changed and the receiver ca n't set its time . ( Note : when using an rsync 3.0.0 client , you might see the- s- flag combined with- t- instead of the proper- T- flag for this time-setting failure . ) <p> A- p- means the permissions are different and are being updated to the senders value ( @ @ @ @ @ @ @ @ @ @ is different and is being updated to the senders value ( requires- owner- and super-user privileges ) . <p> A- g- means the group is different and is being updated to the senders value ( requires- group- and the authority to set the group ) . <p> The- u- slot is reserved for future use . <p> The- a- means that the ACL information changed . <p> The- x- means that the extended attribute information changed . <p> One other output is possible : when deleting files , the " %i " will output the string " *deleting " for each item that is being removed ( assuming that you are talking to a recent enough rsync that it logs deletions instead of outputting them as a verbose message ) . <p> out-format=FORMAT <p> This allows you to specify exactly what the rsync client outputs to the user on a per-update basis . The format is a text string containing embedded single-character escape sequences prefixed with a percent ( % ) character . A default format of " %n%L " is assumed if either- info=name- or- -v- is specified ( @ @ @ @ @ @ @ @ @ @ , if the item is a link , where it points ) . For a full list of the possible escape characters , see the " log format " setting in the rsyncd.conf manpage.Specifying the- out-format- option implies the- info=name- option , which will mention each file , dir , etc. that gets updated in a significant way ( a transferred file , a recreated symlink/device , or a touched directory ) . In addition , if the itemize-changes escape ( %i ) is included in the string ( e.g. if the- itemize-changes- option was used ) , the logging of names increases to mention any item that is changed in any way ( as long as the receiving side is at least 2.6.4 ) . See the- itemize-changes- option for a description of the output of " %i " . <p> Rsync will output the out-format string prior to a files transfer unless one of the transfer-statistic escapes is requested , in which case the logging is done at the end of the files transfer . When this late logging is in effect and- progress- is also specified , @ @ @ @ @ @ @ @ @ @ transferred prior to its progress information ( followed , of course , by the out-format output ) . <p> log-file=FILE <p> This option causes rsync to log what it is doing to a file . This is similar to the logging that a daemon does , but can be requested for the client side and/or the server side of a non-daemon transfer . If specified as a client option , transfer logging will be enabled with a default format of " %i %n%L " . See the- log-file-format- option if you wish to override this.Heres a example command that requests the remote side to log what is happening : <p> rsync -av **36;3733;TOOLONG src/ dest/ <p> This is very useful if you need to debug why a connection is closing unexpectedly . <p> log-file-format=FORMAT <p> This allows you to specify exactly what per-update logging is put into the file specified by the- log-file- option ( which must also be specified for this option to have any effect ) . If you specify an empty string , updated files will not be mentioned in the log file . For a list @ @ @ @ @ @ @ @ @ @ format " setting in the rsyncd.conf manpage.The default FORMAT used if- log-file- is specified and this option is not is %i %n%L . <p> stats <p> This tells rsync to print a verbose set of statistics on the file transfer , allowing you to tell how effective rsyncs delta-transfer algorithm is for your data . This option is equivalent to- info=stats2- if combined with 0 or 1- -v- options , or- info=stats3- if combined with 2 or more- -v- options.The current statistics are as follows : <p> Number of files- is the count of all " files " ( in the generic sense ) , which includes directories , symlinks , etc . The total count will be followed by a list of counts by filetype ( if the total is non-zero ) . For example : " ( reg : 5 , dir : 3 , link : 2 , dev : 1 , special : 1 ) " lists the totals for regular files , directories , symlinks , devices , and special files . If any of value is 0 , it is completely omitted from the @ @ @ @ @ @ @ @ @ @ of how many " files " ( generic sense ) were created ( as opposed to updated ) . The total count will be followed by a list of counts by filetype ( if the total is non-zero ) . <p> Number of deleted files- is the count of how many " files " ( generic sense ) were created ( as opposed to updated ) . The total count will be followed by a list of counts by filetype ( if the total is non-zero ) . Note that this line is only output if deletions are in effect , and only if protocol 31 is being used ( the default for rsync 3.1. x ) . <p> Number of regular files transferred- is the count of normal files that were updated via rsyncs delta-transfer algorithm , which does not include dirs , symlinks , etc . Note that rsync 3.1.0 added the word " regular " into this heading . <p> Total file size- is the total sum of all file sizes in the transfer . This does not count any size for directories or special files , @ @ @ @ @ @ @ @ @ @ transferred file size- is the total sum of all files sizes for just the transferred files . <p> Literal data- is how much unmatched file-update data we had to send to the receiver for it to recreate the updated files . <p> Matched data- is how much data the receiver got locally when recreating the updated files . <p> File list size- is how big the file-list data was when the sender sent it to the receiver . This is smaller than the in-memory size for the file list due to some compressing of duplicated data when rsync sends the list . <p> File list generation time- is the number of seconds that the sender spent creating the file list . This requires a modern rsync on the sending side for this to be present . <p> File list transfer time- is the number of seconds that the sender spent sending the file list to the receiver . <p> Total bytes sent- is the count of all the bytes that rsync sent from the client side to the server side . <p> Total bytes received- is the count of all @ @ @ @ @ @ @ @ @ @ the server side . " Non-message " bytes means that we do n't  count the bytes for a verbose message that the server sent to us , which makes the stats more consistent . <p> -8 , 8-bit-output <p> This tells rsync to leave all high-bit characters unescaped in the output instead of trying to test them to see if they 're valid in the current locale and escaping the invalid ones . All control characters ( but never tabs ) are always escaped , regardless of this options setting.The escape idiom that started in 2.6.7 is to output a literal backslash ( ) and a hash ( # ) , followed by exactly 3 octal digits . For example , a newline would output as " #012 " . A literal backslash that is in a filename is not escaped unless it is followed by a hash and 3 digits ( 0-9 ) . <p> -h , human-readable <p> Output numbers in a more human-readable format . There are 3 possible levels : ( 1 ) output numbers with a separator between each set of 3 digits ( either a @ @ @ @ @ @ @ @ @ @ point is represented by a period or a comma ) ; ( 2 ) output numbers in units of 1000 ( with a character suffix for larger units see below ) ; ( 3 ) output numbers in units of 1024 . The default is human-readable level 1 . Each- -h- option increases the level by one . You can take the level down to 0 ( to output numbers as pure digits ) by specifing the- no-human-readable- ( no-h ) option . <p> The unit letters that are appended in levels 2 and 3 are : K ( kilo ) , M ( mega ) , G ( giga ) , or T ( tera ) . For example , a 1234567-byte file would output as 1.23M in level-2 ( assuming that a period is your local decimal point ) . <p> Backward compatibility note : versions of rsync prior to 3.1.0 do not support human-readable level 1 , and they default to level 0 . Thus , specifying one or two- -h- options will behave in a comparable manner in old and new versions as long as you @ @ @ @ @ @ @ @ @ @ more- -h- options . See the- list-only- option for one difference . <p> partial <p> By default , rsync will delete any partially transferred file if the transfer is interrupted . In some circumstances it is more desirable to keep partially transferred files . Using the- partial- option tells rsync to keep the partial file which should make a subsequent transfer of the rest of the file much faster . <p> partial-dir=DIR <p> A better way to keep partial files than the- partial- option is to specify a- DIR- that will be used to hold the partial data ( instead of writing it out to the destination file ) . On the next transfer , rsync will use a file found in this dir as data to speed up the resumption of the transfer and then delete it after it has served its purpose.Note that if- whole-file- is specified ( or implied ) , any partial-dir file that is found for a file that is being updated will simply be removed ( since rsync is sending files without using rsyncs delta-transfer algorithm ) . <p> Rsync will create the- DIR- @ @ @ @ @ @ @ @ @ @ the whole path ) . This makes it easy to use a relative path ( such as " **26;3771;TOOLONG " ) to have rsync create the partial-directory in the destination files directory when needed , and then remove it again when the partial file is deleted . <p> If the partial-dir value is not an absolute path , rsync will add an exclude rule at the end of all your existing excludes . This will prevent the sending of any partial-dir files that may exist on the sending side , and will also prevent the untimely deletion of partial-dir items on the receiving side . An example : the above- partial-dir- option would add the equivalent of " -f -p . rsync-partial/ " at the end of any other filter rules . <p> If you are supplying your own exclude rules , you may need to add your own exclude/hide/protect rule for the partial-dir because ( 1 ) the auto-added rule may be ineffective at the end of your other rules , or ( 2 ) you may wish to override rsyncs exclude choice . For instance , if you @ @ @ @ @ @ @ @ @ @ be lying around , you should specify- delete-after- and add a " risk " filter rule , e.g.- -f R . rsync-partial/ . ( Avoid using- delete-before- or- delete-during- unless you do n't  need rsync to use any of the left-over partial-dir data during the current run . ) <p> IMPORTANT : the- partial-dir- should not be writable by other users or it is a security risk . E.g. AVOID " /tmp " . <p> You can also set the partial-dir value the RSYNCPARTIALDIR environment variable . Setting this in the environment does not force- partial- to be enabled , but rather it affects where partial files go when- partial- is specified . For instance , instead of using- partial-dir=.rsync-tmp- along with- progress , you could set **26;3799;TOOLONG in your environment and then just use the- -P- option to turn on the use of the . rsync-tmp dir for partial transfers . The only times that the- partial- option does not look for this environment value are ( 1 ) when- inplace- was specified ( since- inplace- conflicts with- partial-dir ) , and ( 2 ) when- delay-updates- was specified @ @ @ @ @ @ @ @ @ @ the daemon-configs " refuse options " setting , - partial-dir- does- not- imply- partial . This is so that a refusal of the- partial- option can be used to disallow the overwriting of destination files with a partial transfer , while still allowing the safer idiom provided by- partial-dir . <p> delay-updates <p> This option puts the temporary file from each updated file into a holding directory until the end of the transfer , at which time all the files are renamed into place in rapid succession . This attempts to make the updating of the files a little more atomic . By default the files are placed into a directory named " . tmp " in each files destination directory , but if you 've specified the- partial-dir- option , that directory will be used instead . See the comments in the- partial-dir- section for a discussion of how this " . tmp " dir will be excluded from the transfer , and what you can do if you want rsync to cleanup old " . tmp " dirs that might be lying around . Conflicts with- inplace- and- append.This @ @ @ @ @ @ @ @ @ @ bit per file transferred ) and also requires enough free disk space on the receiving side to hold an additional copy of all the updated files . Note also that you should not use an absolute path to- partial-dir- unless ( 1 ) there is no chance of any of the files in the transfer having the same name ( since all the updated files will be put into a single directory if the path is absolute ) and ( 2 ) there are no mount points in the hierarchy ( since the delayed updates will fail if they ca n't be renamed into place ) . <p> See also the " atomic-rsync " perl script in the " support " subdir for an update algorithm that is even more atomic ( it uses- link-dest- and a parallel hierarchy of files ) . <p> -m , prune-empty-dirs <p> This option tells the receiving rsync to get rid of empty directories from the file-list , including nested directories that have no non-directory children . This is useful for avoiding the creation of a bunch of useless directories when the sending rsync is @ @ @ @ @ @ @ @ @ @ the use of transfer rules , such as the- min-size- option , does not affect what goes into the file list , and thus does not leave directories empty , even if none of the files in a directory match the transfer rule . <p> Because the file-list is actually being pruned , this option also affects what directories get deleted when a delete is active . However , keep in mind that excluded files and directories can prevent existing items from being deleted due to an exclude both hiding source files and protecting destination files . See the perishable filter-rule option for how to avoid this . <p> You can prevent the pruning of certain empty directories from the file-list by using a global " protect " filter . For instance , this option would ensure that the directory " emptydir " was kept in the file-list : <p> filter protect emptydir/ <p> here 's an example that copies all . pdf files in a hierarchy , only creating the necessary destination directories to hold the . pdf files , and ensures that any superfluous files and directories in the @ @ @ @ @ @ @ @ @ @ being used instead of an exclude ) : <p> rsync -avm del include=*.pdf -f hide , ! */ src/ dest <p> If you did n't  want to remove superfluous destination files , the more time-honored options of " include=*/ exclude=* " would work fine in place of the hide-filter ( if that is more natural to you ) . <p> progress <p> This option tells rsync to print information showing the progress of the transfer . This gives a bored user something to watch . With a modern rsync this is the same as specifying- info=flist2 , name , progress , but any user-supplied settings for those info flags takes precedence ( e.g. " info=flist0 progress " ) . While rsync is transferring a regular file , it updates a progress line that looks like this : <p> 782448 63% 110.64kB/s 0:00:04 <p> In this example , the receiver has reconstructed 782448 bytes or 63% of the senders file , which is being reconstructed at a rate of 110.64 kilobytes per second , and the transfer will finish in 4 seconds if the current rate is maintained until the end @ @ @ @ @ @ @ @ @ @ algorithm is in use . For example , if the senders file consists of the basis file followed by additional data , the reported rate will probably drop dramatically when the receiver gets to the literal data , and the transfer will probably take much longer to finish than the receiver estimated as it was finishing the matched part of the file . <p> When the file transfer finishes , rsync replaces the progress line with a summary line that looks like this : <p> 1,238,099 100% 146.38kB/s 0:00:08 ( xfr#5 , to-chk=169/396 ) <p> In this example , the file was 1,238,099 bytes long in total , the average rate of transfer for the whole file was 146.38 kilobytes per second over the 8 seconds that it took to complete , it was the 5th transfer of a regular file during the current rsync session , and there are 169 more files for the receiver to check ( to see if they are up-to-date or not ) remaining out of the 396 total files in the file-list . <p> In an incremental recursion scan , rsync wont know the @ @ @ @ @ @ @ @ @ @ the ends of the scan , but since it starts to transfer files during the scan , it will display a line with the text " ir-chk " ( for incremental recursion check ) instead of " to-chk " until the point that it knows the full size of the list , at which point it will switch to using " to-chk " . Thus , seeing " ir-chk " let 's you know that the total count of files in the file list is still going to increase ( and each time it does , the count of files left to check will increase by the number of the files added to the list ) . <p> -P <p> The- -P- option is equivalent to- partial- progress . Its purpose is to make it much easier to specify these two options for a long transfer that may be interrupted.There is also a- info=progress2- option that outputs statistics based on the whole transfer , rather than individual files . Use this flag without outputting a filename ( e.g. avoid- -v- or specify- info=name0- if you want to see how the transfer @ @ @ @ @ @ @ @ @ @ names . ( You do n't  need to specify the- progress- option in order to use- info=progress2 . ) <p> password-file=FILE <p> This option allows you to provide a password for accessing an rsync daemon via a file or via standard input if- FILE- is- . The file should contain just the password on the first line ( all other lines are ignored ) . Rsync will exit with an error if- FILE- is world readable or if a root-run rsync command finds a non-root-owned file.This option does not supply a password to a remote shell transport such as ssh ; to learn how to do that , consult the remote shells documentation . When accessing an rsync daemon using a remote shell as the transport , this option only comes into effect after the remote shell finishes its authentication ( i.e. if you have also specified a password in the daemons config file ) . <p> list-only <p> This option will cause the source files to be listed instead of transferred . This option is inferred if there is a single source arg and no destination specified , so @ @ @ @ @ @ @ @ @ @ a copy command that includes a destination arg into a file-listing command , or ( 2 ) to be able to specify more than one source arg ( note : be sure to include the destination ) . Caution : keep in mind that a source arg with a wild-card is expanded by the shell into multiple args , so it is never safe to try to list such an arg without using this option . For example : <p> rsync -av --list-only foo* dest/ <p> Starting with rsync 3.1.0 , the sizes output by- list-only- are affected by the- human-readable- option . By default they will contain digit separators , but higher levels of readability will output the sizes with unit suffixes . Note also that the column width for the size output has increased from 11 to 14 characters for all human-readable levels . Use- no-h- if you want just digits in the sizes , and the old column width of 11 characters . <p> Compatibility note : when requesting a remote listing of files from an rsync that is version 2.6.3 or older , you may encounter @ @ @ @ @ @ @ @ @ @ This is because a file listing implies the- dirs- option w/o- recursive , and older rsyncs do n't  have that option . To avoid this problem , either specify the- no-dirs- option ( if you do n't  need to expand a directorys content ) , or turn on recursion and exclude the content of subdirectories : - -r exclude=/*/* . <p> bwlimit=RATE <p> This option allows you to specify the maximum transfer rate for the data sent over the socket , specified in units per second . The RATE value can be suffixed with a string to indicate a size multiplier , and may be a fractional value ( e.g. " bwlimit=1.5m " ) . If no suffix is specified , the value will be assumed to be in units of 1024 bytes ( as if " K " or " KiB " had been appended ) . See the- max-size- option for a description of all the available suffixes . A value of zero specifies no limit.For backward-compatibility reasons , the rate limit will be rounded to the nearest KiB unit , so no rate smaller than 1024 bytes @ @ @ @ @ @ @ @ @ @ the socket in blocks , and this option both limits the size of the blocks that rsync writes , and tries to keep the average transfer rate at the requested limit . Some " burstiness " may be seen where rsync writes out a block of data and then sleeps to bring the average rate into compliance . <p> Due to the internal buffering of data , the- progress- option may not be an accurate reflection on how fast the data is being sent . This is because some files can show up as being rapidly sent when the data is quickly buffered , while other can show up as very slow when the flushing of the output buffer occurs . This may be fixed in a future version . <p> write-batch=FILE <p> Record a file that can later be applied to another identical destination with- read-batch . See the " BATCH MODE " section for details , and also the- only-write-batch- option . <p> only-write-batch=FILE <p> Works like- write-batch , except that no updates are made on the destination system when creating the batch . This let 's you transport @ @ @ @ @ @ @ @ @ @ and then apply the changes via- read-batch.Note that you can feel free to write the batch directly to some portable media : if this media fills to capacity before the end of the transfer , you can just apply that partial transfer to the destination and repeat the whole process to get the rest of the changes ( as long as you do n't  mind a partially updated destination system while the multi-update cycle is happening ) . <p> Also note that you only save bandwidth when pushing changes to a remote system because this allows the batched data to be diverted from the sender into the batch file without having to flow over the wire to the receiver ( when pulling , the sender is remote , and thus cant write the batch ) . <p> read-batch=FILE <p> Apply all of the changes stored in FILE , a file previously generated by- write-batch . If- FILE- is- , the batch data will be read from standard input . See the " BATCH MODE " section for details . <p> protocol=NUM <p> Force an older protocol version to be used @ @ @ @ @ @ @ @ @ @ is compatible with an older version of rsync . For instance , if rsync 2.6.4 is being used with the- write-batch- option , but rsync 2.6.3 is what will be used to run the- read-batch- option , you should use " protocol=28 " when creating the batch file to force the older protocol version to be used in the batch file ( assuming you cant upgrade the rsync on the reading system ) . <p> iconv=CONVERTSPEC <p> Rsync can convert filenames between character sets using this option . Using a CONVERTSPEC of " . " tells rsync to look up the default character-set via the locale setting . Alternately , you can fully specify what conversion to do by giving a local and a remote charset separated by a comma in the order- iconv=LOCAL , REMOTE , e.g.- iconv=utf8 , iso88591 . This order ensures that the option will stay the same whether you 're pushing or pulling files . Finally , you can specify either- no-iconv- or a CONVERTSPEC of " - " to turn off any conversion . The default setting of this option is site-specific , and can @ @ @ @ @ @ @ @ @ @ of what charset names your local iconv library supports , you can run " iconv list " . <p> If you specify the- protect-args- option ( -s ) , rsync will translate the filenames you specify on the command-line that are being sent to the remote host . See also the- files-from- option . <p> Note that rsync does not do any conversion of names in filter files ( including include/exclude files ) . It is up to you to ensure that you 're specifying matching rules that can match on both sides of the transfer . For instance , you can specify extra include/exclude rules if there are filename differences on the two sides that need to be accounted for . <p> When you pass an- iconv- option to an rsync daemon that allows it , the daemon uses the charset specified in its " charset " configuration parameter regardless of the remote charset you actually pass . Thus , you may feel free to specify just the local charset for a daemon transfer ( e.g.- iconv=utf8 ) . <p> -4 , ipv4- or- -6 , ipv6 <p> Tells rsync @ @ @ @ @ @ @ @ @ @ sockets that rsync has direct control over , such as the outgoing socket when directly contacting an rsync daemon . See also these options in the- daemon- mode section.If rsync was complied without support for IPv6 , the- ipv6- option will have no effect . The- version- output will tell you if this is the case . <p> checksum-seed=NUM <p> Set the checksum seed to the integer NUM . This 4 byte checksum seed is included in each block and MD4 file checksum calculation ( the more modern MD5 file checksums do n't  use a seed ) . By default the checksum seed is generated by the server and defaults to the current- time()- . This option is used to set a specific checksum seed , which is useful for applications that want repeatable block checksums , or in the case where the user wants a more random checksum seed . Setting NUM to 0 causes rsync to use the default of- time()- for checksum seed . <h> DAEMON OPTIONS <p> The options allowed when starting an rsync daemon are as follows : <p> daemon <p> This tells rsync that @ @ @ @ @ @ @ @ @ @ you start running may be accessed using an rsync client using the- host : : module- or- rsync : //host/module/- syntax.If standard input is a socket then rsync will assume that it is being run via inetd , otherwise it will detach from the current terminal and become a background daemon . The daemon will read the config file ( rsyncd.conf ) on each connect made by a client and respond to requests accordingly . See thersyncd.conf(5) man page for more details . <p> address <p> By default rsync will bind to the wildcard address when run as a daemon with the- daemon- option . The- address- option allows you to specify a specific IP address ( or hostname ) to bind to . This makes virtual hosting possible in conjunction with the- config- option . See also the " address " global option in the rsyncd.conf manpage . <p> bwlimit=RATE <p> This option allows you to specify the maximum transfer rate for the data the daemon sends over the socket . The client can still specify a smaller- bwlimit- value , but no larger value will be allowed . @ @ @ @ @ @ @ @ @ @ for some extra details . <p> config=FILE <p> This specifies an alternate config file than the default . This is only relevant when- daemon- is specified . The default is /etc/rsyncd.conf unless the daemon is running over a remote shell program and the remote user is not the super-user ; in that case the default is rsyncd.conf in the current directory ( typically $HOME ) . <p> -M , dparam=OVERRIDE <p> This option can be used to set a daemon-config parameter when starting up rsync in daemon mode . It is equivalent to adding the parameter at the end of the global settings prior to the first modules definition . The parameter names can be specified without spaces , if you so desire . For instance : <p> rsync --daemon -M pidfile=/path/rsync.pid <p> no-detach <p> When running as a daemon , this option instructs rsync to not detach itself and become a background process . This option is required when running as a service on Cygwin , and may also be useful when rsync is supervised by a program such as- daemontools- or AIXs- System Resource Controller. - no-detach- is @ @ @ @ @ @ @ @ @ @ This option has no effect if rsync is run from inetd or sshd . <p> port=PORT <p> This specifies an alternate TCP port number for the daemon to listen on rather than the default of 873 . See also the " port " global option in the rsyncd.conf manpage . <p> log-file=FILE <p> This option tells the rsync daemon to use the given log-file name instead of using the " log file " setting in the config file . <p> log-file-format=FORMAT <p> This option tells the rsync daemon to use the given FORMAT string instead of using the " log format " setting in the config file . It also enables " transfer logging " unless the string is empty , in which case transfer logging is turned off . <p> sockopts <p> This overrides the- socket options- setting in the rsyncd.conf file and has the same syntax . <p> -v , verbose <p> This option increases the amount of information the daemon logs during its startup phase . After the client connects , the daemons verbosity level will be controlled by the options that the client used and the @ @ @ @ @ @ @ @ @ @ . <p> -4 , ipv4- or- -6 , ipv6 <p> Tells rsync to prefer IPv4/IPv6 when creating the incoming sockets that the rsync daemon will use to listen for connections . One of these options may be required in older versions of Linux to work around an IPv6 bug in the kernel ( if you see an " address already in use " error when nothing else is using the port , try specifying- ipv6- or- ipv4- when starting the daemon ) . If rsync was complied without support for IPv6 , the- ipv6- option will have no effect . The- version- output will tell you if this is the case . <p> -h , help <p> When specified after- daemon , print a short help page describing the options available for starting an rsync daemon . <h> FILTER RULES <p> The filter rules allow for flexible selection of which files to transfer ( include ) and which files to skip ( exclude ) . The rules either directly specify include/exclude patterns or they specify a way to acquire more include/exclude patterns ( e.g. to read them from a file @ @ @ @ @ @ @ @ @ @ is built , rsync checks each name to be transferred against the list of include/exclude patterns in turn , and the first matching pattern is acted on : if it is an exclude pattern , then that file is skipped ; if it is an include pattern then that filename is not skipped ; if no matching pattern is found , then the filename is not skipped . <p> Rsync builds an ordered list of filter rules as specified on the command-line . Filter rules have the following syntax : <p> RULE PATTERNORFILENAMERULE , MODIFIERS PATTERNORFILENAME <p> You have your choice of using either short or long RULE names , as described below . If you use a short-named rule , the , separating the RULE from the MODIFIERS is optional . The PATTERN or FILENAME that follows ( when present ) must come after either a single space or an underscore ( ) . Here are the available rule prefixes : <p> exclude , - specifies an exclude pattern.include , +- specifies an include pattern.merge , . - specifies a merge-file to read for more rules.dir-merge , : - @ @ @ @ @ @ @ @ @ @ hiding files from the transfer.show , S- files that match the pattern are not hidden.protect , P- specifies a pattern for protecting files from deletion.risk , R- files that match the pattern are not protected.clear , ! - clears the current include/exclude list ( takes no arg ) <p> When rules are being read from a file , empty lines are ignored , as are comment lines that start with a " # " . <p> Note that the- include/exclude- command-line options do not allow the full range of rule parsing as described above they only allow the specification of include/exclude patterns plus a " ! " token to clear the list ( and the normal comment parsing when rules are read from a file ) . If a pattern does not begin with " - " ( dash , space ) or " + " ( plus , space ) , then the rule will be interpreted as if " + " ( for an include option ) or " - " ( for an exclude option ) were prefixed to the string . A- filter- option , on @ @ @ @ @ @ @ @ @ @ or long rule name at the start of the rule . <p> Note also that the- filter , - include , and- exclude- options take one rule/pattern each . To add multiple ones , you can repeat the options on the command-line , use the merge-file syntax of the- filter- option , or the- **26;3827;TOOLONG options . <h> INCLUDE/EXCLUDE PATTERN RULES <p> You can include and exclude files by specifying patterns using the " + " , " - " , etc. filter rules ( as introduced in the FILTER RULES section above ) . The include/exclude rules each specify a pattern that is matched against the names of the files that are going to be transferred . These patterns can take several forms : <p> if the pattern starts with a / then it is anchored to a particular spot in the hierarchy of files , otherwise it is matched against the end of the pathname . This is similar to a leading in regular expressions . Thus " /foo " would match a name of " foo " at either the " root of the transfer " ( @ @ @ @ @ @ @ @ @ @ ( for a per-directory rule ) . An unqualified " foo " would match a name of " foo " anywhere in the tree because the algorithm is applied recursively from the top down ; it behaves as if each path component gets a turn at being the end of the filename . Even the unanchored " sub/foo " would match at any point in the hierarchy where a " foo " was found within a directory named " sub " . See the section on ANCHORING INCLUDE/EXCLUDE PATTERNS for a full discussion of how to specify a pattern that matches at the root of the transfer . <p> if the pattern ends with a / then it will only match a directory , not a regular file , symlink , or device . <p> rsync chooses between doing a simple string match and wildcard matching by checking if the pattern contains one of these three wildcard characters : * , ? , and . <p> a * matches any path component , but it stops at slashes . <p> use ** to match anything , including slashes . <p> @ @ @ @ @ @ @ @ @ @ ) . <p> a introduces a character class , such as a-z or : alpha : . <p> in a wildcard pattern , a backslash can be used to escape a wildcard character , but it is matched literally when no wildcards are present . <p> if the pattern contains a / ( not counting a trailing / ) or a " ** " , then it is matched against the full pathname , including any leading directories . If the pattern does n't  contain a / or a " ** " , then it is matched only against the final component of the filename . ( Remember that the algorithm is applied recursively so " full filename " can actually be any portion of a path from the starting directory on down . ) <p> a trailing " dirname/*** " will match both the directory ( as if " dirname/ " had been specified ) and everything in the directory ( as if " dirname/** " had been specified ) . This behavior was added in version 2.6.7 . <p> Note that , when using the- recursive- ( -r @ @ @ @ @ @ @ @ @ @ every subcomponent of every path is visited from the top down , so include/exclude patterns get applied recursively to each subcomponents full name ( e.g. to include " /foo/bar/baz " the subcomponents " /foo " and " /foo/bar " must not be excluded ) . The exclude patterns actually short-circuit the directory traversal stage when rsync finds the files to send . If a pattern excludes a particular parent directory , it can render a deeper include pattern ineffectual because rsync did not descend through that excluded section of the hierarchy . This is particularly important when using a trailing * rule . For instance , this wo n't work : <p> + **39;3855;TOOLONG /file-is-included- * <p> This fails because the parent directory " some " is excluded by the * rule , so rsync never visits any of the files in the " some " or " some/path " directories . One solution is to ask for all directories in the hierarchy to be included by using a single rule : " + */ " ( put it somewhere before the " - * " rule ) , and perhaps @ @ @ @ @ @ @ @ @ @ specific include rules for all the parent dirs that need to be visited . For instance , this set of rules works fine : <p> " - /foo " would exclude a file ( or directory ) named foo in the transfer-root directory <p> " - foo/ " would exclude any directory named foo <p> " - /foo/*/bar " would exclude any file named bar which is at two levels below a directory named foo in the transfer-root directory <p> " - /foo/**/bar " would exclude any file named bar two or more levels below a directory named foo in the transfer-root directory <p> The combination of " + */ " , " + *. c " , and " - * " would include all directories and C source files but nothing else ( see also the- prune-empty-dirs- option ) <p> The combination of " + foo/ " , " + foo/bar.c " , and " - * " would include only the foo directory and foo/bar.c ( the foo directory must be explicitly included or it would be excluded by the " * " ) <p> The following @ @ @ @ @ @ @ @ @ @ - " : <p> A- /- specifies that the include/exclude rule should be matched against the absolute pathname of the current item . For example , " -/ /etc/passwd " would exclude the passwd file any time the transfer was sending files from the " /etc " directory , and " -/ subdir/foo " would always exclude " foo " when it is in a dir named " subdir " , even if " foo " is at the root of the current transfer . <p> A- ! - specifies that the include/exclude should take effect if the pattern fails to match . For instance , " - ! */ " would exclude all non-directories . <p> A- C- is used to indicate that all the global CVS-exclude rules should be inserted as excludes in place of the " -C " . No arg should follow . <p> An- s- is used to indicate that the rule applies to the sending side . When a rule affects the sending side , it prevents files from being transferred . The default is for a rule to affect both sides unless- delete-excluded- @ @ @ @ @ @ @ @ @ @ only . See also the hide ( H ) and show ( S ) rules , which are an alternate way to specify sending-side includes/excludes . <p> An- r- is used to indicate that the rule applies to the receiving side . When a rule affects the receiving side , it prevents files from being deleted . See the- s- modifier for more info . See also the protect ( P ) and risk ( R ) rules , which are an alternate way to specify receiver-side includes/excludes . <p> A- p- indicates that a rule is perishable , meaning that it is ignored in directories that are being deleted . For instance , the- -C- options default rules that exclude things like " CVS " and " *. o " are marked as perishable , and will not prevent a directory that was removed on the source from being deleted on the destination . <h> MERGE-FILE FILTER RULES <p> You can merge whole files into your filter rules by specifying either a merge ( . ) or a dir-merge ( : ) filter rule ( as introduced in the @ @ @ @ @ @ @ @ @ @ kinds of merged files single-instance ( . ) and per-directory ( : ) . A single-instance merge file is read one time , and its rules are incorporated into the filter list in the place of the " . " rule . For per-directory merge files , rsync will scan every directory that it traverses for the named file , merging its contents when the file exists into the current list of inherited rules . These per-directory rule files must be created on the sending side because it is the sending side that is being scanned for the available files to transfer . These rule files may also need to be transferred to the receiving side if you want them to affect what files do n't  get deleted ( see PER-DIRECTORY RULES AND DELETE below ) . <p> A- - specifies that the file should consist of only exclude patterns , with no other rule-parsing except for in-file comments . <p> A- +- specifies that the file should consist of only include patterns , with no other rule-parsing except for in-file comments . <p> A- C- is a way to @ @ @ @ @ @ @ @ @ @ manner . This turns on n , w , and - , but also allows the list-clearing token ( ! ) to be specified . If no filename is provided , " . cvsignore " is assumed . <p> A- e- will exclude the merge-file name from the transfer ; e.g. " dir-merge , e . rules " is like " dir-merge . rules " and " - . rules " . <p> An- n- specifies that the rules are not inherited by subdirectories . <p> A- w- specifies that the rules are word-split on whitespace instead of the normal line-splitting . This also turns off comments . Note : the space that separates the prefix from the rule is treated specially , so " - foo + bar " is parsed as two rules ( assuming that prefix-parsing was n't also disabled ) . <p> You may also specify any of the modifiers for the " + " or " - " rules ( above ) in order to have the rules that are read in from the file default to having that modifier set ( except for the- ! @ @ @ @ @ @ @ @ @ @ For instance , " merge , -/ . excl " would treat the contents of . excl as absolute-path excludes , while " dir-merge , s . filt " and " : sC " would each make all their per-directory rules apply only on the sending side . If the merge rule specifies sides to affect ( via the- s- or- r- modifier or both ) , then the rules in the file must not specify sides ( via a modifier or a rule prefix such as- hide ) . <p> Per-directory rules are inherited in all subdirectories of the directory where the merge-file was found unless the n modifier was used . Each subdirectorys rules are prefixed to the inherited per-directory rules from its parents , which gives the newest rules a higher priority than the inherited rules . The entire set of dir-merge rules are grouped together in the spot where the merge-file was specified , so it is possible to override dir-merge rules via a rule that got specified earlier in the list of global rules . When the list-clearing rule ( " ! " ) is @ @ @ @ @ @ @ @ @ @ inherited rules for the current merge file . <p> Another way to prevent a single rule from a dir-merge file from being inherited is to anchor it with a leading slash . Anchored rules in a per-directory merge-file are relative to the merge-files directory , so a pattern " /foo " would only match the file " foo " in the directory where the dir-merge filter file was found . <p> This will merge the contents of the **25;3896;TOOLONG file at the start of the list and also turns the " . rules " filename into a per-directory filter file . All rules read in prior to the start of the directory scan follow the global anchoring rules ( i.e. a leading slash matches at the root of the transfer ) . <p> If a per-directory merge-file is specified with a path that is a parent directory of the first transfer directory , rsync will scan all the parent dirs from that starting point to the transfer directory for the indicated per-directory file . For instance , here is a common filter ( see- -F ) : <p> --filter= ' @ @ @ @ @ @ @ @ @ @ scan for the file . rsync-filter in all directories from the root down through the parent directory of the transfer prior to the start of the normal directory scan of the file in the directories that are sent as a part of the transfer . ( Note : for an rsync daemon , the root is always the same as the modules " path " . ) <p> The first two commands above will look for " . rsync-filter " in " / " and " /src " before the normal scan begins looking for the file in " /src/path " and its subdirectories . The last command avoids the parent-dir scan and only looks for the " . rsync-filter " files in each directory that is a part of the transfer . <p> If you want to include the contents of a " . cvsignore " in your patterns , you should use the rule " : C " , which creates a dir-merge of the . cvsignore file , but parsed in a CVS-compatible manner . You can use this to affect where the- cvs-exclude- ( -C ) @ @ @ @ @ @ @ @ @ @ into your rules by putting the " : C " wherever you like in your filter rules . Without this , rsync would add the dir-merge rule for the . cvsignore file at the end of all your other rules ( giving it a lower priority than your command-line rules ) . For example : <p> Both of the above rsync commands are identical . Each one will merge all the per-directory . cvsignore rules in the middle of the list rather than at the end . This allows their dir-specific rules to supersede the rules that follow the : C instead of being subservient to all your rules . To affect the other CVS exclude rules ( i.e. the default list of exclusions , the contents of $HOME/.cvsignore , and the value of $CVSIGNORE ) you should omit the- -C- command-line option and instead insert a " -C " rule into your filter rules ; e.g. " filter=-C " . <h> LIST-CLEARING FILTER RULE <p> You can clear the current include/exclude list by using the " ! " filter rule ( as introduced in the FILTER RULES section above @ @ @ @ @ @ @ @ @ @ global list of rules ( if the rule is encountered while parsing the filter options ) or a set of per-directory rules ( which are inherited in their own sub-list , so a subdirectory can use this to clear out the parents rules ) . <h> ANCHORING INCLUDE/EXCLUDE PATTERNS <p> As mentioned earlier , global include/exclude patterns are anchored at the " root of the transfer " ( as opposed to per-directory patterns , which are anchored at the merge-files directory ) . If you think of the transfer as a subtree of names that are being sent from sender to receiver , the transfer-root is where the tree starts to be duplicated in the destination directory . This root governs where patterns that start with a / match . <p> Because the matching is relative to the transfer-root , changing the trailing slash on a source path or changing your use of the- relative- option affects the path you need to use in your matching ( in addition to changing how much of the file tree is duplicated on the destination host ) . The following examples demonstrate this @ @ @ @ @ @ @ @ @ @ source files , one with an absolute path of " /home/me/foo/bar " , and one with a path of " /home/you/bar/baz " . Here is how the various command choices differ for a 2-source transfer : <p> The easiest way to see what name you should filter is to just look at the output when using- verbose- and put a / in front of the name ( use the- dry-run- option if you 're not yet ready to copy any files ) . <h> PER-DIRECTORY RULES AND DELETE <p> Without a delete option , per-directory rules are only relevant on the sending side , so you can feel free to exclude the merge files themselves without affecting the transfer . To make this easy , the e modifier adds this exclude for you , as seen in these two equivalent commands : <p> However , if you want to do a delete on the receiving side AND you want some files to be excluded from being deleted , you 'll need to be sure that the receiving side knows what files to exclude . The easiest way is to include the per-directory @ @ @ @ @ @ @ @ @ @ this ensures that the receiving side gets all the same exclude rules as the sending side before it tries to delete anything : <p> rsync -avF --delete-after host:src/dir /dest <p> However , if the merge files are not a part of the transfer , you 'll need to either specify some global exclude rules ( i.e. specified on the command line ) , or you 'll need to maintain your own per-directory merge files on the receiving side . An example of the first is this ( assume that the remote . rules files exclude themselves ) : <p> In the above example the extra.rules file can affect both sides of the transfer , but ( on the sending side ) the rules are subservient to the rules merged from the . rules files because they were specified after the per-directory merge rule . <p> In one final example , the remote side is excluding the . rsync-filter files from the transfer , but we want to use our own . rsync-filter files to control what gets deleted on the receiving side . To do this we must specifically exclude the per-directory @ @ @ @ @ @ @ @ @ @ ) and then put rules into the local files to control what else should not get deleted . Like one of these commands : <h> BATCH MODE <p> Batch mode can be used to apply the same set of updates to many identical systems . Suppose one has a tree which is replicated on a number of hosts . Now suppose some changes have been made to this source tree and those changes need to be propagated to the other hosts . In order to do this using batch mode , rsync is run with the write-batch option to apply the changes made to the source tree to one of the destination trees . The write-batch option causes the rsync client to store in a " batch file " all the information needed to repeat this operation against other , identical destination trees . <p> Generating the batch file once saves having to perform the file status , checksum , and data block generation more than once when updating multiple destination trees . Multicast transport protocols can be used to transfer the batch update files in parallel to many hosts @ @ @ @ @ @ @ @ @ @ every host individually . <p> To apply the recorded changes to another destination tree , run rsync with the read-batch option , specifying the name of the same batch file , and the destination tree . Rsync updates the destination tree using the information stored in the batch file . <p> For your convenience , a script file is also created when the write-batch option is used : it will be named the same as the batch file with " . sh " appended . This script file contains a command-line suitable for updating a destination tree using the associated batch file . It can be executed using a Bourne ( or Bourne-like ) shell , optionally passing in an alternate destination tree pathname which is then used instead of the original destination path . This is useful when the destination tree path on the current host differs from the one used to create the batch file . <p> In these examples , rsync is used to update /adest/dir/ from /source/dir/ and the information to repeat this operation is stored in " foo " and " foo.sh " . The @ @ @ @ @ @ @ @ @ @ data going into the directory /bdest/dir . The differences between the two examples reveals some of the flexibility you have in how you deal with batches : <p> The first example shows that the initial copy does n't  have to be local you can push or pull data to/from a remote host using either the remote-shell syntax or rsync daemon syntax , as desired . <p> The first example uses the created " foo.sh " file to get the right rsync options when running the read-batch command on the remote host . <p> The second example reads the batch data via standard input so that the batch file does n't  need to be copied to the remote machine first . This example avoids the foo.sh script because it needed to use a modified- read-batch- option , but you could edit the script file if you wished to make use of it ( just be sure that no other option is trying to use standard input , such as the " exclude-from=- " option ) . <p> Caveats : <p> The read-batch option expects the destination tree that it is updating @ @ @ @ @ @ @ @ @ @ to create the batch update fileset . When a difference between the destination trees is encountered the update might be discarded with a warning ( if the file appears to be up-to-date already ) or the file-update may be attempted and then , if the file fails to verify , the update discarded with an error . This means that it should be safe to re-run a read-batch operation if the command got interrupted . If you wish to force the batched-update to always be attempted regardless of the files size and date , use the- -I- option ( when reading the batch ) . If an error occurs , the destination tree will probably be in a partially updated state . In that case , rsync can be used in its regular ( non-batch ) mode of operation to fix up the destination tree . <p> The rsync version used on all destinations must be at least as new as the one used to generate the batch file . Rsync will die with an error if the protocol version in the batch file is too new for the batch-reading @ @ @ @ @ @ @ @ @ @ a way to have the creating rsync generate a batch file that an older rsync can understand . ( Note that batch files changed format in version 2.6.3 , so mixing versions older than that with newer versions will not work . ) <p> When reading a batch file , rsync will force the value of certain options to match the data in the batch file if you did n't  set them to the same as the batch-writing command . Other options can ( and should ) be changed . For instance- write-batch- changes to- read-batch , - files-from- is dropped , and the- filter/include/exclude- options are not needed unless one of the- delete- options is specified . <p> The code that creates the BATCH.sh file transforms any filter/include/exclude options into a single list that is appended as a " here " document to the shell script file . An advanced user can use this to modify the exclude list if a change in what gets deleted by- delete- is desired . A normal user can ignore this detail and just use the shell script as an easy way to @ @ @ @ @ @ @ @ @ @ <p> The original batch mode in rsync was based on " rsync+ " , but the latest version uses a new implementation . <h> SYMBOLIC LINKS <p> Three basic behaviors are possible when rsync encounters a symbolic link in the source directory . <p> By default , symbolic links are not transferred at all . A message " skipping non-regular " file is emitted for any symlinks that exist . <p> If- links- is specified , then symlinks are recreated with the same target on the destination . Note that- archive- implies- links . <p> If- copy-links- is specified , then symlinks are " collapsed " by copying their referent , rather than the symlink . <p> Rsync can also distinguish " safe " and " unsafe " symbolic links . An example where this might be used is a web site mirror that wishes to ensure that the rsync module that is copied does not include symbolic links to- /etc/passwd- in the public section of the site . Using- copy-unsafe-links- will cause any links to be copied as the file they point to on the destination . Using- safe-links- @ @ @ @ @ @ @ @ @ @ Note that you must specify- links- for- safe-links- to have any effect . ) <p> Symbolic links are considered unsafe if they are absolute symlinks ( start with- / ) , empty , or if they contain enough " .. " components to ascend from the directory being copied . <p> here 's a summary of how the symlink options are interpreted . The list is in order of precedence , so if your combination of options is n't mentioned , use the first line that is a complete subset of your options : <p> copy-linksTurn all symlinks into normal files ( leaving no symlinks for any other options to affect ) . <p> links copy-unsafe-linksTurn all unsafe symlinks into files and duplicate all safe symlinks . <h> DIAGNOSTICS <p> rsync occasionally produces error messages that may seem a little cryptic . The one that seems to cause the most confusion is " protocol version mismatch is your shell clean ? " . <p> This message is usually caused by your startup scripts or remote shell facility producing unwanted garbage on the stream that rsync is using for its transport . The @ @ @ @ @ @ @ @ @ @ shell like this : <p> ssh remotehost /bin/true &gt; out.dat <p> then look at out.dat . If everything is working correctly then out.dat should be a zero length file . If you are getting the above error from rsync then you will probably find that out.dat contains some text or data . Look at the contents and try to work out what is producing it . The most common cause is incorrectly configured shell startup scripts ( such as . cshrc or . profile ) that contain output statements for non-interactive logins . <p> If you are having trouble debugging filter patterns , then try specifying the- -vv- option . At this level of verbosity rsync will show why each individual file is included or excluded . <h> EXIT VALUES <p> 0 <p> Success <p> 1 <p> Syntax or usage error <p> 2 <p> Protocol incompatibility <p> 3 <p> Errors selecting input/output files , dirs <p> 4 <p> Requested action not supported : an attempt was made to manipulate 64-bit files on a platform that can not support them ; or an option was specified that is supported by the @ @ @ @ @ @ @ @ @ @ <p> Specify a default- iconv- setting using this environment variable . ( First supported in 3.0.0 . ) <p> RSYNCPROTECTARGS <p> Specify a non-zero numeric value if you want the- protect-args- option to be enabled by default , or a zero value to make sure that it is disabled by default . ( First supported in 3.1.0 . ) <p> RSYNCRSH <p> The RSYNCRSH environment variable allows you to override the default shell used as the transport for rsync . Command line options are permitted after the command name , just as in the- -e- option . <p> RSYNCPROXY <p> The RSYNCPROXY environment variable allows you to redirect your rsync client to use a web proxy when connecting to a rsync daemon . You should set RSYNCPROXY to a hostname:port pair . <p> RSYNCPASSWORD <p> Setting RSYNCPASSWORD to the required password allows you to run authenticated rsync connections to an rsync daemon without user intervention . Note that this does not supply a password to a remote shell transport such as ssh ; to learn how to do that , consult the remote shells documentation . <p> USER- or- LOGNAME @ @ @ @ @ @ @ @ @ @ determine the default username sent to an rsync daemon . If neither is set , the username defaults to " nobody " . <p> HOME <p> The HOME environment variable is used to find the users default . cvsignore file . <h> VERSION <p> This man page is current for version 3.1.1pre1 of rsync . <h> INTERNAL OPTIONS <p> The options- server- and- sender- are used internally by rsync , and should never be typed by a user under normal circumstances . Some awareness of these options may be needed in certain scenarios , such as when setting up a login that can only run an rsync command . For instance , the support directory of the rsync distribution has an example script named rrsync ( for restricted rsync ) that can be used with a restricted ssh login . <h> CREDITS <p> rsync is distributed under the GNU General Public License . See the file COPYING for details . <p> A WEB site is available at- http : //rsync.samba.org/ . The site includes an FAQ-O-Matic which may cover questions unanswered by this manual page . 
@@45151439 @5151439/ 55329 @qwx465329 <h> Category Archive : Programming <p> Resolved : The " **37;3923;TOOLONG " task could not be loaded from the assembly I recently have been working on an ASP.Net application and at some point I started to get build errors that although they were not stopping the build and allowed me to continue with the applications execution , they were starting to get quite annoying . This <p> How to : Get the Sum of the Values from List in C#.Net ? Currently I am working on a new project in which I need to calculate some statistics that involve some basic statistics which require the total amount . The easy way out is just to do a foreach statement and iterate through the list adding <p> . Net : Supported Console Colors The days of writing applications that write to the console seem to be over for a while now . Notwithstanding , we find ourselves using the console from time to time to help us debug or quickly test application logic . In my case , I am currently using the console to monitor logging information @ @ @ @ @ @ @ @ @ @ a small programming project on the site . It has been a while since Ive done any programming , so seeing all the new technologies and cloud offerings I must confess I am as excited and fascinated as I am daunted by how much there is and how far <p> Pro Tip : Never use type double for money / financial applications When you are new into coding financial applications one of the mistakes people tend to do is using a data type that its precision / significant digits are not enough to properly represent numers . If you 've taken some CS classes you know how decimals <p> How to : Use Git ( Git Clone , Git Fetch , Git Pull , Git Clone &amp; Git Rebase ) I am new to Git and using repositories in Linux based systems ( Being a Visual Studio / Team Foundation guy is not like I am starting from scratch ) . The point was just to understand what does each command does so <p> Resolved : fatal : Not a git repository ( or any of the parent directories ) : @ @ @ @ @ @ @ @ @ @ private Git repositories which is something I was looking for and GitHub obviously charges for that . I am going to use my repository for something small and simple not a huge project so justifying <p> How to : Use Regex to create instructions out of a list of inputs Using Notepad++ : Use Regex to identify the inputs you want to put inside a command : ( bd1,3.d1,3.d1,3.d1,3b ) Then Replace it with the command identifying the matched Regex on the search : &lt;add ipAddress= " 1 ? allowed= " false " /&gt; <p> After writing code using entity framework for sometime and seeing the code base continue to grow I realized the importance of coding with performance in mind . There are things that come to mind like eager loading , but that alone wont do much as your application continues to grow . Below are some key best practices I <p> Recently I came across the need to convert a string stored in the database into an enumeration in our application . I was n't really excited about having to write a switch statement nor having to maintain @ @ @ @ @ @ @ @ @ @ Net that allows you to parse that string into any enumeration which 
@@45151442 @5151442/ <p> How to : Create an SSH connection using Terminal on Mac OS X &amp; save the configuration for later use / shortcut So now that I bought myself a new Mac I decided I would try to avoid by all means installing Windows on it . I use a lot of applications ( and some games ) that either <p> What is a privileged port on a Mac / Darwin ? Recently I was trying to connect to a remote SSH server using my MacBook . At one point when trying to configure a Tunnel I got an error saying : Privileged ports can only be forwarded by root My first instinct was to make sure I had <p> Why does my Mac OSX say " update needed " on every boot ? If you recently reinstalled your Mac with OS X you might notice that every time you reboot when you try to login instead of showing the username of the first user account- created it shows an icon with the text " Update Needed " . You can of <p> Sometimes you want to clear your @ @ @ @ @ @ @ @ @ @ you type a password and now it is being remembered by your terminal session . Ive read this information is stored in your profile as a History file , and many people go as far as suggesting you should delete it . However , <p> Sometimes you need to run certain commands on the terminal with elevated credentials otherwise you wont be able to make modifications to the operating system , etc . In order to run such a command you need to use sudo to elevate your current session . For example , you can run " sudo -s " and that will ask for <p> How to : Clear or Flush DNS cache on a Mac OS X computer Most operating systems- implement a local DNS cache for a number of reasons : lower the load on DNS servers , faster domain name resolution , etc . This is great for a number of reasons , but it also poses a problem from time to time . Most <p> How to : Take a screenshot on a Mac ( either to the clipboard or to a file on @ @ @ @ @ @ @ @ @ @ the need to share a screenshot of something on the screen . If it is not to show someone else something on our screen it could be to capture information <p> Apparently after some research there is no way to directly print your screen on a Mac . What I recommend is simply taking a screenshot and saving it to the desktop and then printing that document . You can learn how to take a screenshot on this article . 
@@45151443 @5151443/ <h> Delta : Enjoy BusinessElite This Summer For Half The Miles ( 50% off an upgrade 7.5k miles vs 15k miles ) <p> So Delta just came out with an offer that could be great if you 're planning a flight internationally . It is definitively worth the extra 15k miles to get an upgrade when taking a long flight , but one thing to keep in mind as with any promotion is the fine print . If you are planning to take advantage of this promotion make sure you buy your economy class ticket using the proper fare otherwise this wont apply . Unfortunately I think this is the same as when you try to use those regional upgrades you get when you become platinum : It pretty much as to be a full fare economy seat which costs usually about 2 to 3 times what the cheapest most restrictive fare would so there you have it . <p> Below is a copy of the email with the promotional code good until the end of August this year . Enjoy ! <p> This summer , upgrade to BusinessElite for @ @ @ @ @ @ @ @ @ @ the most indulgent ones yet . For a limited time , - get a 50% mileage discount toward a BusinessElite- upgrade.For example , a flight from New York to Madrid , Spain , would normally require 15,000 miles for a one-way Upgrade Award from full fare Economy Class ( Y fare class ) to BusinessElite . With this offers 50% discount , you would need only 7,500 miles to upgrade one-way to BusinessElite from full-fare Economy Class.To qualify , purchase an Economy Class ticket in Y , B or M class of service on select transoceanic international flights . This 50% discount on mileage upgrades is also available on select international Business Class service to Mexico , the Caribbean , and South and Central America.When you fly BusinessElite on our transoceanic flights , you 'll enjoy exclusive amenities , including : <p> A state-of-the-art entertainment system with movies , HBO- , TV , music , and games <p> Availability for these premium seats is limited , so hurry ! - Call Delta Reservations at 1-800-323-2323 to get your BusinessElite upgrade for travel through August 31 , 2011 " and use your @ @ @ @ @ @ @ @ @ @ SkyMiles number and PIN when prompted.Learn More- <p> Terms &amp; ConditionsEligible Fares/Booking : - All taxes , fees and blackout dates are governed by the rules of the Y , B or M Economy Class fare purchased . Additional upgrade tax of up to $200.00 may apply for travel departing the United Kingdom , France or Ghana . SkyMiles members can request a one-way Upgrade Award for 50% OFF for paid tickets purchased in Y , B , or M Economy Class between the continental United States , Alaska , Canada , Mexico , and any international destination ( including the Caribbean ) that offers either J class fares ( BusinessElite ) or Business Class where upgrade class of service is available on Delta-operated flights only . SkyMiles members must call a Delta reservations representative for upgrades . <p> Tickets : - Must be purchased and upgrade requested no later than August 31 , 2011 . <p> Travel Period : - Travel must be completed by August 31 , 2011 . <p> Restrictions : - Availability of one-way upgrade inventory is limited and may not be available on all flights @ @ @ @ @ @ @ @ @ @ Members may reissue existing tickets to be eligible for upgrade offer , but will need to pay applicable fees . Customers may combine this Upgrade Award with other one-way Upgrade Awards . Upgrades not available on Air France and KLM or any other SkyTeam- or codeshare partner operated flights . Tickets are nontransferable . SkyMiles accrual will be for class of service originally purchased . 
@@45151445 @5151445/ 55329 @qwx465329 <h> Category Archive : Technologies <p> How to : Create a php file to check on the status of a Drupal Server I was working on creating a php status file that would indicate if a server was able to handle client requests or not . The idea is that you can have a loadbalancer check periodically those status.php files to see if <p> Resolve : Failed to establish a connection with host : the credentials supplied to the package were not recognized ( 0x8009030D ) . When working with Hyper-V you might have the need to transfer a Virtual Machine from one host to another but might run into the following error message when trying : Failed to establish a connection with host TARGETVMHOST ' : <p> How to : Not serve cached pages to logged in users using Varnish Generally speaking caching is a good thing , it takes a bit load off the web servers and serves the content more quickly to our site visitors . However , there are a few scenarios where caching is not that great : A highly dynamic site @ @ @ @ @ @ @ @ @ @ of WordPress running in IIS To be entirely honest I am still struggling with this topic . However I have identified a few points that should help improve the performance / speed of a WordPress site running in IIS . Now , if a distinction should be made it would be <p> How to : Configure Central Certificate Store ( CCS ) with IIS 8 ( Windows Server 2012 ) There are a few things you might run across as you try to develop a web farm that has to share SSL Certificates , one of them is the Central Certificate Store . What is CCS ? Central Certificate Store or Centralized SSL Certificate Support <p> How to : Resolve error " The Module DLL **35;3962;TOOLONG failed to load . - The data is the error . " This error might be caused for a number of reasons , but generally speaking the resolution is pretty simple . LogCust.Dll is a library used to log custom errors . There are a few IIS components like " Custom Logs " that depend on <p> How to : Resolve error " Faulting @ @ @ @ @ @ @ @ @ @ , Unable to create log file " To save people some time the error " Faulting application name : w3wp.exe " refers to an issue with IIS , while the error " Faulting module name : ntdll.dll " is incredibly generic it could be so many things , so you really need to <p> How to : Get full metrics including Bandwidth and/or Referral Information on SmarterStats By default IIS 8.5 does not provide on its log file enough information for SmarterStats to calculate bandwidth utilization and Referral information . However , enabling the logs to include this information is fairly simple , the hardest thing is knowing why you were not getting <p> How to : Remotely manage a Hyper-V Server / Enable Firewall exceptions Unfortunately one of the things you will find out when you install a Hyper-V Server 2012 is that it is completely locked down . You are provided- with a very neat utility that allows you to easily perform common tasks like enabling remote management , configuring your <p> How to : Configure Hyper-V Replica using certificate-based authentication ( https ) Disclaimer : Before you @ @ @ @ @ @ @ @ @ @ able to do this for a Hyper-V Core Windows Server- Installation ( the free Hyper-V version you download off of Microsoft ) . I get the certificate and everything to show up but last-minute- it throws and error 
@@45151446 @5151446/ 55329 @qwx465329 <h> Tag Archive : Domain name <p> How to : Configure a Dynamic DNS Client ( DDClient ) with NameCheap ? Recently I came across a new set of DNS service providers that offer Dynamic DNS services ( for free ! ) One that I ended up really liking was NameCheap . They have a nice user interface , and what 's best is that you can use your own domain name <p> What is : http : //ocsp.msocsp.com ? MS-OCSP stands for " Online Certificate Status Protocol ( OCSP ) Extensions " from Microsoft . Although this might seem a bit daunting the plain english version of that is n't : " Microsoft publishes Open Specifications documentation for protocols , file formats , languages , standards as well as overviews of the interaction among each of these technologies . " This is a protocol <p> How to : Set up multiple Administrator accounts in Microsofts Live Domains Sometimes you want to distribute the administrative tasks- of maintaining a system across a number of individuals to- provide- business continuity in- case you are not available . Fortunately if you @ @ @ @ @ @ @ @ @ @ manage your domain ( create accounts , etc . ) Obviously each <p> Changes with- NginX- 1.5.9 Below are the list of changes from the 1.5.8 release of NginX to version 1.5.9 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.9 22 Jan 2014 * ) Change : now nginx expects escaped URIs in " X-Accel-Redirect " headers. * ) Feature : the " sslbuffersize " directive . * ) Feature : the " limitrate " directive <p> How to : Enable proper domain DNS search in Ubuntu when using Windows Azure So as part of the latest using your own DNS server with Linux machines inside Windows Azure I had a bit of a problem . Even though my corporate DNS server was set up correctly , DNS entries were correct , etc . Ubuntu would just <p> How to : Redirect users to a given site when the visited sub-domain does not exist in a WordPress Multisite deployment Now , this is a tricky one and I say that because I searched and could n't find the answer @ @ @ @ @ @ @ @ @ @ need to word my search better . Anyway , for <p> How to : Resolve error " Computer/Name Domain Changes . The following error occurred attempting to join the domain : The requested resource is in use . " One of my latest missteps was not completing the migration of all servers to the new domain name when I performed an Active Directory Domain Name change . Well , one of the unintended consequences <p> How to : Rename an Active Directory Domain Name There are many reasons why you might want to rename an Active Directory Domain Name . Microsoft names situations were there is a reorganization of the enterprise which could simply be internal or due to an adquisition , etc . In my case I have been reading about how to <p> How to map your WordPress blog to multiple domains ? / WordPress multi domain default site After looking around several blogs I found many interesting ways in order to achieve this behavior . Generally what happens is that you own your primary domain say http : //technology.bauzas.com but decide you want to purchase the same domain with a . com.mx 
@@45151447 @5151447/ <p> When working with SharePoint 2013 and Sky Drive Pro I was transferring some files from our OnPremise server to Microsofts Office 365 site when I came across error - 0x800700DF : The File Size Exceeds the Limit Allowed and Can not be Saved . Generally speaking this is due to any of the following reasons : <p> The upload quota size on your SharePoint site . <p> The almost 50mb file upload limit that comes by default on Windows . <p> To be safe always check #1 . As far as I know Office 365 has a 2gb file upload limit last I heard so that is not the issue in my case . The second scenario happens when you try to exceed the default limit of 50000000 bytes ( 47.68 Mb ) . In order to avoid this issue you have to manually increase the size in the Windows registry to the maximum allowed ( I hear WebDav allows 4gb maximum which happens to be the 32bit limit of the field or- 4294967296 bytes ) . Below are some simple steps you can follow to set the @ @ @ @ @ @ @ @ @ @ by using the regedit command ( Control + R or type regedit when launching the start menu in Windows 8 ) . <p> Browse to <p> HKEYLOCALMACHINE <p> SYSTEM <p> CurrentControlSet <p> Services <p> WebClient <p> Parameters <p> Right click on the FileSizeLimitInBytes and click " Modify " . <p> Click on Decimal , and type 4294967295 ; then- click " Ok " to save . We are using one byte less as- 4294967296 exceeds the 32bit size of the field by 1 ( remember 0 counts ) . If you prefer you can type 0xffffffff for the hexadecimal value . 
@@45151448 @5151448/ 55329 @qwx465329 <h> How to : Install Zend OpCache for php and WordPress <h> How to : Install Zend OpCache for php and WordPress <p> OpCache is a mayor tool to increase the performance of your site . Every time a WordPress page is visited the system needs to compile the php code in order to render the page . OpCache allows you to store those compiled bits in memory and serve them much faster . If used correctly Zend OpCache would identify a change in the physical file and reload the compiled code into the memory cache . I read that it can increase your speed almost 10 fold ! In my case I did n't  notice much of a difference but then again maybe my server is just that good = But seriously speaking this is a great feature , the server is showing- 99.84% cache hit which is really good . <p> Another great point is that Zend OpCache is included with php starting with version 5.5 ! Because of that there is no need to install it just activate it and start enjoying the benefits @ @ @ @ @ @ @ @ @ @ memory for interned strings in **38;3999;TOOLONG <p> ; The maximum number of keys ( scripts ) in the OPcache hash table . ; Only numbers between 200 and 100000 are **41;4039;TOOLONG <p> ; The maximum percentage of " wasted " memory until a restart is **40;4082;TOOLONG <p> ; When this directive is enabled , the OPcache appends the current working ; directory to the script key , thus eliminating possible collisions between ; files with the same name ( basename ) . Disabling the directive improves ; performance , but may break existing applications . ; opcache.usecwd=1 <p> ; When disabled , you must reset the OPcache manually or restart the ; webserver for changes to the filesystem to take effect . **28;4124;TOOLONG <p> ; How often ( in seconds ) to check file timestamps for changes to the shared ; memory storage allocation . ( " 1 " means validate once per second , but only ; once per request . " 0 " means always validate ) opcache.revalidatefreq=0 <p> ; If disabled , all PHPDoc comments are dropped from the code to reduce the ; size of the @ @ @ @ @ @ @ @ @ @ PHPDoc comments are not loaded from SHM , so " Doc Comments " ; may be always stored ( savecomments=1 ) , but not loaded by applications ; that do n't  need them anyway . ; opcache.loadcomments=1 <p> ; If enabled , a fast shutdown sequence is used for the accelerated code opcache.fastshutdown=1 <p> ; A bitmask , where each bit enables or disables the appropriate OPcache ; passes ; **36;4154;TOOLONG <p> ; opcache.inheritedhack=1 ; opcache.dupsfix=0 <p> ; The location of the OPcache blacklist file ( wildcards allowed ) . ; Each OPcache blacklist file is a text file that holds the names of files ; that should not be accelerated . The file format is to add each filename ; to a new line . The filename may be a full path or just a file prefix ; ( i.e. , /var/www/x blacklists all the files and directories in /var/www ; that start with x ) . Line starting with a ; are ignored ( comments ) . ; **26;4192;TOOLONG <p> ; Allows exclusion of large files from being cached . By default all files ; are cached. ; @ @ @ @ @ @ @ @ @ @ . ; The default value of " 0 " means that the checks are disabled . ; **27;4220;TOOLONG <p> ; How long to wait ( in seconds ) for a scheduled restart to begin if the cache ; is not being **40;4249;TOOLONG <h> II . Web Viewer <p> I am not sure why but I am crazy about graphs , reports , statistics , etc . Seeing how effective your cache is and the management of system resources is something important . There are several OpCache monitors but I think OpCache-Status by Rasmuf Lerdorf is rather simple to deploy , simple and easy on the eyes . <h> 2 comments <p> Well , if your php configuration has it then there is nothing else you need to do on WordPress . You can try something like https : **36;4291;TOOLONG to see the status of your opcache . If you have n't configured it on your php installation you can do so this way : 
@@45151449 @5151449/ 55329 @qwx465329 <h> Category Archive : Ubuntu <p> Packages are manually installed via the dpkg command ( Debian Package Management System ) . dpkg is the backend to commands like apt-get and aptitude , which in turn are the backend for GUI install apps like the Software Center and Synaptic . Something along the lines of : dpkg &gt; apt-get , aptitude &gt; Synaptic , Software Center But of course the <p> How to : Assign multiple IP addresses to one interface in Ubuntu using the Command Line Interface ( CLI ) ? I have been working with Ubuntu more lately and ran into the need to direct traffic going to one server ( via IP ) to go to a new server but I could n't change the clients configuration . Because part of <p> Resolved : " ( initramfs ) unable to find a medium containing a live file system " while installing Ubuntu from a USB device Like I mentioned in a previous post I am working on installing an Ubuntu machine in order to do some GPU mining ( this time around Ethereum ) and now @ @ @ @ @ @ @ @ @ @ Resolved : USB keyboard stops working on Ubuntu installation at language selection I am currently working on setting up an ethereum mining rig and I wanted to use Ubuntu as my operating system ( no need to pay for a windows license . ) However , as I was installing Ubuntu 14.0.4 LTS on my new computer I ran into <p> How to : Launch a Process that Persists even if you Disconnect your SSH Terminal I 'm not sure if this has happened to you before , but there are times when you have an application you wish to launch but if you disconnect from the terminal ( SSH or physical ) that process gets terminated . For example , currently I <p> How to : Erase a log file in Ubuntu Ever had this huge error log , full of nightmares and bad memories ? Well , once you 're done fixing the problems then you 're stuck with megs if not gigs worth of bad memories that perhaps you wish to get rid off . Well , in my case one of my errors <p> How to : Create a @ @ @ @ @ @ @ @ @ @ phases of a Web Server deployment we are in need of testing secure communications ( and its configuration ) using a certificate . Unfortunately we might not have a valid certificate from a certification authority at the time . There are also other scenarios were you <p> How to : Create directory in /var/run/ at start-up in Ubuntu As you probably have already noticed , the /var/run directory is temporary storage used by your Ubuntu system . It is mapped to your RAM disk ( Ubuntu uses I think about 10% of your RAM and creates a disk mount for it . This is used for folders <p> How to : Make a directory structure in Ubuntu only if the directories do not exist Lately I have been working on start up scripts but because I use temporary storage for some mounts the information contained within them is cleared with each reboot ( shutdown / turn on Windows Azure ) . This means that the folder 
@@45151454 @5151454/ 55329 @qwx465329 <h> Tag Archive : Nginx <p> How to : Override a Location directive on NginX Sometimes when coding you are in need to re-use a lot of the logic across different systems but find yourself needing to overwrite some of that functionality on a special case . In this particular scenario I needed to overwrite a Location directive on NginX . What do I <p> How to : Move your NginX website to HTTPs- / SSL It comes at no surprise that a lot of people are looking into moving their sites to HTTPs due to recent events : Googles decision to give ranking points to sites that use SSL / HTTPs and eavesdropping by governments world wide . There are a number <p> How to : Create a Self Signed Certificate in Ubuntu Many- times during the initial phases of a Web Server deployment we are in need of testing secure communications ( and its configuration ) using a certificate . Unfortunately we might not have a valid certificate from a certification authority at the time . There are also other scenarios were you <p> Resolved @ @ @ @ @ @ @ @ @ @ late I have been a bit busy so keeping up with updates to the web server has been pretty much neglected . Because of that I decided to switch to the nginx.org supported distribution to get the latest updates although that means a distribution without any <p> Changes with- NginX- 1.4.7 Below are the list of changes from the 1.4.6 release of NginX to version 1.4.7 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update addresses a bug fix and a security issue . Because of the security issue we rate this update of high priority . Changes with nginx 1.4.7 18 Mar <p> Changes with- NginX- 1.4.6 Below are the list of changes from the 1.4.5 release of NginX to version 1.4.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update mainly provides- bug-fixes which are not critical . Changes with nginx 1.4.6 04 Mar 2014 * ) Bugfix : the " clientmaxbodysize " directive might not work when reading a request body <p> Changes with- NginX- 1.6 Below are the @ @ @ @ @ @ @ @ @ @ version 1.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update makes the development / mainline version 1.5.13 part of the stable branch. - Because of this several new features are included and we highly suggest people upgrade to <p> Changes with- NginX- 1.7 Below are the list of changes from the 1.5.13 release of NginX to version 1.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several new Features- so updating to this version is not considered urgent/critical . Changes with nginx 1.7.0 24 Apr 2014 * ) Feature : backend SSL certificate verification . * ) <p> How to : Improve SSL performance on NginX You would be surprised but a lot of people face SSL performance issues when using NginX . I recently deployed SPDY over SSL for my sites and came to realize that SPDY was in fact much slower than using standard HTTP . I proceeded to leave SSL alone and see <p> Resolved : Ubuntu php5-fpm throws error " unknown instance " on @ @ @ @ @ @ @ @ @ @ turns out there is a known issue that when you perform a service reload on php5-fpm you might encounter yourself with a bit of a problem . The use of reload is meant to result in a reloading of 
@@45151455 @5151455/ 55329 @qwx465329 <h> Tag Archive : Ubuntu ( operating system ) <p> Packages are manually installed via the dpkg command ( Debian Package Management System ) . dpkg is the backend to commands like apt-get and aptitude , which in turn are the backend for GUI install apps like the Software Center and Synaptic . Something along the lines of : dpkg &gt; apt-get , aptitude &gt; Synaptic , Software Center But of course the <p> Resolved : " ( initramfs ) unable to find a medium containing a live file system " while installing Ubuntu from a USB device Like I mentioned in a previous post I am working on installing an Ubuntu machine in order to do some GPU mining ( this time around Ethereum ) and now I am running into more issues . When booting <p> How to : Erase a log file in Ubuntu Ever had this huge error log , full of nightmares and bad memories ? Well , once you 're done fixing the problems then you 're stuck with megs if not gigs worth of bad memories that perhaps you wish to get @ @ @ @ @ @ @ @ @ @ my errors <p> Ubuntu 14.04 and above in a Generation 2 Hyper-V Virtual Machine ( VM ) As most of you know , a Generation 2 Hyper-V Virtual Machine is generally reserved for Windows 2012 or 64 bit versions of Windows 8 as the New virtual Machine Wizard specifies : Generation 2 This virtual machine generation provides support for features such as 
@@45151456 @5151456/ <p> The Windows Time service provides time synchronization to peers and clients , which ensures consistent time throughout an enterprise . I-ve been struggling with getting my Domain to not end up with strange times . The issue at hand is that my Domain Controller is a Virtual Machine and I am guessing the host does not do a good job keeping track of the time . You can imagine how this could end up . Fortunately its usually just a few seconds and soon enough the entire domain is behind the rest of the world but consistently behind . In order to avoid this I researched how to configure Windows Server to use the Network Time Protocol to query external servers . <p> By default , the first domain controller that you deploy holds the primary domain controller ( PDC ) emulator operations master role . Set the PDC emulator to synchronize with a valid Network Time Protocol ( NTP ) source . If you have not configured a source , the Windows Time service logs a message to the event log , and then uses the local @ @ @ @ @ @ @ @ @ @ the Windows Time service to synchronize with an external time source . External time sources allow users to synchronize computer clocks through the NTP protocol over an IPv4 or IPv6 network . <p> The Microsoft time server ( time.windows.com ) uses NIST , the National Institute of Standards and Technology , located in Boulder , Colorado , as its external time provider . NIST provides the Automated Computer Time Service ( ACTS ) , which can set a computer clock with an uncertainty of less than 10 milliseconds . The U.S. Naval Observatory ( USNO ) Time Service Department in Washington , D.C. , is another source for accurate time synchronization in the United States . Many other sites exist throughout the world that you can use for time synchronization . <p> Note <p> Because synchronization with an external time source is not authenticated , it is less secure . <p> To configure the Windows Time service on the first forest root domain controller <p> Log on to the first domain controller that you deployed . <p> At a command prompt , type the following command ( where &lt;target&gt; is @ @ @ @ @ @ @ @ @ @ is and &lt;number&gt; determines the number of samples ( or comparisons ) it is going to make . I would say 10 would suffice to get an idea ) , and then press ENTER:w32tm /stripchart /computer : &lt;target&gt; /samples : &lt;number&gt; /dataonly <p> After you are done configuring the peer list you can start the sync process manually by typing w32tm /config /update , which indicates the OS that you 've made changes and that they are ready . I am not entirely sure but this results in a slow sync , while the w32tm /resync command forces it to re-synchronize immediately . So in the first scenario it will move about 1/3 of a second every second towards the peers time while in the other case it will jump straight to it . <p> Parameter <p> Description <p> W32tm /stripchart <p> Displays a strip chart of the offset between synchronizing computers . <p> W32tm /config /update <p> Configures the PDC emulator . <p> /computer : &lt;target&gt; <p> Specifies the Domain Name System ( DNS ) name or IP address of the NTP server whose time you want to compare to @ @ @ @ @ @ @ @ @ @ server is time.windows.com . <p> /samples : &lt;number&gt; <p> Specifies the number of time samples that the target computer returns . <p> /dataonly <p> Specifies that results show only data , not graphics . <p> /manualpeerlist : &lt;peers&gt; <p> Specifies the list of DNS names or IP addresses for the NTP time source with which the PDC emulator synchronizes . ( This list is referred to as the manual peer list . ) For example , you can specify time.windows.com as the NTP time server . When you specify multiple peers , use a space as the delimiter and enclose the names of the peers in quotation marks . <p> /syncfromflags:manual <p> Specifies to synchronize time with peers in the manual peer list . <p> /reliable:yes <p> Specifies that the computer is a reliable time service . <p> Note <p> When you specify a peer that is in the manual peer list , do not use the DNS name or IP address of a computer that uses the forest root domain controller as its source for time , such as another domain controller in the forest . The time service @ @ @ @ @ @ @ @ @ @ time source configuration . <h> W32tm <p> You can use the W32tm.exe tool to configure Windows Time service ( W32time ) settings.You can also use W32tm.exe to diagnose problems with the time service.W32tm.exe is the preferred command-line tool for configuring , monitoring , or troubleshooting the Windows Time service.For examples of how you can use this command , see Examples . <h> Syntax <p> W32tm &lt;/parameter&gt; &lt;/param2&gt; <h> Parameters <p> Parameter <p> Description <p> W32tm / ? <p> W32tm command-line Help <p> W32tm /register <p> Registers the time service to run as a service , and adds default configuration to the registry . <p> W32tm /unregister <p> Unregisters the time service , and removes all configuration information from the registry . <p> Domain " Specifies which domain to monitor.If no domain name is specified , or neither the domain nor computers option is specified , the default domain is used.This option might be used more than once . <p> computers " Monitors the given list of computers.Computer names are separated by commas , with no spaces.If a name has a prefix of a * ' , it is treated as @ @ @ @ @ @ @ @ @ @ use this option more than once . <p> threads " Specifies the number of computers to analyze simultaneously.The default value is 3 . The allowed range is 1 through 50 . <p> w32tm /ntte &lt;NT time epoch&gt; <p> Converts a Windows- NT system time , in ( 10-7 ) s intervals from 0h 1-Jan 1601 , into a readable format . <p> w32tm /ntpte &lt;NTP time epoch&gt; <p> Converts a Network Time Protocol ( NTP ) time , in ( 2-32 ) s intervals from 0h 1-Jan 1900 , into a readable format . <p> w32tm /resync <p> /computer : &lt;computer&gt; <p> /nowait <p> /rediscover <p> /soft <p> Tells a computer that it should resynchronize its clock as soon as possible , throwing out all accumulated error statistics . <p> computer : &lt;computer&gt; " Specifies the computer that should resynchronize.If a computer is not specified , the local computer will resynchronize . <p> nowait " Do not wait for the resynchronization to occur ; return immediately.Otherwise , wait for the resynchronization to complete before returning . <p> soft " This option is only provided for compatibility with older time servers @ @ @ @ @ @ @ @ @ @ /stripchart <p> /computer : &lt;target&gt; <p> /period : &lt;refresh&gt; <p> /dataonly <p> /samples : &lt;count&gt; <p> /packetinfo <p> ipprotocol : &lt;46&gt; <p> Displays a strip chart of the offset between this computer and another computer . <p> computer : &lt;target&gt; " The computer to measure the offset against . <p> period : &lt;refresh&gt; " The time between samples , in seconds.The default value is 2 seconds . <p> Dataonly " Display only the data , without graphics . <p> samples : &lt;count&gt; " Collect &lt;count&gt; samples ; then , stop.If a value is not specified , samples will be collected until the user types Ctrl+C is pressed . <p> packetinfo " Print out NTP packet response message . <p> Ipprotocol " Specify the IP protocol to use.The default is to use whatever is available . <p> w32tm /config <p> /computer : &lt;target&gt; <p> /update <p> /manualpeerlist : &lt;peers&gt; <p> /syncfromflags : &lt;source&gt; <p> /LocalClockDispersion : &lt;seconds&gt; <p> /reliable : ( YESNO ) <p> /largephaseoffset : &lt;milliseconds&gt; <p> computer : &lt;target&gt; " Adjusts the configuration of &lt;target&gt;.If a value is not specified , the default is the local computer . <p> @ @ @ @ @ @ @ @ @ @ changed , causing the changes to take effect . <p> manualpeerlist : &lt;peers&gt; " Sets the manual peer list to &lt;peers&gt; , which is a space-delimited list of Domain Name System ( DNS ) and/or IP addresses.When you are specifying multiple peers , this option must be enclosed in quotation marks ( ) . <p> syncfromflags : &lt;source&gt; " Sets what sources the NTP client should synchronize from . &lt;source&gt; should be a comma-separated list of these keywords ( not case sensitive ) : <p> MANUAL " Include peers from the manual peer list . <p> DOMHIER " Synchronize from a domain controller in the domain hierarchy . <p> NO " Do not synchronize from any server . <p> ALL " Synchronize from both manual and domain peers . <p> LONG ... the accuracy of the internal clock that W32time will assume when it can not acquire time from its configured sources . <p> reliable : ( YESNO ) " Set whether this computer is a reliable time source . <p> This setting is meaningful only on domain controllers . <p> YES " This computer is a reliable time service @ @ @ @ @ @ @ @ @ @ time service . <p> largephaseoffset : &lt;milliseconds&gt; " Sets the time difference between local time and network time that W32time will consider to be a spike . <p> w32tm /tz <p> Displays the current time zone settings . <p> w32tm /dumpreg <p> /subkey : &lt;key&gt; <p> /computer : &lt;target&gt; <p> Displays the values that are associated with a given registry key . <p> The default key is LONG ... <p> ( the root key for the time service ) . <p> subkey : &lt;key&gt; " Displays the values that are associated with subkey &lt;key&gt; of the default key . <p> This parameter was first made available in the Windows Time client versions of Windows- Vista and Windows Server- 2008 . <p> Enables or disables local computer Windows Time service private log . <p> disable " Disable the private log . <p> enable " Enable the private log . <p> file : &lt;name&gt; " Specify the absolute file name . <p> size : &lt;bytes&gt; " Specify the maximum size for circular logging . <p> entries : &lt;value&gt; " Contains a list of flags , specified by number and separated by commas @ @ @ @ @ @ @ @ @ @ logged.Valid numbers are 0 to 300 . A range of numbers is valid , in addition to single numbers , such as 0 through 100,103,106 . Value 0-300 is for logging all information . <p> truncate " Truncate the file if it exists . <h> Remarks <p> The Windows Time service is not a full-featured NTP solution that meets time-sensitive application needs , and it is not supported by Microsoft as such.For more information , seearticle 939322 in the Microsoft Knowledge Base LONG ... <p> If you have questions about the Windows Time service , please post them to the Directory Services forum LONG ... <h> Examples <p> If you want to set the local Windows Time client to point to two different time servers , one named ntpserver.contoso.com and another named clock.adatum.com , type the following command at the command line , and then press ENTER : 
@@45151457 @5151457/ <h> Travel <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <p> Jiuzhai Valley National Park , China Photo by : Richard Janecki Blue and green lakes as well as waterfalls dot Jiuzhai Valley National Park in southwestern China north of Chengdu . The area was declared a UNESCO World Heritage Site in 1992 , in part for the natural beauty and in part for its endangered plant and animal species . <p> Las Pozas , Xilitla , Mexico World Monuments Fund Las Pozas , which means " the pools " in Spanish , is a collection of surrealist structures created by English aristocrat Edward James . Born into wealth , James left his English mansion to create a fantasyland amid central Mexicos jungle . The 20 acres also include a staircase to nowhere . <p> United Airlines : Update Priority This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it @ @ @ @ @ @ @ @ @ @ a <p> The Q1 promotion for Hilton " More Points " has been announced and will continue through 3/31/12 . Details are provided below : Promotion period : 1/1/12-3/31/12 ; eligible nights are within these dates only ( ie , a stay 3/30-4/1 would only earn bonus points for 3/30-31 ) You must register prior to your hotel stays to be eligible Most Hilton <p> LONG ... Attached is the PDF containing the list of international phone numbers you can use to call the 1k United line. - intlcallsupport1Konly <h> Jiuzhai Valley National Park , China <p> Photo by : Richard Janecki <p> Blue and green lakes as well as waterfalls dot Jiuzhai Valley National Park in southwestern China north of Chengdu . The area was declared a UNESCO World Heritage Site in 1992 , in part for the natural beauty and in part for its endangered plant and animal species . <h> Las Pozas , Xilitla , Mexico <h> World Monuments Fund <p> Las Pozas , which means " the pools " in Spanish , is a collection of surrealist structures created by English aristocrat Edward James . Born @ @ @ @ @ @ @ @ @ @ a fantasyland amid central Mexicos jungle . The 20 acres also include a staircase to nowhere . <h> United Airlines : Update Priority <p> This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it ca n't be secured at the time it is applied to a reservation , it gives you preference on the waitlist over others who are doing the standard complimentary upgrade . I would advice checking with- United as they change their policies from time to time . <p> Upgrade priority . <p> Upgrade waitlists will be prioritized by status , fare class and time of request . Starting at five days ( 120 hours ) before departure , select flights will also be eligible for Complimentary Premier Upgrades . The confirmation windows will be : <p> Just a couple days go I received an email for a promotion with AA . Its a pretty sweet deal and its great for anyone trying to secure status @ @ @ @ @ @ @ @ @ @ year . Sadly I 've only flown American once this year so this does n't  help me much . <p> Qualify For AAdvantage-- Elite Status Faster <p> Dear &lt;&lt;&gt;&gt; , <p> As my last act as outgoing president of the AAdvantage-- program , and as a great introduction to the new president Suzanne Rubin , we decided to give you what you 've been asking for ( yes , I heard you already ! Now , drumroll , please ) Double Elite Qualifying Miles for travel worldwide on American Airlines ! <p> This email was sent to- because you subscribe to AAirmailSM- or AAdvantage-- eSummarySM <p> Double Elite-Qualifying Miles Offer Terms and Conditions : AAdvantage-- promotion is valid for all travel on purchased , published fare tickets on American Airlines , American- Eagle-- or the AmericanConnection-- carrier marketed and operated flights from December 13 , 2011 , through January 31 , 2012 . This offer does not require travel to be booked during a specific time frame ; travel booked prior to the start date of this promotion is eligible . Offer applies to all eligible segments flown , regardless of the @ @ @ @ @ @ @ @ @ @ trip or trips including open jaws . Bonus elite qualifying miles earned in conjunction with flight activity during 2011 will count towards your 2012 membership year ; bonus elite qualifying miles earned in conjunction with flight activity during 2012 will count toward your 2013 membership year . This offer does not increment the members prize eligible mileage balance , elite qualifying points or segments . Double elite-qualifying miles will be calculated at 100% of the base miles earned . Double elite qualifying miles will be posted to the account of the traveling AAdvantage member within 6-8 weeks after qualifying activity posts to your account . Promotion does not apply to travel on mileage award tickets or fares that are- ineligible- for AAdvantage mileage accrual . Flights operated by our codeshare partners ( except American Eagle and the AmericanConnection carrier ) or oneworld alliance carriers are not eligible for this promotion . This offer can be combined with other AAdvantage promotions . Registration prior to travel is required . <p> This is a post-only email . Please do not reply to this message . For all inquiries , visit- www.aa.com/contactaa- send @ @ @ @ @ @ @ @ @ @ at American Airlines , 4255 Amon Carter Blvd. , MD 2400 , Fort Worth , Texas 76155-2603 . <p> American Airlines reserves the right to change the AAdvantage program and its terms and conditions at any time without notice , For complete details about the AAdvantage program , visit- AAdvantage program terms and conditions . <h> Craft <p> Summary : Nice restaurant and a fan after eating frequently at their Atlanta ( now closed ) location . Unfortunately Ive got mixed feelings about this location . Lately the service has been extremely slow to the point no one wants to go there with me . Last time we had dinner there the food was not good ( nor bad , but for the price you @ @ @ @ @ @ @ @ @ @ food ) . The only reason it is not a 2 star restaurant its because the atmosphere is good and I really like two of their dishes . This restaurant is located inside the W Dallas Victory hotel . <p> Favorite dishes/drinks : - Big fan of their French toast for breakfast , Croque madame for lunch and a glass of Moscato DAsti with dessert for dinner . <h> W Dallas Victory Hotel <p> Summary : Not impressed . As a fan of the W brand , the W Dallas Victory Hotel is one of my least favorites ( I 'll put it out there with the W Atlanta in Buckhead ) . The pool is up around the 15th floor which is cool , and the gym is spacious but old . The service is not good though , they take forever to answer the phone and for a while this lady would pick up and she sounded very uninterested in whatever my request was . I also had an unfortunate experience while staying there which prompted me to change hotels to The Joule which I find it to be @ @ @ @ @ @ @ @ @ @ hotel and my experiences have been mixed . The Craft in Atlanta was much better . My last experience in the restaurant included slow service and lackluster overpriced food ( my first experiences were quite good and I 'm still a huge fan of their french toast ) . <p> SPG upgrades : Asked repeatedly for an upgrade and never got it . I even had the ambassador request an upgrade on my behalf with no luck ( it was the last week of one of our team members and we wanted a large room to gather and hang out ) . I also had a bad experience which required a room change and even after that they would n't try to accommodate me in a better room . Very disappointed . 
@@45151458 @5151458/ 55329 @qwx465329 <h> Changes with NginX 1.5.7 <p> Below are the list of changes from the 1.5.6 release of NginX to version 1.5.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES <p> Changes with nginx 1.5.7 19 Nov 2013 * ) Security : a character following an unescaped space in a request line was handled incorrectly ( CVE-2013-4547 ) ; the bug had appeared in 0.8.41 . Thanks to Ivan Fratric of the Google Security Team . * ) Change : a logging level of authbasic errors about no user/password provided has been lowered from " error " to " info " . * ) Feature : the " proxycacherevalidate " , " fastcgicacherevalidate " , " scgicacherevalidate " , and " uwsgicacherevalidate " directives . * ) Feature : the " sslsessionticketkey " directive . Thanks to Piotr Sikora. * ) Bugfix : the directive " addheader Cache-Control ' ' " added a " Cache-Control " response header line with an empty value . * ) Bugfix : the " satisfy any " directive might return 403 error instead of @ @ @ @ @ @ @ @ @ @ to Jan Marc Hoffmann . * ) Bugfix : the " acceptfilter " and " deferred " parameters of the " listen " directive were ignored for listen sockets created during binary upgrade . Thanks to Piotr Sikora. * ) Bugfix : some data received from a backend with unbufferred proxy might not be sent to a client immediately if " gzip " or " gunzip " directives were used . Thanks to Yichun Zhang. * ) Bugfix : in error handling in **25;4329;TOOLONG * ) Bugfix : responses might hang if the ngxhttpspdymodule was used with the " authrequest " directive . * ) Bugfix : memory leak in nginx/Windows. 
@@45151459 @5151459/ <h> How to : Override a Location directive on NginX <p> Sometimes when coding you are in need to re-use a lot of the logic across different systems but find yourself needing to overwrite some of that functionality on a special case . In this particular scenario I needed to overwrite a Location directive on NginX . What do I mean by that ? Well , I do a lot of includes as a lot of functionality is shared across several sites but from time to time I have a special request/need where I need to implement custom behavior . For that reason I was defining a Location directive twice on NginX . To avoid itI could had dropped an include but it would mean manually maintaining a copy of some directives specially for this site . To avoid that I was looking for a way to define a Location directive twice and have one of them take preference . For those using EasyCamps EasyEngine you might find yourself trying to overwrite some of the default **27;4356;TOOLONG but come across issues when trying to define the same location elsewhere @ @ @ @ @ @ @ @ @ @ NginX you get an error indicating : <p> In order to achieve this functionality I relied on NginX ability to stop processing further location directives . You need to change the declaration of the Location directive by- using a different operator . Like , if you were using the = operator , have the new definition use- the- operator . Below is an example : <p> All remaining directives with conventional strings . If this match used the " " prefix , searching stops . <p> Regular expressions , in the order they are defined in the configuration file . <p> If #3 yielded a match , that result is used . Otherwise , the match from #2 is used . <p> So all directives were searching stop would help you to completely overwrite a Location directive . As I mentioned , you need to leverage the fact that NginX considers a query with different operator an entirely different Location directive avoiding the " duplicate location " error message when running " nginx -t " to verify your configuration . 
@@45151461 @5151461/ <h> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 <h> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 <p> Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now we observe that they are getting grouped together and the change can easily be made via the UI . For example , your Active Directory Users and Machines GUI allows you to easily move 3 of the 5 roles . Back in the day you needed to register some weird Dll and open a mysterious snap-in for the Management Console but those days are over . Today though I am going to focus on how we can move all 5 roles from within Powershell . This is important as with just one simple but powerful command , we can achieve the transfer @ @ @ @ @ @ @ @ @ @ places to do it . <h> Step 1 : Identify the current master(s) : <p> This can be easily achieved using the- netdom query fsmo command , it will return something like this : <h> Step 2 : Move the roles to another server : <p> This is done using the following command : - **41;4385;TOOLONG -Identity DC-02 -OperationMasterRole SchemaMaster , DomainNamingMaster , PDCEmulator , RIDMaster , InfrastructureMaster <p> As you can observe from the command , you are specifying the five Operation Master Roles you wish to transfer over to the DC-02.Cloudingenium.com server . You could , however , specify only one or more roles . This allows you to spread the roles around as you best see fit . The command will then ask you to confirm if you wish to move each of the roles to the server . You can chose between Yes , Yes to All , No , No to All , Suspend and Help . <h> Step 3 : Validate the Move <p> Run again the netdom query fsmo to make sure the change happened correctly . Keep in mind the replication of the roles could take some time . 
@@45151463 @5151463/ 55329 @qwx465329 <h> Tag Archive : Makecert <p> How to : Manage vault certificates in Windows Azure Backup Obtained from : - LONG ... Windows Azure Recovery Services encompasses a set of Windows Azure vaults that help to protect your organization from data loss , and aid in continuity of operations . Vaults are used to store and protect information that is specified as part of your recovery services configuration . 
@@45151464 @5151464/ 55329 @qwx465329 <h> What is : **27;4428;TOOLONG ? <h> What is : **27;4457;TOOLONG ? <p> As of late I have been paying- closer attention to my firewall logs in an effort to- increase security but most importantly reduce the lag time we sometimes face when accessing the Internet . Having efficient rules allow for traffic to move more quickly through the firewall and in this case , examining denied connections reveal recurring attempts of what could be valid and acceptable traffic . Such is the case of **27;4486;TOOLONG . <p> The site **27;4515;TOOLONG exposes the web services protocol known as Software Quality Metrics- ( SQM ) from Microsoft . It is used by Windows applications- to recollect telemetry data ( product usage and failure information ) . Applications also use this site to download information specifying what instrumentation data needs to be- uploaded as determined by- the application developer . <p> The SQM- instrumentation data provided by SQM-enabled- clients allows application developers to understand product usage and failure information in order to- improve their products . Each SQM-enabled- client belongs to a SQM- namespace known as a SQM- partner @ @ @ @ @ @ @ @ @ @ namespace in the SQM service . <p> The structure and method of transferring the data from the SQM-enabled- client to the SQM- service is defined by the- SQM- Client-to-Service Version 2 Protocol . The method of creating the SQM- instrumentation data definition is SQM service implementation-specific . <p> The SQM- Client-to-Service Version 2 Protocol also defines a method for a SQM-enabled- client to download SQM- partner-specific information . Typically this information is used by the- SQM-enabled- client to control what instrumentation data is uploaded . This functionality is known as Adaptive Software Quality Metrics ( A-SQM ) . A-SQM data is created- at the SQM- service by the SQM-enabled client application- owner if the SQM- partner wants to download and use this functionality . The method of creating the A-SQM- data is SQM service implementation-specific . <p> The SQM Client-to-Service Version 2 Protocol provides the following communication : <p> Uploading instrumentation data from the client to the SQM service . <p> Uploading instrumentation data through a proxy ( relay ) to the SQM service . <p> Downloading A-SQM- data created at the SQM service . <p> The client receives a @ @ @ @ @ @ @ @ @ @ client compares the version ( ver ) number in the rsrc- command against the current client version number . If the version numbers are not equal , the SQM- client downloads the A-SQM manifest described in the response . <p> The SQM client forms an HTTP(S) GET path by concatenating the path argument ( as specified in section 2.2.3.6.3 ) from the command response message . The form is shown in the following example , where %PATH%- is replaced with the value specified in the path argument . <p> https : **36;4544;TOOLONG <p> The client downloads the resource file by using HTTP(S) GET . The resource is described- in MS-SQMCS section 2.2.6. 
@@45151467 @5151467/ <p> So this is the latest in the " I want to improve my Google-s PageSpeed " saga . I was running Googles PageSpeed to determine what fixes I need to implement on my site to get a higher ranking and hopefully improve the rank it gives to my site . One of the key aspects it shows is that there is a render-blocking JavaScript being executed . Further inquiry revealed the following script being executed on every single page : <p> At this point I have absolutely no clue where this is coming from . Searching for " ? dm= " yielded only references to multisite and I figured this had to do with the Cache I had recently implemented so I completely dismissed it . However , it turns out this is in fact a script added by the " WordPress MU Domain Mapping " plugin that is supposed to help with the remote login to a site although effectively what it is doing is loading the homepage on every page which seems unnecessary and google keeps complaining about this script so I figured I would remove @ @ @ @ @ @ @ @ @ @ at this point : <p> Edit the code for the plugin to adjust its functionality : Create an asynchronous JavaScript or whatever improvement you could think of . I believe this is located at the head and the script is referenced as " remoteloginjsloader ' " found in LONG ... around line 672 . <p> I am still not sure how this cross-domain login support is really supposed to work . For now I have only disabled the functionality via option 2 but if it turns out I do need this functionality I might have to look further into option 1 and probably just go for the asynchronous JavaScript method . <p> UPDATE : <p> I figured out what this actually does ! Well , sort of . I figured out what I lose if I unselect Remote Login : When you visit one of your sites that is not in the same domain name of your root site you do n't  get automatically logged in . So , you do n't  get the fancy WordPress bar at the top , etc . So here is the code I used @ @ @ @ @ @ @ @ @ @ to indicate that the script should be loaded asynchronously and boom ! solved I think.function remoteloginjsloader() - global $currentsite , $currentblog ; 
@@45151468 @5151468/ <h> How to : " Change product key " in Windows 8 or in Windows Server 2012 <h> How to : " Change product key " in Windows 8 or in Windows Server 2012 <p> I 've learned over the years to not activate a Windows product until I am confident it is stable . I say that because I used to activate Windows right after installation and either because of third party update , malfunctioning hardware , etc . I had to reinstall and eventually I had to call Microsoft to get my product key unlocked . However , something strange happened . In order to not activate windows right off the bat I installed it via the network which uses I guess an evaluation license or something . Once activation time came I just could n't type my license key . I really was n't ready to re-install as that defies the whole purpose of not activating on day cero . I did some research and found a way to provide once again your license / product key . <h> Symptoms <p> When you try to change the product key @ @ @ @ @ @ @ @ @ @ not find a " Change product key " link in the System item in Control Panel . This used to be available in previous versions of Windows but for some reason they decided to get rid of it . I cant phantom why , seems like a pretty useful feature . For example , if you want to convert a default setup product key to a Multiple Activation Key ( MAK ) on a computer that is running Windows 8 . <p> Fortuantely we have a few options at our disposal to change the product key in the new versions of Windows . <h> Solution <h> Method 1 <p> Swipe in from the right edge of the screen , and then tap Search . Or , if you are using a mouse , point to the lower-right corner of the screen , and then click Search . <p> In the search box , type Slui 3 . <p> Tap or click the Slui 3 icon . <p> Type your product key in the Windows Activation window , and then click Activate . <h> Method 2 <p> Swipe in from the @ @ @ @ @ @ @ @ @ @ . Or , if you are using a mouse , point to the lower-right corner of the screen , and then click Search . <p> Type Command Prompt in the Search box . <p> Right-click Command Prompt , and then click Run as administrator . If you are prompted for an administrator password or for a confirmation , type the password , or click Allow . <p> Run the following command at the elevated command prompt : <p> slmgr.vbs /ipk &lt;Your product key&gt; <p> Note You can also use the Volume Activation Management Tool ( VAMT ) 3.0 to change the product key remotely , or if you want to change the product key on multiple computers . 
@@45151469 @5151469/ <h> Importing WordPress sites : Why wont the Import tool finish ? <h> Importing WordPress sites : Why wont the Import tool finish ? <p> Lately I have been consolidating WordPress sites into one single installation to simplify the management process but one of the issues I 've come across is that the Export / Import tool wont completely import an entire site . Sure , as a site grows the work of moving over all the media and posts becomes daunting . Well , it so happens that unless you have a super fast host and db connection your WordPress worker process ( or however you want to call it / it is called ) times out . I had a site even with dedicated db and web host it took over 30 minutes to process the import . So , if you think of it that way then it only makes sense that the solution follows one of the two options : <p> Break up your exported XML so that WordPress imports only a few things at a time . <p> Increase the timeouts of your php.ini so @ @ @ @ @ @ @ @ @ @ long time . <p> For most users option 1 is the one and only option . Why ? Because they are tied up to a web host provider that wont allow them to tune their php settings . This only makes sense , you are sharing a server with probably hundreds if not more of users and having a job run for say an hour is going to have a significant impact on everyones site performance . 
@@45151470 @5151470/ <h> Resolved : Get-AzureVM Requested value enable was not found <h> Resolved : Get-AzureVM Requested value enable was not found <p> Lately I started to assign ACLs to my VMs and realized this was a very repetitive task and that many of the several ACLs were repeated across the VMs . Because of that I decided I needed to use PowerShell to manage this task and more quickly deploy the same set of ACLs to our Servers . For example , imagine you are trying to configure an external service like CloudFlare and you need to establish the valid IPs from which it may connect to your servers . With 10 entries only for the IPv4 space multiply that by your number of servers and you start getting the idea of the importance of this . <p> The big problem I am facing is that when I perform Get-AzureVM which is needed to configure the ACLs I get an error : <p> After much struggle I found out that this is a problem with the latest February release of the Azure SDK for PowerShell . Uninstalling the currently latest @ @ @ @ @ @ @ @ @ @ issue . <p> To downgrade simply follow this instructions : <p> Go to : All Programs and Features and find " Windows Azure PowerShell February 2014 " . You 'll find it has a version number of 0.7.3 . Go ahead and click on uninstall. 
@@45151471 @5151471/ <h> What is 1e100.net ? <h> What is 1e100.net ? <p> Perhaps many of you have come across the domain 1e100.net . As I was working with custom search I noticed my firewall was blocking traffic so the results were not coming up . A little bit of digging revealed the following : <p> Denied Connection <p> Log type : Web Proxy ( Forward ) <p> Status : 12227 The name on the SSL server certificate supplied by a destination server does not match the name of the host requested . <p> As you can observe from the error message above the connection was blocked because the certificate supplied did not match the name of the host . I was n't sure why 1e100.net was being used and I was worried it might compromise my security . However further research showed it is a Google owned domain and it is legit . Below is the answer I found on the Google website about it : <h> Answer obtained from Google : <p> 1e100.net is a Google-owned domain name used to identify the servers in our network . <p> Following standard @ @ @ @ @ @ @ @ @ @ a corresponding hostname . In October 2009 , we started using a single domain name to identify our servers across all Google products , rather than use different product domains such as youtube.com , blogger.com , and google.com . We did this for two reasons : first , to keep things simpler , and second , to proactively improve security by protecting against potential threats such as cross-site scripting attacks . <p> Most typical Internet users will never see 1e100.net , but we picked a Googley name for it just in case ( 1e100 is scientific notation for 1 googol ) . <h> Further research <p> If you do some further research , you will see that in fact the IP addresses that the domain 1e100.net are owned by Google . Below is an example from the IP referred to above : 
@@45151474 @5151474/ <h> Resolved : Zemanta Editorial Assistant issues with https/SSL <p> One of the main reasons why I had left my admin panel open to server non-ssl requests was that Zemanta Editorial Assistant would not work properly behind HTTPS . I did some looking around and found that the same scripts were being sent over to the browser so I was at a loss of what was causing the issue . Pretty much everytime I worked on a post the " Content Recommendations " , " Related Articles " , " In-Text Links " and the " Tags " sections would be blank ; They would show on screen but have no content on them . This pretty much render them useless . Because of that I could n't force myself to move to an SSL only WordPress backend . <p> Finally though I got tired of having to log-in every time I switched between the HTTPs and HTTP version of the site . I decided it was time to resolve this issue . I went in and digged through the HTML trying to understand what was going on : Was @ @ @ @ @ @ @ @ @ @ . <p> To my dismay the answer was much simpler than I could had imagined . It turns out because Zemanta runs a script that is not part of this domain and does not travel via HTTPs , most browsers will block it . Take a look at Chrome for example : <p> Blocking this script made Zemanta unable to operate . This resulted as I mentioned on the different sections being present on the screen but unable to get content from the Zemanta servers . <p> The good news is just clicking on the Load unsafe script resolves the issue . Unfortunately neither Chrome or Firefox as far as I know let you chose which " unsafe " scripts to run . This could potentially present a security risk . I run my own WP install so I am assuming all the scripts I have are safe , but if you visit other sites you might have your doubts . It is even possible your site has been compromised and truly unsafe scripts are also being executed with scripts you want like Zemanta and there is no way to @ @ @ @ @ @ @ @ @ @ I can see the Zemanta scripts are being blocked not because they do n't  travel via HTTPs but because they are not part of the domain the website is on . I see their own domain name and Amazons domain as well . At this point there is little you can do if you want the message not to show up . One alternative is hosting your own versions of the scripts therefore having them load from your own server / domain avoiding chrome to consider them unsafe . This however could go against the terms of use of the third party scripts/components . If in doubt consult a lawyer . But if this is possible Google has a PageSpeed module for Apache and NginX that allows for third party components to be rewritten for your own domain with the aim of lowering DNS lookups and connections to more domains speeding up the site . You will see there they will also have a disclaimer that doing so could go against the terms of use of said code . 
@@45151475 @5151475/ <h> Nginx : How to correctly use Time and Size postfix / parameters <h> Nginx : How to correctly use Time and Size postfix / parameters <p> I was struggling on figuring out why my cache size was not growing as intended . I had setup my configuration so that pages would be cached for a really long time but refreshed often . The idea being able to cache my entire site but at the same time having NginX frequently checking for new updates . If the backed or upstream server was down , then most likely a " stale " copy of the page would be available and served to the client . That way visitors would suffer of reduced downtime . But the problem I was having is that the cache was growing up to a point and then started to decrease and oscillated around a few mb . Long hours checking headers and debugging until I finally realized I was n't using the Time parameters correctly . I was indicating " m " in the configuration thinking that stood for months but then it hit me What @ @ @ @ @ @ @ @ @ @ it , my cache was n't growing because it was getting deleted shortly after it was saved . Quite the undesired behavior and result of my lack of attention . <p> Regardless if you make silly mistakes like I do , I always find myself in need of reference material to figure out what is the right postfix when using size or time parameters in NginX . Because of that I have prepared two tables to help anyone who is also in need of a quick reference when configuring Nginx : <h> Size <p> By default when you specify a parameter of type size it is defaulted to bytes . If you wish to indicate something more practical you could use the postfix m or k . As fas as I am concerned g for gigabyte also works but then again I could not find it in the documentation . I have used it in my configuration file with nginx -t not complaining and the behavior looks correct ( but truly I have not come close to the limit I set so I cant say for sure ) <p> Syntax <p> @ @ @ @ @ @ @ @ @ @ M <p> megabytes <h> Time <p> There is more variety with the time parameters than with the sizes ( doubtful many would need fore than g ) . Some might expect the default might be in milliseconds as that is the smaller unit that nginx recognizes , but it actually defaults to seconds . <p> Syntax <p> Description <p> ms <p> milliseconds <p> s <p> seconds <p> m <p> minutes <p> h <p> hours <p> d <p> days <p> w <p> weeks <p> M <p> Months ( 30 days ) <p> y <p> Years ( 365 days ) <p> The cool thing with time is that you could combine different units . For example : " 1h 30m " is used for one hour and thirty minutes . Obviously this is far cooler than having to type 90m . If you want to combine postfixes , you need to order them from most to least significant ( 30m 1h ) would not work . 
@@45151476 @5151476/ <h> How to : Clone the mac address for the WAN interface on a Ubiquiti Unifi Security Gateway <h> How to : Clone the mac address for the WAN interface on a Ubiquiti Unifi Security Gateway <h> Background <p> Nowadays you somewhat expect that cloning a mac address on a gateway/router would be a basic feature but as Apple has taught us we do n't  know what we want even if we need it . But getting back into topic , recently we started testing Ubiquiti products and part of that included the Unifi Security Gateway . One of the features we are not able to configure via the web administration console / controller is that of cloning a MAC address . For obvious reasons , we require to do that in order for our ISP to assign the correct IP address . <h> Solution <p> The solution is a bit more difficult than what I would have hoped for . As you probably already know , you are able to access the USG ( Unifi Security Gateway ) via SSH in order to configure several things . This @ @ @ @ @ @ @ @ @ @ is not compatible with our network . This time thought , it was useful in order to access the configuration console and change the MAC address of the WAN interface ( eth0 in my case . ) To do this , you need to follow these steps : <p> Connect to your USG via SSH <p> Run the following commands ( where you replace the Xs for the right numbers ) : <p> Now , if you connect your Ethernet cable to the USG you 'll notice the change took place and you 're using the new mac address . But here comes the tricky part , when you reboot/restart the USG the settings will be lost ! In order to persist the settings , you need to export the running configuration ( after performing the changes/commands mentioned above ) and include it in your config.gateway.json file . At this point I was like you , clueless as to what to do next . If you want to learn how to do this , please visit : UniFi How to further customize USG configuration with config.gateway.json . <p> Now that you have @ @ @ @ @ @ @ @ @ @ <p> " interfaces " : <p> " ethernet " : <p> " eth0 " : <p> " mac " : " AB:CD:EF:01:23:45 " <p> <p> <p> <p> <p> remember to put your actual MAC address there as well as confirm that your WAN interface is eth0 . Also , do n't  forget to validate the json file , you can use something like- http : //jsonlint.com/ which is a JSON Validator . Just to make sure the syntax is correct . <p> Now , if you are using Ubuntu you need to create this json file on your controller . After logging in the default folder where you need to place the file is located at : - **28;4582;TOOLONG . Keep in mind default here is the name of the site , so if your site has a different name you will need to access that folder not this one . Once you 're there , you will create a file ( nano config.gateway.json ) and paste there our configuration we obtained just before in json format . If you are doing this for different settings , you 'll need to add @ @ @ @ @ @ @ @ @ @ you 're done , reboot your USG and see if the setting worked . If it booted correctly and the MAC address changed as indicated , then you 're all set and good to go ! 
@@45151478 @5151478/ 55329 @qwx465329 <h> Tag Archive : Memory leak <p> Changes with NginX 1.4.5 Below are the list of changes from the 1.4.4 release of NginX to stable version 1.4.5 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.5 11 Feb 2014 * ) Bugfix : the $sslsessionid variable contained full session serialized instead of just a session i 'd . Thanks to <p> Changes with NginX 1.5.7 Below are the list of changes from the 1.5.6 release of NginX to version 1.5.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.7 19 Nov 2013 * ) Security : a character following an unescaped space in a request line was handled incorrectly ( CVE-2013-4547 ) ; the bug 
@@45151481 @5151481/ 55329 @qwx465329 <h> Category Archive : iPad <p> How to : Setup your iOS device ( iPhone , iPad , etc ) to use a proxy server for its internet connection If you are working inside a corporate network- you might need to use a proxy server in order to- successfully connect to the internet . Proxy servers have several advantages , as the resolution of the web address takes place at <p> How to : Sync my Google Contacts with my iPhone or any other iOS device One of the weirdest issues I-ve faced as of late is that my Google Contacts were not being brought in correctly to my iPhone . What was really going on behind the scenes was that all the contacts instead of being imported <p> Deploying Microsoft Lync 2010 mobility services for iPad and iPhone After much struggle to get my iPad to work with Lync server ( I tried everything from certificates to configuring the edge server , etc . ) I found out Microsoft let 's those mobile devices connect via web services but you need to install them . The installation @ @ @ @ @ @ @ @ @ @ from the server when logging in from the iPad / iPhone When trying to log in from an iPad or iPhone you may run across the error message : " Ca n't verify the certificate from the server . Please contact your support team . " Something I 've noticed through experience working with anything from 
@@45151482 @5151482/ 55329 @qwx465329 <h> Category Archive : SharePoint <p> SharePoint Error 0x800700DF : The File Size Exceeds the Limit Allowed and Can not be Saved When working with SharePoint 2013 and Sky Drive Pro I was transferring some files from our OnPremise server to Microsofts Office 365 site when I came across error - 0x800700DF : The File Size Exceeds the Limit Allowed and Can not be Saved . Generally <p> How to : Enable the " Connect to Outlook " grayed out/disabled button on SharePoint 2013 to synchronize your content The short answer : just hit F5 ( reload the page ) . I had been terribly confused as to why the " Connect to Outlook " button on the SharePoint 2013 ribbon would be grayed out most of the time . The first time <p> How to : Synchronize content between SharePoint 2013 and Outlook One of the great features of SharePoint- 2013 is its ability to synchronize a variety of information with Outlook . You can connect a SharePoint- calendar , library , contact list , or other type of list with Outlook . I think by @ @ @ @ @ @ @ @ @ @ <p> How to : Install the Language Packs for SharePoint 2013 to deploy site on a different language While installing- SharePoint- 2013 I ran in to several issues that appear- to have corrected by installing SharePoint 2013 in English on a Windows 2012 Server in English ( same language ) . I should note I was trying to deploy a SP server in <p> How to : Resolve issue " This workbook- can not be opened because it is not stored in a trusted location . To create a trusted location , contact your system administrator . " on SharePoint 2013 Right after installing SharePoint 2013 you might run into the issue that when your users try to open an Excel file you run into the error <p> How to : Resolve error " The management agent " MOSSAD-Connection with AD " failed on run profile " DSFULLIMPORT " because of connectivity issues . " in SharePoint Server 2013 When you are configuring SharePoint Server 2013s User Profile Service you might run into the error message- indicating that the management agent failed on run profile " DSFULLIMPORT " because of @ @ @ @ @ @ @ @ @ @ issue LONG ... A failure- was encountered- while attempting to create the site . " while using SharePoint 2013 I am honestly starting to ask myself , who approved this to be released to manufacturing ? ! ? lol . Anyway , on a fresh SharePoint- 2013 installation I am encountering issues when trying to follow a site . As you probably know when you <p> How to : Resolve the error : The Execute method of job definition LONG ... ( I 'd **36;4612;TOOLONG ) threw an exception- when using SharePoint 2013 I am having a hard time fixing this error . I have reinstalled SharePoint like 10 times , created new databases , different installation options and even resorted to using all English products ( Windows Server 2012- and SharePoint- 2013 ) to <p> How to : Resolve error The EXECUTE permission was denied on the object procputObjectTVP , database SharePointConfig when using SharePoint 2013 My saga with SharePoint is still far from over apparently . The latest and greatest error message found on the event log- reads " How to : Resolve error The EXECUTE permission was denied- @ @ @ @ @ @ @ @ @ @ 2013 error : Windows- can not open the 32-bit extensible counter DLL- ASP.NET2.0.50727 in a 64-bit- environment . Contact the file vendor to obtain- a 64-bit version . Alternatively , you can open the 32-bit- extensible counter DLL by using the 32-bit version of Performance Monitor . To use this tool , open the Windows folder , open the Syswow64 folder , and then start Perfmon.exe . I am 
@@45151483 @5151483/ <h> How to : Clear the history on your Macs terminal ( command prompt ) <p> Sometimes you want to clear your history due to a number of reasons , like when you type a password and now it is being remembered by your terminal session . Ive read this information is stored in your profile as a History file , and many people go as far as suggesting you should delete it . However , there is an easy command line you can execute that will clear your history and save you any troubles of messing with system files . Just execute the following line to clear your terminal history on your mac : 
@@45151484 @5151484/ <h> How to : Install the MySQL Agent for NewRelic in an Ubuntu server <h> How to : Install the MySQL Agent for NewRelic in an Ubuntu server <p> As part of a series of posts ( How to : Install the Memcached Agent for NewRelic in an Ubuntu server and- How to : Install the NginX Agent for NewRelic in an Ubuntu server ) I am including instructions on how to install the MySQL monitoring agent for NewRelic . I generally only add instructions to those plugins that are a bit tricky . For example , the MS SQL plugin is very straight forward and there is n't much to add , but when it comes to the MySQL plugin I always struggle with deciding how to install Java and then how to create a script that will run on start-up to monitor MySQL ( you guessed right , as the instructions stand you need to manually launch the MySQL agent monitor ) . <p> Configure- **27;4650;TOOLONG for each MySQL instance you want to monitor <p> Run- java -Xmx128m -jar &lt;the plugin jar name&gt;- in your console <p> Note @ @ @ @ @ @ @ @ @ @ for the Java JVM process to 128mb . See the README for more information . <p> As I mentioned there are a few things that I usually struggle every blue moon I decide to install this plugin : 1 ) How to get Java installed ( it is a pre-requisite ) and 2 ) How to script the execution of the monitoring agent on boot . Below are my installation steps that I hope will help others struggling with the same questions find a quick answer : <h> O. Summary ( Just the commands ) <p> For those whove done this in the past and just want to get it over with quickly : <p> sudo bash <p> ( Install Java ) <p> apt-get install- openjdk-7-jre <p> ( Download the plugin to a folder like /etc/newrelic and extract the files to a subdirectory . See to get the latest version ) <p> And for those who are going through this for the first time : Step by Step with explanations : <h> I. Install Java <p> As of late Ive been recommending using the- webupd8team/java ppa as you get Oracles @ @ @ @ @ @ @ @ @ @ well in my case when using this plugin . I wrote a post : How to : Install Java JRE on an Ubuntu computer that walks you through using different ppas , the original download from Oracles site or deploying OpenJDK . All options as far as I have tried them worked well . The easiest by far would be the webup8teams ppa as you get updates with that or OpenJDK . <p> II . Get the Server Agent <p> Visit NewRelics site to download the latest version ( 1.2.1 as of the time of writing ) . Find a suitable location and extract the tar.gz file . Once you are done you are going to have to copy the configuration templates so you can work with them : <p> cd- /etc/newrelic <p> ( Download the MySQL plugin . tar.gz file from Plugin Central ) <p> tar -zxvf **32;4679;TOOLONG <h> III . Configure the Server Agent <p> Now we are going to configure the Monitoring server agent . These step simply consist on creating ( copying from templates ) the files that contain the instances to monitor and your newrelic @ @ @ @ @ @ @ @ @ @ nano **26;4805;TOOLONG # Modify to add your NewRelic 's key <p> cp **34;4833;TOOLONG **26;4869;TOOLONG <p> nano **26;4897;TOOLONG # Modify the configuration to monitor the instance(s) <p> There is an init.d script ( and others for other linux distributions ) that comes with the download . Simply navigate to : LONG ... to find the script and note following instructions : <p> # CONFIGURATION <p> # Change DAEMONDIR to your chosen location for the plugin jar file . <p> # To run on boot , do n't  forget to run update-rc.d newrelic-mysql-plugin defaults . <p> I also recommend making the following changes ( in bold ) to limit the amount of memory the plugin may use : <p> The first -Xmx64m is to limit the ammount of memory Java may use to run the plugin . Configure this based on the number of instances/data you will be collection as well as your servers available memory . I believe NewRelic recommends 128mb but 64mb has worked well for me . The second change is adding " - &gt; **28;4925;TOOLONG 2&gt;&amp;1 ? which pretty much directs all output to the log file as @ @ @ @ @ @ @ @ @ @ the Daemon <p> First let 's do a test to make sure things work . Execute your daemon to see if it is working fine . At this point the output should be going to the log file and if all goes well you should start seeing data on your NewRelic dashboard . But before that you need to mark the file as an executable otherwise you 'll get the following error message : 
@@45151485 @5151485/ <h> 80244019 Windows update encountered an unknown error <p> Windows update error 80244019 usually refers to an issue downloading the content from the WSUS server . If you look at the Windows Update Log located at- **25;4955;TOOLONG you-ll probably find entries similar to the one below : <p> As you can notice it indicates that Error0x80244019 has- occurred- while- downloading- an update . Furthermost you can type in your browser the URL : LONG ... and you-ll get a 404 error code . What this simply means is that that your client can not download the updates from the server as the web page does not offer them . Below are two reasons why this could be happening : <p> Your web server does not support the necessary MIME/Types needed for proper WSUS functioning <p> Most people will come across Error- 0x80244019- because of the MIME types apparently . Most of the answers I found online included the addition of them as the solution , while others wanted me to edit the registry . Regardless , as a first step after checking the windows update log file I would @ @ @ @ @ @ @ @ @ @ registered . For that go to your IIS server and select the Content folder . There you can see the option to display all the MIME types for that folder . Many people make the mistake of checking directly against the root site , but these needs to be done at the Content level folder . There look for the following 4 types and if one is missing make sure to add it : <p> In my particular case I had all the MIME types so I proceeded to the next step . As I mentioned above if you check for them on the root site you might find that . psf is not there but that would not be an issue . <h> Step 2 : The files are not present on your content directory in the server-s hard drive <p> Another reason why Error- 0x80244019- might be happening is that you do n't  actually have the files the client is looking for in your hard drive . There are a number of reasons why this could be happening . In my particular scenario I used the Windows Small @ @ @ @ @ @ @ @ @ @ -&gt; Server Storage -&gt; Move Windows Update Repository Data ) and for some reason some of the updates went missing . If you ever by accident delete files from the content directory this would also apply to you . In order to identify this what you must do is the following : <p> Go to the log file as mentioned above and find an instance that failed to download and obtain the URL <p> Go to the URL and confirm that you get the 404 message <p> Go to your WSUS server and browse to the location where the content ( updates ) are stored ( For example : C:WSUSWsusContent ) <p> Navigate to the folder structure where your update file should be located . If it is not there then you are missing files . <p> To resolve this is fairly simple although it took me sometime to find out the solution . You must start a command line prompt with Administrator credentials . After that you are going to use the WSUSutil.exe . You can find it usually on this path : %drive%Program FilesUpdate ServicesTools- and can read @ @ @ @ @ @ @ @ @ @ browse to the location of the WSUSutil.exe tool you must execute the following command : <p> WSUSutil.exe reset <h> Reset : <p> You use this command if you store updates locally on your WSUS server and want to ensure that the metadata information stored in your WSUS database is accurate . With this command , you verify that every update metadata row in the WSUS database corresponds to update files stored in the local update file storage location on your WSUS server . If update files are missing or have been corrupted , WSUS downloads the update files again . This command might be useful to run after you restore your database , or as a first step when troubleshooting update approvals . <p> As described above using the reset parameter will force WSUS to re-download any corrupted or missing files again from Windows Update . After performing this command I was able to use WSUS again . <h> Step 3 : General conectivity issues <p> The last reason why Error- 0x80244019- might show up is just a general connectivity issue . Assume you can reach the WSUS server but @ @ @ @ @ @ @ @ @ @ Another reason would be a firewall or poorly configured IIS that causes the content directory to be inaccessible . 
@@45151486 @5151486/ <p> I 've had some issues with automatic updates in- WordPress- and I 'm kind of blaming Godaddy for it . When I was using my previous hosting provider all the updates always worked from within the application but now either a folder ca n't be created / deleted , it gets stuck I do n't  know anymore . Regardless , Jetpack was n't able to get automatically updated so I got the " An automated WordPress update has failed to complete - please attempt the update again now . " message . I went ahead and performed a manual update but now that message remains there and wont go away . <p> Solution : <p> Make sure you delete the appropriate directory within the " . /wp-content/upgrade " folder <p> Edit the . maintenance on the root directory to make sure the entry for the upgrade in progress that failed is removed 
@@45151488 @5151488/ <h> How to : Call a base constructor in C#.Net <p> Recently I decided I wanted to start creating children classes to handle my Exceptions , but I ran into the issue that I could n't quite call the base constructor and I kept asking myself why if I did n't  define a constructor the class couldnt use the base class and automatically expose them . So below is kind of what I was doing : <p> As you can tell you can only use the base() method as part of the declaration of the constructor . So what if I want to modify that message in a special way ? Say , you want to prefix all your messages with something . No big deal , you can use static methods to change a parameter this way : 
@@45151489 @5151489/ <p> Resolved : Outlook.com/People contacts not syncing to iPhone Although I have been recommending people to migrate to Outlook.com as an alternative to Gmail ( You can read more about that here : How to : Migrate from Gmail to Outlook.com ( previously Hotmail.com &amp; Live.com ) . Basically having ActiveSync and Customs Domains ( until the end of July 2014 ) as the big <p> Resolved : Microsoft Office has detected a problem with your Information Rights Management configuration . About a week ago one of our client computers started facing issues regarding the ability to access the Information Rights Management IRM server . We use Windows Azures IRM service ( Azure Rights Management ) so there are a few things you need to do <p> Resolved : Get-AzureVM Requested value enable was not found Lately I started to assign ACLs to my VMs and realized this was a very repetitive task and that many of the several ACLs were repeated across the VMs . Because of that I decided I needed to use PowerShell to manage this task and more quickly <p> - How to : Manage @ @ @ @ @ @ @ @ @ @ account via PowerShell with an array of commands at your disposal . Below is a quick reference guide of the commands available . You can connect to Microsofts Azure Cloud using the Azure Powershell SDK . After you have installed the SDK you should be <p> How to : Set up multiple Administrator accounts in Microsofts Live Domains Sometimes you want to distribute the administrative tasks- of maintaining a system across a number of individuals to- provide- business continuity in- case you are not available . Fortunately if you are using Live Domains you can have multiple administrators to manage your domain ( create accounts , etc . ) Obviously each <p> How to : Add an email Alias to your Outlook.com account There are several different kinds of aliases that you can create for an Outlook.com account . The easiest way by far is to use what I call for a lack of a better name an " on demand " alias . After that your best choice would be to <p> How to : Use your own Domain name with Outlook.com as the Backend Recently I decided to @ @ @ @ @ @ @ @ @ @ for free accounts ( you can read about that here : - How to move from Gmail to Outlook.com ) . But I 've been using my own domain in Google Apps for a while now <p> How to : Clean up your Gmail Inbox by deleting large attachments ( Sort Gmail by Date/Size ) So , apparently it is possible to fill out your Google Quota on Email . I am constantly reminded by my Google Drive application that Google requires more storage space and that it is at its capacity while Gmail shows a message 
@@45151490 @5151490/ <p> When you try to activate your Windows 8 , Windows 2012 Server or MultiPoint Server 2012 sometimes you have a pre-set key ( trial , etc. ) which wont let you activate your windows installation . There are other scenarios where you might want to change your product key ( licensing , feature set , etc. ) but unfortunately you cant even though the system is throwing you errors that it cant activate , etc . If you go to the System item in the Control Panel as before , you 'll see the " Change product key " link is not there anymore . <p> Fortunately there are other ways to get that Windows Activation // Enter a product key to activate Windows dialog box . Below are the two methods published by Microsoft on how to do this : <h> Method 1 <p> Swipe in from the right edge of the screen , and then tap Search . Or , if you are using a mouse , point to the lower-right corner of the screen , and then click Search . <p> In the search box , @ @ @ @ @ @ @ @ @ @ 3 icon . <p> Type your product key in the Windows Activation window , and then click Activate . <h> Method 2 <p> Swipe in from the right edge of the screen , and then tap Search . Or , if you are using a mouse , point to the lower-right corner of the screen , and then click Search . <p> Type Command Prompt in the Search box . <p> Right-click Command Prompt , and then click Run as administrator . If you are prompted for an administrator password or for a confirmation , type the password , or click Allow . <p> Run the following command at the elevated command prompt : <p> slmgr.vbs /ipk &lt;Your product key&gt; <p> Note You can also use the Volume Activation Management Tool ( VAMT ) 3.0 to change the product key remotely , or if you want to change the product key on multiple computers . 
@@45151492 @5151492/ <h> What is : http : //OCSP.msocsp.com ? <h> What is : http : //ocsp.msocsp.com ? <p> MS-OCSP stands for " Online Certificate Status Protocol ( OCSP ) Extensions " from Microsoft . Although this might seem a bit daunting the plain english version of that is n't : " Microsoft publishes Open Specifications documentation for protocols , file formats , languages , standards as well as overviews of the interaction among each of these technologies . " This is a protocol extension provided by Microsoft to check on the Certificate Status . Performing a whois on the domain name confirms this is a domain owned by Microsoft so it is considered safe . If your firewall software is notifying you of connections to ocsp.msocsp.com you can relax . I am including below the WhoIs information at the date of publishing confirming it is a Microsoft domain . 
@@45151493 @5151493/ 55329 @qwx465329 <h> How to : Improve SSL performance on NginX <h> How to : Improve SSL performance on NginX <p> You would be surprised but a lot of people face SSL performance issues when using NginX . I recently deployed SPDY over SSL for my sites and came to realize that SPDY was in fact much slower than using standard HTTP . I proceeded to leave SSL alone and see its performance vs regular HTTP and again the speed was equally slow . Because of that I realized that SPDY was not the issue but rather the SSL layer . There are certain algorithms or cyphers that require a lot of processing ( cpu power ) which results on your SSL configurations being slow . Coming from Windows I never really messed with that or realized you could , but after using NginX you come to realize the wide range of things you can control but really getting to know them all requires a lot of specialized knowledge the amateur user might not have . Also while researching this topic I came across security advisories like these ones @ @ @ @ @ @ @ @ @ @ Taming BEAST : Faster , Safer SSL now on CloudFlare.The list keeps going on and on , and not surprisingly the recommendations keep changing with time . So as SSL gets more use things like performance and security start getting more attention and start receiving improvements . <p> So getting back on topic , there are a number of things you can do to speed SSL like OCSP Stapling but also disable certain ciphers because they are simply terribly slow . For example , NginX uses the DHE algorithm to create the cypher . This algorithm is really slow with NginX . Disabling it results in dramatic improvements ( at least it did for me and reading online it is mentioned a lot . ) <p> Long story short , there are a few recommendations ( obviously with time you learn you cant get it 100% right. ) : 
@@45151494 @5151494/ <p> One of the great features of NginX is its ability to cache content from a number of sources including but not limited to a proxy server or a fast cgi server . But in order to understand truly how well your cache is performing it is important to log the access to your server and record how the cache behaves . There are other things worth capturing which is why I recommend creating a new log which format you can customize and therefore monitor to see different metrics . <h> I. Define a new logging format : <p> As I mentioned I recommend creating a new logging format so you can over time customize it to the information that matters to you and study the behavior of your caching server . <p> Now , I have additional information that I am interesting in as my caching is skipped under certain conditions and I group the user agents in two groups : Desktop and Mobile clients so as you can see below I have added additional variables to record this : <p> I have a mapping that sets $devicetype @ @ @ @ @ @ @ @ @ @ Mobile <p> The last line includes all the variables I use to determine if I will bypass the cache . This are useful for troubleshooting but not necessary if you do n't  bypass the cache . <h> II . Register your new log in the server block to capture caching data <p> Just like your access and error logs , this new cache log we defined needs to be activated for logging to take place . Simply add a line like your access log line to enable it with a few differences : <p> accesslog /var/log/nginx/cache.log nginxcache ; <p> As you may notice , we are overwriting- the accesslog implementation by adding at the end the format we wish to follow which we defined previously as nginxcache . Also , our implementation of the nginxcache log format replicates to a degree the access log . If you wish to you could consolidate your access log into your caching log or keep them separate , up to you . <h> III . Analyze your log <p> As expected we want to get some valuable information out of our log . We @ @ @ @ @ @ @ @ @ @ and output useful information to us . <p> Here are a few commands useful for that : <h> A. Get a sense of how - many times the cache is used , bypassed , etc . In the case of a proxy cache - might be used as Bypassed to the upstream server <p> awk ' print $3 ' /var/log/nginx/cache.log sort uniq -c sort -r <p> Here is some sample output : <p> 9 MISS 6 STALE 19 BYPASS 156 127 HIT <p> Here are what those different results mean : <p> Miss <p> When a page that is candidate for caching was not found in the cache . <p> Stale <p> A cached page that has expired and was visited . NginX will update the cached content . <p> Bypass <p> When a page has been indicated to not be cached . <p> <p> A number of things I do n't  quite understand yet . Seems to be used also as a Bypass by my proxy cache , although technically it means the request never reached the upstream server . I see this on connections that do not fall @ @ @ @ @ @ @ @ @ @ <p> What you really want to see . That a page that was tried to be accessed was found in the cache and returned from it . <p> So at the beginning you probably are going to get lots of Misses but eventually the Hits need to predominate . If you are getting too many Stales you should review your expiration policy . <h> B. Desktop vs Mobile visitors <p> This is another interesting metric . In my case I was n't sure how worth it was to cache responses for mobile users . Knowing how many kinds of visitors you get helps you understand your audience . Remember , all this queries depend on the order you use in your log file so keep that in mind . <p> awk ' print $7 ' /var/log/nginx/cache.log sort uniq -c sort -r <p> will output something like : <p> 649 desktop <p> 3 mobile <p> Now , keep in mind that anything that is not mobile I am cataloging as desktop on my user agent mapping . This might be misleading as bots , etc. will appear as desktop clients and that @ @ @ @ @ @ @ @ @ @ by using the user agent instead of the device type variable . I created a new mapping table to identify some common user agents for things like availability monitoring which pings my server every x seconds . This can help you further understand your visitors ( or just use Google Analytics ) : <p> 52 Desktop 1 HighEnd-Mobile 16 Microsoft-NetworkProbe <h> C. Response codes <p> Understanding how often a request for a non-existent- page is made , etc. is also important . Again , leverage awk to get a summary : <p> awk ' print $12 ' /var/log/nginx/cache.log sort uniq -c sort -r <p> will output : <p> 8 403 34 304 27 404 25 302 2 500 19 499 142 301 1305 200 <h> D. Many more ! <p> As you can see we are just scratching the surface of what you can do here with custom logs . I just havent had the chance or interest to dig too much into them , but there are resources out there to help you if that is what you want . Below are a few resources I found on the @ @ @ @ @ @ @ @ @ @ what you can do . At the end of the day it is all about how you parse your log file , so keep that in mind and have it in an easy to parse format . 
@@45151495 @5151495/ 55329 @qwx465329 <h> The IO operation at logical block address 0 for Disk 7 was retried . <h> The IO operation at logical block address 0 for Disk 7 was retried . <p> When working with Multipath IO ( MPIO ) like when using iSCSI it is possible you might run across the message : <p> The IO operation at logical block address X for Disk X was retried . <p> This initially looks like trouble although truly it is more on the level of a general warning . Should you receive many warning on a short period of time then you should consider addressing this as you might be suffering of performance issues . This warning message appears when the IRP sent over MPIO times out . This could be network connectivity issues ( latency , connectivity , etc ) but more like it is a matter of the target being too busy to respond timely . So again , sporadic warnings during high IO operations could be expected but recurrent messages means you need to look into the work load or networking of your shared storage . @ @ @ @ @ @ @ @ @ @ lost . It simply means that the IRP ( IO Request Packet ) timed out while the IO System waited for it to complete , and so it was tried again . When a thread begins any IO operation , the IO manager creates an IRP to represent the operation as it passes through the system . <p> The IRP gets stored in its initial state in a buffer/look-aside list , so that it can be retried if it fails the first time . That provides the atomicity that one would expect from any transactional system so that we can be more confident that you 're not going to get a bunch of corrupted or incomplete data written to your disk . <p> This event makes perfect sense in the event of an MPIO failure . Say Windows goes to read or write something from SAN storage . The request is dispatched , and at the same instant , I cut one of the cables to the SAN . That request is never going to complete , and so Windows will try the request again , only this time the request @ @ @ @ @ @ @ @ @ @ occur when the disks are overburdened or just really slow . You might notice these messages coincide with scheduled backups , etc . The disk might just be slow and busy , and some random IRP timed out and had to try again . The IRP could be getting stuck in an interrupt service routine , or a deferred procedure call , or whatever . <p> I could see having a lot of IO filter drivers in your stack exacerbating this issue as well . <p> Its not that this behavior did not occur just like this in previous versions of Windows , its just that Microsoft apparently decided to surface these events in Win8/Server 2012 . <p> Edit : - You can find the outstanding IRPs of a thread with a kernel debugger : - kd&gt; ! irp 1a2b3c4d , where you previously found that address by issuing the command- kd&gt; ! process 8f7d6c4a- which will list all the IRPs associated to the threads associated with that process. - kd&gt; ! process 0 0- to list all the processes running . <p> Once you list the information about an @ @ @ @ @ @ @ @ @ @ spot which driver last handled the IRP because it will have a- &gt;- pointing to it in the list . Then to get more information about what that driver was doing with that IRP , do a- kd&gt; ! devobj 1a2b3c4d5e6f- where that is the actual address of the device object . <p> Then do a- kd&gt; dt 0x1a2b3c3c2b1a CLASSPRIVATEFDODATA- using the address of the PrivateFdoData structure you got . ( Just one backtick ; I could n't get the parser to do it . ) <p> Now you 're ready to dump the AllTransferPacketsList data structure you got from PrivateFdoData . <p> The idea is , you 're tracking down what driver was doing what with the IRP the last time it was seen . If the IRP is AWOL for too long , its timed out and retried from the beginning . This can be caused by so many things even a stray cosmic ray . But the important thing is that the transaction will be retried from the beginning , and it will not be considered complete until the IO manager says it is . <p> Oh , and there 's @ @ @ @ @ @ @ @ @ @ worms . = 
@@45151496 @5151496/ <h> Ubuntu 14.04 and above in a Generation 2 Hyper-V Virtual Machine ( VM ) <h> Ubuntu 14.04 and above in a Generation 2 Hyper-V Virtual Machine ( VM ) <p> As most of you know , a Generation 2 Hyper-V Virtual Machine is generally reserved for Windows 2012 or 64 bit versions of Windows 8 as the New virtual Machine Wizard specifies : <p> Generation 2 <p> This virtual machine generation provides support for features such as Secure Boot , SCSI boot , and PXE boot using a standard network adapter . Guest operating systems must be running at least Windows Server 2012 or 64-bit versions of Windows 8 . <p> This was true until recently when Ubuntu released version 14.04 ( currently 14.10 is also available ) . Ubuntu 14.04 is the first linux release to support running inside a Generation 2 Virtual Machine . Needless to say you are going to need the 64 bit version . <p> The key to get this running is that you need to disable Secure Boot . This needs to be done before you commence the installation of the OS @ @ @ @ @ @ @ @ @ @ <p> Hardware <p> Firmware <p> On the top there is a " Secure Boot " option . Disable the checkbox on " Enable Secure Boot " <p> As the window indicates : <p> Secure Boot is a feature that helps prevent unauthorized code from running at boot time . It is recommended that you enable this setting . <p> Just disregard the recommendation . As mentioned this is required to run Ubuntu on a Generation 2 VM . If you are running Windows 2012 or Windows 8 and above you might want to follow the recommendation ( and default setting ) of using Secure Boot . <p> The Integration Services offered by Ubuntu have improved and not only will you be able to enjoy some of the Gen 2 improvements but also things like Dynamic memory being available during the installation process and online backup . Its pretty nice that Ubuntu is supporting Hyper-V features making it a viable option when deploying linux VMs on Hyper-V. 
@@45151497 @5151497/ 55329 @qwx465329 <h> Tag Archive : Patch ( computing ) <p> Changes with- NginX- 1.4.7 Below are the list of changes from the 1.4.6 release of NginX to version 1.4.7 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update addresses a bug fix and a security issue . Because of the security issue we rate this update of high priority . Changes with nginx 1.4.7 18 Mar <p> Changes with- NginX- 1.4.6 Below are the list of changes from the 1.4.5 release of NginX to version 1.4.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update mainly provides- bug-fixes which are not critical . Changes with nginx 1.4.6 04 Mar 2014 * ) Bugfix : the " clientmaxbodysize " directive might not work when reading a request body <p> Resolve : Error 0x800B0100 when you try to install Windows Updates or Microsoft Updates on a fresh MultiPoint 2012 Server If you read my previous post that reads quite similarly to this then you know you get lots of 0x800B0100 error messages @ @ @ @ @ @ @ @ @ @ a fresh MultiPoint 2012 <p> Changes with NginX 1.5.6 Below are the list of changes from the initial release of NginX to mainline version 1.5.6 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.6 01 Oct 2013 * ) Feature : the " fastcgibuffering " directive . * ) Feature : the " proxysslprotocols " and " proxysslciphers " directives . Thanks to Piotr Sikora. * ) Feature : 
@@45151500 @5151500/ <h> What is NAXSI ? <p> Technically , it is a third party nginx module , available as a package for many UNIX-like platforms . This module , by default , reads a small subset of simple rules ( naxsicore.rules ) containing 99% of known patterns involved in websites vulnerabilities . For example , &lt; , or drop are not supposed to be part of a URI . <p> Being very simple , those patterns may match legitimate queries , it is Naxsis administrator duty to add specific rules that will whitelist those legitimate behaviours . The administrator can either add whitelists manually by analyzing nginxs error log , or ( recommended ) start the project by an intensive auto-learning phase that will automatically generate whitelisting rules regarding websites behaviour . <p> In short , Naxsi behaves like a DROP-by-default firewall , the only job needed is to add required ACCEPT rules for the target website to work properly . <h> Why it is different ? <p> On the contrary of most Web Application Firewall , Naxsi does n't  rely on a signature base , like an antivirus , @ @ @ @ @ @ @ @ @ @ " attack pattern . Another main difference between Naxsi and other WAF , Naxsi filters Get &amp; Posts resquests and is OpenSource and free to use for your company or personal own use ( ie : as long as you do n't  resell a service or product based on Naxsi to customers ) . 
@@45151501 @5151501/ <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <h> Category Archive : United <p> United Airlines : Update Priority This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it ca n't be secured at the time it is applied to a <p> LONG ... Attached is the PDF containing the list of international phone numbers you can use to call the 1k United line. - intlcallsupport1Konly <p> Earn 50 bonus miles per check in ( per day ) via Facebook or Foursquare , within 2 miles of approved airports within the 50 states ( http : **31;4982;TOOLONG ) Upon check in , you will receive a confirmation message to the social media chosen , containing information about nearby retailers " you can opt out of these messages if you 'd like <p> Marriott Hotelers , More Stays , More Miles Earn up to 30k bonus miles from 9/1/11-12/31/11 Beginning with your @ @ @ @ @ @ @ @ @ @ to the general miles you earn : 2nd stay = +1000 miles 3rd stay = +1500 miles 4th stay = +1500 miles 5th stay or more = +2000 miles You must 
@@45151502 @5151502/ <h> What RSA key length should I use for my SSL certificates ? <p> Recently I was working on setting up an SSL certificate for a site and Internet Explorer asked me what key length I wanted to use . Usually providers offer 2048 and 4096 as their standard options so I was tempted to give it a shot to 16384 . When IIS started warning me that generating the key would take forever and if I was totally sure I began to doubt myself after all the bigger the length of the key the more processing power your webserver will need . I decided I needed to do some research on the impact something like this would have on performance . <p> Sadly I cant say I was able to put a figure on the impact on the length of the key but I was able to put into perspective what the different sizes mean in terms of your protection . Now , I believe all this with time will lose relevance as history has taught us technology breakthroughs are more awesome than what we expect . For @ @ @ @ @ @ @ @ @ @ indeed and anything beyond that by any measure meaningless . People were happy with 128-bit keys walking around . fast-forward a decade and what do you have ? 1024 bit keys have deemed crackable . The National Institute of Standards and Technology ( NIST ) has disallowed the use of 1024-bit keys after 31 December 2013 because they are insecure and already proven crackable as of 2010 . RSA expected 2048-bit keys to be good until about 2030 I wonder how theyll be feeling in a few years . <p> But well , in summary I think this are the conclusions we could draw : <p> Anything less than 2048 is officially insecure and at the end of this year will stop being supported by browsers so CAs are not emitting them any more . <p> 4096 bit should be very- good , almost guaranteed theyll be good past 2030 as 2048 bit keys are expected to be good till then . Most people use certificates for information that in 20 years would have no relevance , and passwords I hope you already changed by then . For all practical @ @ @ @ @ @ @ @ @ @ ( like financial transactions ) . The credit card number you used 20 years ago is useless . Anything beyond is a very special situation . <p> The 8192 bit key is some serious business . Think of : during my lifetime I do n't  want this embarrassing picture from ever being decrypted sort of thing . <p> The 16384 bit key is more on the side of computational power across parallel universes or quantum physics . We need a technological breakthrough that challenges our current notions of physics and the universe . <p> So having all this information and putting it into perspective brought me to the conclusion that a 4096 probably is good enough for me . By all means for what I use it even a- 1024 bit key would suffice I do n't  care or think the NSA or any hacker- has the slightest interest on my information to dedicate the computing power to crack them . So rest assure , a 4096 key is a good balance between performance and security probably 2048 more so but that 's just being dull . <p> Absolutely , cryptography @ @ @ @ @ @ @ @ @ @ computing . It will be interesting to see what we come up with then to address our need for encryption and privacy . So many things rely on the current methods of cryptography that the impact of this this would easily rival and exceed that of the year 2k bug . For all practical purposes it will impact every single computer out there . It is mind blowing and I really cant imagine what well do then , so I am incredibly curious about that too . 
@@45151503 @5151503/ 55329 @qwx465329 <h> Category Archive : Jetpack <p> Does JetPacks Photon damage Image SEO ? Thus far I have n't been able to find concrete proof of this but it probably does have an impact . Why is that ? Because the URL being served is from a different site than yours which results in Google or other search engines associating that picture to them not you . <p> How to resolve Jetpack error : Invalid request , please go back and try again.Error Code : invalidrequest . Error Message : Mismatch in redirecturi . I have a multisite installation and thus far on all my sites I have been able to relink all of them to Jetpack . However , in one of my sites when I click the Link account <p> Error activating Jetpack : SSL certificate problem : self signed certificate in certificate chain I havent had the need to activate Jetpack on a new site for a while but now that I did I came across this issue : Jetpack could not contact WordPress.com : **25;5015;TOOLONG . This usually means something is incorrectly configured on your @ @ @ @ @ @ @ @ @ @ this one : - **25;5042;TOOLONG SSL connection timeout ( WordPress forums ) - I realized that the issue was in fact an SSL connection timeout from your WordPress host to the Jetpack server . In my particular case it was a firewall issue as the Jetpack server did not pass the https validation . My guess is that 
@@45151504 @5151504/ 55329 @qwx465329 <h> Tag Archive : Security <p> Changes with- NginX- 1.6 Below are the list of changes from the 1.5.13 release of NginX to version 1.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update makes the development / mainline version 1.5.13 part of the stable branch. - Because of this several new features are included and we highly suggest people upgrade to <p> Changes with- NginX- 1.7 Below are the list of changes from the 1.5.13 release of NginX to version 1.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several new Features- so updating to this version is not considered urgent/critical . Changes with nginx 1.7.0 24 Apr 2014 * ) Feature : backend SSL certificate verification . * ) <p> Resolved : Passwords must meet complexity requirements Recently we were working on synchronizing passwords between an onPremise AD deployment and Azure Directory Services . As part of that we had to enable a strong password policy ( You can configure this security setting by @ @ @ @ @ @ @ @ @ @ Computer ConfigurationWindows SettingsSecurity SettingsAccount PoliciesPassword <p> pgp Not enough random bytes available . Please do some other work to give the OS a chance to collect more I was working on getting a private key in order to sign a PPA and upload it to Launchpad . However , when I ran- gpg gen-key the server kept asking me to give it more entropy : <p> How to : Generate Random Passwords Sometimes one of the issues I have is coming up with Random Passwords . It takes time to play around with the keyboard and although just hitting random keys seems easy I struggle with that . This is specially true when you are trying to set passwords for a long list of <p> How to secure WordPress This is a coleciton of let 's call them best practices for securing your WordPress installation . I used to be very carefree when it came to the Internet . But as you have probably heard hacking is a much more common activity nowadays . Back in the day usually the main targets were large <p> How to : Use the Certificate @ @ @ @ @ @ @ @ @ @ you are using the Certificate MMC snap-in and/or try to perform- a certificate auto-enrollment in your localhost/TMG server you 'll most likely run into an error message- on-screen- that reads " RPC- failure " . If you try requesting a certificate on other computers joined to your <p> How to : Remotely manage a Hyper-V Server / Enable Firewall exceptions Unfortunately one of the things you will find out when you install a Hyper-V Server 2012 is that it is completely locked down . You are provided- with a very neat utility that allows you to easily perform common tasks like enabling remote management , configuring your <p> How to : Add your Interactive Brokers account(s) to your Mint.com account After several months of waiting for Mint.com- to support Interactive Brokers finally they have that option available to their users ! Unfortunately , it does come with additional- work users are not used to . Usually just typing in your username and password is all you need to download <p> Lync 2010 : Ca n't verify the certificate from the server when logging in from the iPad / iPhone When trying to @ @ @ @ @ @ @ @ @ @ across the error message : " Ca n't verify the certificate from the server . Please contact your support team . " Something I 've noticed through experience working with anything from 
@@45151506 @5151506/ 55329 @qwx465329 <h> Category Archive : Exchange <p> Resolved : Exchange 2013 SP1 readiness check failing with AD errors ( User permissions and connectivity ) Recently Ive been having issues with our storage array for which reason I decided to deploy our Exchange Server 2013 SP1 server on the cloud instead of onPremise . As I tried to install the system I came across multiple readiness check <p> Exchange Server 2013 : HTTP 500 Errors for ECP and OWA ( Fresh Install ) Sometime ago I changed my domain name ( you can read more about it here : - How to : Rename an Active Directory Domain Name ) to better address some business needs but unfortunately I had my Exchange Server offline as we are mostly using Exchange Online now . <p> Exchange 2013 : How to completely remove all settings from Active Directory If you want to completely wipe all traces of Exchange Server 2013 from your Active Directory then follow this simple instructions . This has worked thus far for me but perhaps I missed something so feel free to provide any feedback you may @ @ @ @ @ @ @ @ @ @ 2013 SP1 As usual every time you try to install a new version of Exchange Server there are new caveats and what you expect will take 30 minutes takes longer . This time around was no exception . I am still hoping the day will come the installer decides it is going <p> Exchange 2013 : Wild Card Certificates and Hybrid Configuration Wizard As part of the latest migration to the Cloud I ran into some issues with my Exchange Hybrid Migration Wizard . This seems to be an obscure error but a lot of people out there who use a Wild Card Certificate for their server might run into <p> How to : Recover an Exchange server using the RecoverServer switch So a bit of background : I was performing a migration from onPremise to Cloud Mailboxes and the hardware the VM was running on was not the fastest . So I had this great idea : Perform a scheduled fail over to the faster server and continue working <p> Resolve : Setup Failure Occurred While Installing a Server RoleInstallWatermark Generally speaking this error is caused when an installation @ @ @ @ @ @ @ @ @ @ was in the process of being installed as part of an Exchange installation is not completely installed . Under the hood what happens is that there is a <p> How to : Compact the size of an Exchange Mailbox Database As of late I have been working on moving mailboxes form an onPremise deployment to the cloud in a hybrid environment . As I made progress I noticed the servers that hosted the Mailbox databases were using a lot of hard drive space and that backups <p> What is : Circular Logging in Microsoft Exchange Server I had some issues with an Exchange Server and had to perform a restore operation . While doing so I noticed that even though the user mailboxes were not huge , the size of the mailbox store was . As I read more about it , Exchange maintains the Mailbox Database 
@@45151507 @5151507/ 55329 @qwx465329 <h> How to : Secure a site using Apache <h> How to : Secure a site using Apache <p> If I had to , I would probably say that most people who use Apache do it to publish sites that are publicly accessible and/or have built-in security ( say WordPress which manages itself access to the application and data ) . So now that I wanted to publish a site that request a username/password or some sort of restriction so that only I can see its data ( think management console ) I had no clue where to start . <p> Apache does offer both modalities which can be used independently or in combination : - Authentication and Authorization . <p> Authentication means the user needs to provide credentials in order to access and its session is authenticated ( identified ) throughout its stay . <p> Authorization means the client is authorized to access the page . The individual user is not necessarily identified . For example if authorization is based on the source host ( ip address ) then there is no need for authentication ( @ @ @ @ @ @ @ @ @ @ both an authentication and authorization provider . The module- modauthnalias- is not an authentication provider in itself , but allows other authentication providers to be configured in a flexible manner . <p> The module- modauthzhost- provides authorization and access control based on hostname , IP address or characteristics of the request , but is not part of the authentication provider system . <p> First , you need to create a password file . Exactly how you do this will vary depending on what authentication provider you have chosen . More on that later . To start with , well use a text password file . <p> This file should be placed somewhere not accessible from the web . This is so that folks can not download the password file . For example , if your documents are served out of- **25;5069;TOOLONG you might want to put the password file(s) **26;5096;TOOLONG . <p> To create the file , use the- htpasswd- utility that came with Apache . This will be located in the- bin- directory of wherever you installed Apache . If you have installed Apache from a third-party package , @ @ @ @ @ @ @ @ @ @ create the file , type : <p> htpasswd -c **34;5124;TOOLONG rbowen <p> htpasswd- will ask you for the password , and then ask you to type it again to confirm it : <p> If- htpasswd- is not in your path , of course you 'll have to type the full path to the file to get it to run . With a default installation , its located at- **31;5160;TOOLONG <p> Next , you 'll need to configure the server to request a password and tell the server which users are allowed access . You can do this either by editing the- httpd.conf- file or using an- . htaccess- file . For example , if you wish to protect the **40;5193;TOOLONG , you can use the following directives , either placed in the file- **41;5235;TOOLONG , or placed in- httpd.conf- inside a &lt;Directory **42;5278;TOOLONG section . <p> Let 's examine each of those directives individually . The- AuthType- directive selects that method that is used to authenticate the user . The most common method is- Basic , and this is the method implemented by- modauthbasic . It is important to be aware , however @ @ @ @ @ @ @ @ @ @ to the server unencrypted . This method should therefore not be used for highly sensitive data , unless accompanied by- modssl . Apache supports one other authentication method:AuthType Digest . This method is implemented by- modauthdigest- and is much more secure . Most recent browsers support Digest authentication . <p> The- AuthName- directive sets the- Realm- to be used in the authentication . The realm serves two major functions . First , the client often presents this information to the user as part of the password dialog box . Second , it is used by the client to determine what password to send for a given authenticated area . <p> So , for example , once a client has authenticated in the- " Restricted Files " - area , it will automatically retry the same password for any area on the same server that is marked with the- " Restricted Files " - Realm . Therefore , you can prevent a user from being prompted more than once for a password by letting multiple restricted areas share the same realm . Of course , for security reasons , the client @ @ @ @ @ @ @ @ @ @ the hostname of the server changes . <p> The- AuthBasicProvider- is , in this case , optional , since- file- is the default value for this directive . You 'll need to use this directive if you are choosing a different source for authentication , such as- modauthndbm- or- modauthndbd . <p> The- AuthUserFile- directive sets the path to the password file that we just created with- htpasswd . If you have a large number of users , it can be quite slow to search through a plain text file to authenticate the user on each request . Apache also has the ability to store user information in fast database files . The- modauthndbm- module provides the- AuthDBMUserFile- directive . These files can be created and manipulated with the- dbmmanage- program . Many other types of authentication options are available from third party modules in the- Apache Modules Database . <p> Finally , the- Require- directive provides the authorization part of the process by setting the user that is allowed to access this region of the server . In the next section , we discuss various ways to use the- Require- @ @ @ @ @ @ @ @ @ @ specify that several criteria may be considered when trying to decide if a particular user will be granted admission . Satisfy can take as an argument one of two options - all- or- any . By default , it is assumed that the value is- all . This means that if several criteria are specified , then all of them must be met in order for someone to get in . However , if set to- any , then several criteria may be specified , but if the user satisfies any of these , then they will be granted entrance . <p> An example of this is using access control to assure that , although a resource is password protected from outside your network , all hosts inside the network will be given unauthenticated access to the resource . This would be accomplished by using the Satisfy directive , as shown below . <p> The directives above only let one person ( specifically someone with a username of- rbowen ) into the directory . In most cases , you 'll want to let more than one person in . This is @ @ @ @ @ @ @ @ @ @ to let more than one person in , you 'll need to create a group file that associates group names with a list of users in that group . The format of this file is pretty simple , and you can create it with your favorite editor . The contents of the file will look like this : <p> GroupName : rbowen dpitts sungo rshersey <p> That 's just a list of the members of the group in a long line separated by spaces . <p> To add a user to your already existing password file , type : <p> htpasswd **34;5322;TOOLONG dpitts <p> You 'll get the same response as before , but it will be appended to the existing file , rather than creating a new file . ( Its the- -c- that makes it create a new password file ) . <p> Now , you need to modify your- . htaccess- file or- &lt;Directory&gt;- block to look like the following : 
@@45151508 @5151508/ 55329 @qwx465329 <h> Category Archive : Windows Servers <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker I am going to have to start with a confession , " Resolved " might be a bit of a stretch . For starters , the best case scenario looks like recovering the information but you 'll need a temporary place where to store it , <p> How to : " Change product key " in Windows 8 or in Windows Server 2012 I 've learned over the years to not activate a Windows product until I am confident it is stable . I say that because I used to activate Windows right after installation and either because @ @ @ @ @ @ @ @ @ @ I had <p> How to : Enable Shadow Copy or Previous Version in Windows 2012 R2 ? I recently had a bit of an issue with a program . It uses an access database and one of the employees modified it but we needed to revert the changes . Seemed simple enough , just reach out to pick a previous version from the <p> Resolved : Passwords must meet complexity requirements Recently we were working on synchronizing passwords between an onPremise AD deployment and Azure Directory Services . As part of that we had to enable a strong password policy ( You can configure this security setting by opening the appropriate policy and expanding the console tree : Computer ConfigurationWindows SettingsSecurity SettingsAccount PoliciesPassword <p> Walkaround : Workfolders error 0x80c80037 No sync share is configured for the user . I am still trying to figure out why Workfolders works at times and then it complete stops working indicating the following error messages : Log Name : **39;5358;TOOLONG Source : **27;5399;TOOLONG Event I 'd : 4016 Description : No sync share is configured for the user . User : UserName ; <p> Resolve @ @ @ @ @ @ @ @ @ @ or Microsoft Updates on a fresh MultiPoint 2012 Server If you read my previous post that reads quite similarly to this then you know you get lots of 0x800B0100 error messages when you run Windows Update for the first time on a fresh MultiPoint 2012 <p> Resolved : Essentials Server Integration must be done on the Domain Controller Recently we performed a migration from our on premise Exchange server to Microsofts cloud solution : Exchange Online . Because of that I wanted to perform an integration at the domain controller level using Windows Server Essentials Experience . Unfortunately whether I tried using a member server <p> Resolve : Error 0x800B0100 when you try to install Windows Updates or Microsoft Updates I recently had to re-install a MultiPoint server and with over 80 updates I guess I should not be surprised something went wrong . I ended up trying to install one update at a time and I was successful in getting about half 
@@45151509 @5151509/ <h> General Commands Manual : dphys-swapfile <h> General Commands Manual : dphys-swapfile <h> NAME <h> SYNOPSIS <h> DESCRIPTION <p> dphys-swapfile computes the size for an optimal swap file ( and resizes an existing swap file if necessary ) , mounts an swap file , unmounts it , and and delete it if not wanted any more . <h> OPTIONS <p> There is only one parameter , an command , which can be either of these : setup Tells dphys-swapfile to compute the optimal swap file size and ( re- ) generate an fitting swap file . Default it 2 times RAM size . This can be called at boot time , so the file allways stays the right size for current RAM , or run by hand whenever RAM size has changed . swapon and swapoff These run the swapon and swapoff commands on the swapfile . Note that direct swapon/off from /etc/fstab is not possible , as that is ( at least on Debian ) done in the same script that mounts /var ( which is where the swap file most likely resides ) . And we need @ @ @ @ @ @ @ @ @ @ up /etc/fstab , and do our own swapon/off. uninstall Gets rid of an unwanted swap file , reclaiming the disk space . <h> CONFIG <p> The config file /etc/dphys-swapfile allows the user to set up the working environment for dphys-swapfile . This config file is a sh script fragment full of assignments , which is sourced . Standard sh syntax rules apply . Assignments are : CONFSWAPFILE Set where the swap file should be placed . Defaults to /var/swap . It is unlikely that you will need to change this , unless you have very strange partitioning , and then you will most likely be using an swap partition anyway . CONFSWAPSIZE Force file size to this . Default is 2*RAM size . This is unlikely to be needed , unless in strange diskspace situations . Note that swap enabled and smaller than RAM causes kernal-internal VM trouble on random systems . CONFSWAPFACTOR Set the relation between RAM and swap size . Must be an integer . Defaults to 2 which means swap size = 2 * RAM size CONFMAXSWAP Set maximum size of the swap file in MBytes . @ @ @ @ @ @ @ @ @ @ the swapfile size and is now a limit to prevent unusual big swap files on systems with a lot of RAM . <h> FILES <h> EXAMPLES <p> dphys-swapfile is usually run at system startup and shutdown from an /etc/init.d ( or /etc/rc.d ) script , such as this ( minimal ) one : # ! /bin/sh # **26;5428;TOOLONG - automatically set up an swapfile # author franklin , last modification 2004.06.04 # This script is copyright ETH Zuerich Physics Departement , # use under either modified/non-advertising BSD or GPL license case " $1 " in start ) /sbin/dphys-swapfile setup /sbin/dphys-swapfile swapon ; ; stop ) /sbin/dphys-swapfile swapoff ; ; esac exit 0 If an sysadmin wants to have his swapfile in annother place , say /var/run/swap , he can use : In /etc/dphys-swapfile : **26;5456;TOOLONG 
@@45151513 @5151513/ <p> The Joule , Dallas 1530 Main Street , - Dallas , - TX- 75201 Phone : - ( 214 ) 748-1300 Site : - http : **25;5484;TOOLONG My Rating : 4 1/2 stars Category : 4 Summary : Amazing . The service is very personal and candid , the rooms have nice details and they value spg members . The restaurant on site is good , valet parking is $27 . They have a house car ( runs from 7 AM to <p> Earn double Starpoints on every stay , and make it triple on stays including a Thursday or Sunday night . With the newest Starwood Preferred Guest- promotion , SPG- Triple Up , you 'll earn unlimited bonus Starpoints- faster when you register and stay from September 6 through December 18 , 2011 . Earn- triple Starpoints- on stays including a Thursday or Sunday <p> Marriott &amp; Hilton have announced summer sales , now is a great time to find a deal on your next summer getaway so hurry up ! Be sure to check each hotels website for their terms and conditions . Hilton : Save @ @ @ @ @ @ @ @ @ @ Resorts during The Great Getaway . Simply stay <p> US Open Suite Sweeps For every 2 nights in any SPG property ( consecutive or not ) now through 8/22/10 , you will earn 1 entry in the SPG US Open Suite Sweeps . Entrants have a chance to win one of 10 pairs of tickets to the SPG luxury suite for the men 's singles semifinals matches on 9/11/10 . <p> Free Nights at Aloft &amp; Element Now through 9/30/10 , - you can earn 1 free night award for every 3 stays at Aloft and Element hotels . Free night awards can be redeemed at Aloft and Element hotels now through 12/19/10 , 7 days a week . To qualify , register by 9/30/10 : https : **31;5511;TOOLONG 
@@45151516 @5151516/ <h> How to : Resolve error " Computer/Name Domain Changes . The following error occurred attempting to join the domain : The requested resource is in use . " <h> How to : Resolve error " Computer/Name Domain Changes . The following error occurred attempting to join the domain : The requested resource is in use . " <p> One of my latest missteps was not completing the migration of all servers to the new domain name when I performed an Active Directory Domain Name change . Well , one of the unintended consequences of doing this was that one of the servers that did not migrate to the new domain name remained with the old one , you could not log in using NLA ( Network Level Authentication ) as it would n't speak with the Domain Controller . At this point I was going to do as I was doing with the rest of the servers/workstations that did n't  get to migrate : Manually changing the domain name for them to pick the new settings . But guess what , this one particular server kept throwing error " @ @ @ @ @ @ @ @ @ @ join the domain : The requested resource is in use . " <p> This had worked perfectly for all our servers with no issues except for our Exchange Server . Now , I had already migrated all of our users / mailboxes to the cloud so this was just a glorified Exchange box . It was offline and I forgot about it so hence it did n't  get the new settings and once you complete the rendom commands computers which did n't  get rebooted and received those settings do n't  migrate over . So there is a lesson for free . <p> So , getting back to the issue of- " Computer/Name Domain Changes . The following error occurred attempting to join the domain : The requested resource is in use . " I did all the basic troubleshooting : Networking , AD connectivity , etc . At the end the issue became clear when I tried another set of credentials to join the computer to the new domain . I got a message indicating the computer was added originally with different credentials so I could n't use the new credentials @ @ @ @ @ @ @ @ @ @ be with the Computer Account in AD . I reset the account and even tried to disable it with no avail . Finally the solution : Join the domain with a new computer name . You could potentially just delete the old computer name from AD but I was afraid I might lose some settings as it is an Exchange Server so I kept it and if need be I will go in with ADsedit to retrieve that . <p> UPDATE : This turned out to be more complicated because it is an Exchange box . I was able to join it to the domain with the different name but it experiences all sort of issues with AD . When you install Exchange Server in a computer it gives it several permissions and references it a lot in AD so I am back to trying to solve the original problem using the same computer name . If anyone has any ideas please do share . Thanks ! 
@@45151517 @5151517/ <h> Tag Archive : MySQL <p> Resolved : hphp Warning : Parameter 1 to W3PluginTotalCache : : obcallback() expected to be a reference , value given in **26;5544;TOOLONG on line 3282 As you probably know ( otherwise you would probably not be here ) , after deciding to move form PHP to HHVM , if you are running a WordPress site and use W3 Total Cache you will encounter an endless <p> Resolved : Blank pages when using NginX with php-fpm As of late I have been a bit busy so keeping up with updates to the web server has been pretty much neglected . Because of that I decided to switch to the nginx.org supported distribution to get the latest updates although that means a distribution without any <p> How to : Install and use MySQLTuner to Optimize your MySQL configuration / performance I am writing this post mostly because there is in my opinion a much better way to install MySQLTuner on your machine that the usual apt-get install MySQLTuner . As many of you would probably already know MySQLTuner is a well known and <p> How @ @ @ @ @ @ @ @ @ @ Ubuntu server As part of a series of posts ( How to : Install the Memcached Agent for NewRelic in an Ubuntu server and- How to : Install the NginX Agent for NewRelic in an Ubuntu server ) I am including instructions on how to install the MySQL monitoring agent <p> How to : Have WordPress communicate with MySQL via Socket As I keep looking into how to improve the performance of website one of the recurrent points mentioned is to use Linux sockets where possible . I really do n't  have much experience and I can see how avoiding the TCP stack might help but I figured at <p> How to : Install , Update and Remove RubyGems I just had a brief experience installing a RubyGem so I thought I would share a little bit of what I learned about managing RubyGems . Similar to packages ( apt-get ) you can search through them , install them , remove them , and update them with simple commands . Below is a brief <p> How to : Get the size of your Databases in MySQL Query Browser Imagine you @ @ @ @ @ @ @ @ @ @ you need to know the size of the databases you are thinking of transfering . Unlike Microsoft SQL Server which graphical interface allows you to see the size of the database files <p> MySQL : How to list all available tables in the system Because sometimes you just need to know ( the available tables are out there so you can use that list in a script ) . If you are running MySQL Workbench you can execute the following SQL Query and export the results to a file : select tablename from <p> How to : Install MySQL in an Ubuntu Server If you are looking into deploying MySQL as part of a " LAMP " installation or whatever flavors you like best , Ubuntu is a good OS to do so . I say that because they make the installation super easy from the command line with- apt-get , it does n't  get much simpler <p> How to : Reset the Auto Increment value for a MySQL table Although this is something most people wo n't need to do , there are some scenarios where you want to reset @ @ @ @ @ @ @ @ @ @ my case for example , I am working with WordPress and the blog I 'd is set based 
@@45151518 @5151518/ 55329 @qwx465329 <h> Category Archive : Thread Management Server <p> How to : Configure Remote SQL Server Logging for Microsofts Thread Management Gateway ( TMG ) 2010 Microsofts Thread Management Gateway is a solution that aims at providing advanced firewall and reverse proxy capabilities to the enterprise . Unfortunately Microsoft wont be releasing new versions in the future so I have n't been writing much about it lately ; However , many <p> Obtained from : LONG ... How to Configure TMG for Office 365 ( Exchange ) Hybrid deployments The purpose of this article to give some general guidance on how to configure TMG for use with Office 365 Exchange related components . The idea is to give some general guidance mainly around authentication settings needed on the TMG rule <p> How to : Make my DNS server resolve my WPAD entry for my proxy server and what is the DNS Block List ? Ive been working on setting up my corporate network to use Microsofts TMG proxy in order to better handle the traffic ( restricting sites , etc ) . And while trying to configure the proxys @ @ @ @ @ @ @ @ @ @ 2010 with Forefront UAG and Forefront TMG I 've been trying to publish Exchange Server 2013 with Forefront TMG with no avail . However , I did find a good guide on how to publish Exchange Server 2010 with TMG so I thought I would share . I cant find the link to the <p> How to : Use the Certificate Enrollment MMC in the TMG host machine Behavior : When you are using the Certificate MMC snap-in and/or try to perform- a certificate auto-enrollment in your localhost/TMG server you 'll most likely run into an error message- on-screen- that reads " RPC- failure " . If you try requesting a certificate on other computers joined to your <p> How to Configure TMG logging to use a central SQL Server store The default log settings for Forefront TMG are set to a local Microsoft SQL Server Express 2008 SP1 database . During the Forefront TMG installation a local Microsoft SQL Server Express database will be installed . If you want to change this local SQL Server <p> How to resolve common HTTPS inspection issues while using Microsoft TMG 2010 While working with @ @ @ @ @ @ @ @ @ @ Internet Security and Acceleration server ) https inspection is a big new component of it . If you ever look through your logs and you find https-inspection as the protocol that is causing <p> Publishing Outlook Web Access ( OWA ) using Microsofts Threat Management Gateway ( TMG ) Publishing Outlook Web Access ( OWA ) is a usual step in the enterprise . You want to be able to provide access to your corporate emails via a web interface users can access anywhere with an Internet Connection . Below are some steps and recommendations for making <p> Using Exchange 2010 Autodiscovery with Microsofts Forefront Threat Management Gateway After working with Windows Small Business Server and Threat Management Gateway one of the challenges you come across is what kind of policies do you need to set it place to allow access to Exchange 2010 Web services for your corporate users . Well , it all <p> Description : Forefront TMG detected changes in Microsoft Exchange Server or Microsoft Forefront Protection configuration , and reapplied the e-mail policy configuration . After reading about it this issue is due at times because @ @ @ @ @ @ @ @ @ @ Microsoft has also released a hotfix which you can use to address it . In my 
@@45151519 @5151519/ 55329 @qwx465329 <h> Tag Archive : Sigmatel Audio <p> What is " Andrea ST Filters Service " ( AESTFilters ) ? Many people wonder what is " Andrea ST Filters " windows service when going- through the list of processes that were running on their machine . I was one of them as I was trying to find out what is causing it to run so slowly and one of the processes that was 
@@45151522 @5151522/ <h> Google Sync wo n't sync correctly my contacts on my iPhone <p> One of the weirdest issues I-ve faced as of late is that my Google Contacts were not being brought in correctly to my iPhone . What was really going on behind the scenes was that all the contacts instead of being imported to my iPhone as First Name &amp; Last Name they were coming in as Nicknames . You can imagine what kind of issues that caused me . Because the names were being brought- in as Nicknames , iOS is unable to sort the names correctly . So I would have some of my contacts sorted the way I wanted and others would show up sorted by First Name not Last Name . This was a pain . <h> The Cause <p> Who knows , a lazy developer perhaps ? . All I know is that I use Google Sync- to get my email , contacts and calendars over to my iPhone . I am disappointed that I can only bring in my default contacts as I do like to group them on Google but that 's @ @ @ @ @ @ @ @ @ @ option for my account but that would only bring over my emails , which is not doing much for me . All I could guess at this point is that Google Sync is broken and I needed an alternate method to get my contacts over into my iPhone . <h> Resolution <p> Google has come up with a new feature that allows you to sync your Google Contacts via CardDav . Using CardDav resolved my issues as now my Contacts come in as First Name , Last Name &amp; Nickname so I can sort them the way I wanted it . 
@@45151524 @5151524/ 55329 @qwx465329 <h> Tag Archive : Ubuntu <p> How to : Create directory in /var/run/ at start-up in Ubuntu As you probably have already noticed , the /var/run directory is temporary storage used by your Ubuntu system . It is mapped to your RAM disk ( Ubuntu uses I think about 10% of your RAM and creates a disk mount for it . This is used for folders <p> How to : Make a directory structure in Ubuntu only if the directories do not exist Lately I have been working on start up scripts but because I use temporary storage for some mounts the information contained within them is cleared with each reboot ( shutdown / turn on Windows Azure ) . This means that the folder <p> Resolved : Ubuntu php5-fpm throws error " unknown instance " on service reload Rather short post this time around . It turns out there is a known issue that when you perform a service reload on php5-fpm you might encounter yourself with a bit of a problem . The use of reload is meant to result in a reloading of <p> @ @ @ @ @ @ @ @ @ @ MySQL configuration / performance I am writing this post mostly because there is in my opinion a much better way to install MySQLTuner on your machine that the usual apt-get install MySQLTuner . As many of you would probably already know MySQLTuner is a well known and <p> How to : Install the MySQL Agent for NewRelic in an Ubuntu server As part of a series of posts ( How to : Install the Memcached Agent for NewRelic in an Ubuntu server and- How to : Install the NginX Agent for NewRelic in an Ubuntu server ) I am including instructions on how to install the MySQL monitoring agent <p> How to : Configure Swappiness in Ubuntu In one of my previous articles on how to setup virtual memory / swap memory in Ubuntu I covered how to install a swap space for a Windows Azure VM . I deployed swap mostly as a precaution against running out of memory which could result on applications unable to <p> How to : Install the Memcached Agent for NewRelic in an Ubuntu server Everytime I try to install a plugin for New @ @ @ @ @ @ @ @ @ @ perform each of the steps . For example , take this instructions on how to install the Memcached agent to get monitoring on your web server : <p> How to : Install Ruby in an Ubuntu Server to use it with NewRelics Plugins : Memcached &amp; NginX Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . Also , most of them require other applications to be installed as well like Ruby <p> How to : Add or Remove Symbolic links in Ubuntu Unlike Windows , in Linux based systems symbolic links are use quite frequently . The advantage of using symbolic links is that they allow you to have the same file in multiple locations , but the content to remain the same across all of them regardless of where you 
@@45151526 @5151526/ 55329 @qwx465329 <h> Tag Archive : SSL <p> How to : Move your NginX website to HTTPs- / SSL It comes at no surprise that a lot of people are looking into moving their sites to HTTPs due to recent events : Googles decision to give ranking points to sites that use SSL / HTTPs and eavesdropping by governments world wide . There are a number <p> How to : Create a Self Signed Certificate in Ubuntu Many- times during the initial phases of a Web Server deployment we are in need of testing secure communications ( and its configuration ) using a certificate . Unfortunately we might not have a valid certificate from a certification authority at the time . There are also other scenarios were you <p> Changes with- NginX- 1.7 Below are the list of changes from the 1.5.13 release of NginX to version 1.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several new Features- so updating to this version is not considered urgent/critical . Changes with nginx 1.7.0 24 Apr 2014 * @ @ @ @ @ @ @ @ @ @ <p> How to : Improve SSL performance on NginX You would be surprised but a lot of people face SSL performance issues when using NginX . I recently deployed SPDY over SSL for my sites and came to realize that SPDY was in fact much slower than using standard HTTP . I proceeded to leave SSL alone and see <p> Resolved : Microsoft Office has detected a problem with your Information Rights Management configuration . About a week ago one of our client computers started facing issues regarding the ability to access the Information Rights Management IRM server . We use Windows Azures IRM service ( Azure Rights Management ) so there are a few things you need to do <p> What is : NginX Lately I have been writing a lot about NginX so I thought I would write a small post to describe NginX and what it is.Although I like to write it NginX because its origins are in the Linux world they write it all in lower case nginx ( I know , boring right ) . The <p> Changes with NginX 1.4.5 Below are the list @ @ @ @ @ @ @ @ @ @ version 1.4.5 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.5 11 Feb 2014 * ) Bugfix : the $sslsessionid variable contained full session serialized instead of just a session i 'd . Thanks to <p> Changes with- NginX- 1.5.10 Below are the list of changes from the 1.5.9 release of NginX to version 1.5.10 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.10 04 Feb 2014 * ) Feature : the ngxhttpspdymodule now uses SPDY 3.1 protocol . Thanks to Automattic and MaxCDN for sponsoring this work . * ) Feature : <p> How to : Redirect an URL in NGINX One might need to redirect to another URL in a number of scenarios . I will cover two examples for scenarios I have come across and how I addresses those issues : I. Redirect from a naked domain to the www subdomain : server listen 80 ; servername test.com ; return 301 <p> Custom NginX Distribution Available Packages As part of the custom NginX distribution available on @ @ @ @ @ @ @ @ @ @ chose from depending on your needs that can be deployed . Below is the list of packages , additional information and description : Different packages available : Package : - nginx Architecture : all Depends : nginx-full nginx-light , This 
@@45151530 @5151530/ <h> How to : Launch a Process that Persists even if you Disconnect your ssh Terminal <h> How to : Launch a Process that Persists even if you Disconnect your SSH Terminal <p> I 'm not sure if this has happened to you before , but there are times when you have an application you wish to launch but if you disconnect from the terminal ( SSH or physical ) that process gets terminated . For example , currently I am running a proxy application that displays to the screen the status ( connections to the upstream servers , work load , connection to the downstream clients , etc . ) . I want to be able to disconnect without causing it to be shut off . In order to achieve this , I am looking at using a terminal multiplexer . What this does is ti allows a user to access multiple separate terminal sessions inside a single terminal windows ( physi8cal or remote like SSH . ) This opens a new set of possibilities . If you are like me and sometimes end up with several SSH connections @ @ @ @ @ @ @ @ @ @ ( thinking of- tail -f ) then what a multiplexer gives you is the ability to do so all in one connection . Thus far my favourite multiplexer is- screen- which comes built in with Ubuntu . Below I 'll get into the details on how to use it : <p> Screen- is a- terminal- multiplexer , which allows a user to access multiple separate terminal sessions inside a single terminal window or remote terminal session ( such as when using- SSH ) . <h> Installation <p> The screen package can be installed on Ubuntu using apt-get or any other method you prefer . As far as I know it already comes installed with the latest versions ( at least I have n't needed to install it for some reason ) <p> Starting with the Jaunty release , the screen-profiles package ( later renamed Byobu ) provides advanced features such as status bars , clocks , and notifiers . The package can also be manually installed on previous Ubuntu releases . <h> Usage <p> Screen can be started by typing <p> screen <p> in a terminal . Press Enter after reading the @ @ @ @ @ @ @ @ @ @ manipulated by pressing the Ctrl+A key combination , and subsequently pressing a key to execute one of the commands given below : <p> c creates a new virtual console <p> n switches to the next available virtual console <p> p switches back to the previous virtual console <p> " lists all available virtual consoles and their assigned numbers <p> hitting a number key brings the corresponding virtual console to the foreground <p> Esc let 's you scroll back and forth in your terminal output <p> d detaches the current screen sessions and brings you back to the normal terminal <p> When a Screen session is detached , the processes that were running inside it are n't  stopped . You can re-attach a detached session by typing <p> screen -r <p> in a terminal <p> To remove the annoying copyright notice at startup , edit your /etc/screenrc with <p> gksudo gedit /etc/screenrc <p> and remove the hash which begins the line <p> #startupmessage off <p> Save the file , and you will not see it again- <p> For further information and more advanced commands , you can refer to the screenman page . 
@@45151531 @5151531/ 55329 @qwx465329 <h> Changes with NginX 1.4.6 Moderate <p> Below are the list of changes from the 1.4.5 release of NginX to version 1.4.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update mainly provides- bug-fixes which are not critical . <p> Changes with nginx 1.4.6 04 Mar 2014 * ) Bugfix : the " clientmaxbodysize " directive might not work when reading a request body using chunked transfer encoding ; the bug had appeared in 1.3.9 . Thanks to Lucas Molas. * ) Bugfix : a segmentation fault might occur in a worker process when proxying WebSocket connections . 
@@45151532 @5151532/ 55329 @qwx465329 <h> Category Archive : Man Pages <p> General Commands Manual : dphys-swapfile NAME dphys-swapfile set up , mount/unmount , and delete an swap file SYNOPSIS dphys-swapfile **27;5572;TOOLONG DESCRIPTION dphys-swapfile computes the size for an optimal swap file ( and resizes an existing swap file if necessary ) , mounts an swap file , unmounts it , and and delete it if not wanted any more . OPTIONS There is only <p> General Commands Manual : dpkg-buildpackage NAME dpkg-buildpackage build binary or source packages from sources SYNOPSIS dpkg-buildpackage option DESCRIPTION dpkg-buildpackage is a program that automates the process of building a Debian package . It consists of the following steps : 1 . It prepares the build environment by setting various environment variables ( see ENVIRONMENT ) and calls dpkg-source before-build ( unless <p> How to : Find a file in Ubuntu- using complex criteria- - ( or not ) Sometimes you need to find a file and all you know is the extension or the beginning of the file name . A powerful flexible search to look in all sub directories for files that match a criteria is important . Ubuntu comes with such a command 
@@45151533 @5151533/ 55329 @qwx465329 <h> Tag Archive : Regular expression <p> How to : Override a Location directive on NginX Sometimes when coding you are in need to re-use a lot of the logic across different systems but find yourself needing to overwrite some of that functionality on a special case . In this particular scenario I needed to overwrite a Location directive on NginX . What do I <p> How to : Find a file in Ubuntu- using complex criteria- - ( or not ) Sometimes you need to find a file and all you know is the extension or the beginning of the file name . A powerful flexible search to look in all sub directories for files that match a criteria is important . Ubuntu comes with such a command <p> How to : Use Regex to create instructions out of a list of inputs Using Notepad++ : Use Regex to identify the inputs you want to put inside a command : ( bd1,3.d1,3.d1,3.d1,3b ) Then Replace it with the command identifying the matched Regex on the search : &lt;add ipAddress= " 1 ? allowed= " false " /&gt; 
@@45151534 @5151534/ <h> SQL Server 2012 : How to copy a database from one server to another using the Copy Database Wizard <p> I was provisioning my SQL box when I found I didn-t need an already provisioned Windows Server box I had used for Exchange 2013 . Lazy enough I installed SQL Server 2012 on my already installed Windows box . Later on I learned that Exchange leaves things behind when you uninstall it and I figured it would be better to start with a fresh Windows Installation for SQL . Long story short , that is when I needed a method to transfer all my databases to my new server without too much struggle . I used to backup my databases and then restore them on the destination server but things like server logins usually result in some manual work and if you are not careful something might not work right off the bat . Clearly I wanted a better method to transfer my databases to another server : The Copy Database Wizard ! <h> How to Use the Copy Database Wizard <p> The Copy Database Wizard allows you @ @ @ @ @ @ @ @ @ @ also used to upgrade databases from a previous version Server to SQL 2012 . Remember that using this method you can copy associated metadata , for example , logins and objects from the master database that are required by a copied database . <p> Here are some things to keep in mind : <p> I recommend using the Copy options rather than Move . After you are done copying you can take the database offline if you want . Just keep in mind moving does delete the database from the source server after completion . <p> You must be a member of the sysadmin fixed server role on both the source and destination servers . <p> Here are the steps to follow : <p> Open SQL Server Management Studio <p> Go to Object Explorer <p> Expand Databases <p> right-click a database you want to copy or move <p> under Tasks click Copy Database . <p> From the Select a Source Server page , specify the server with the database to move or copy , and to enter login information . After you select the authentication method and enter login information @ @ @ @ @ @ @ @ @ @ server . This connection remains open throughout the session . <p> Select the name of the server on which the database or databases you want to move or copy are located , or click the browse ( ) button to locate the server you want . The server must be at least SQL Server 2005 . <p> From the Select a Destination Server page , specify the server where the database will be moved or copied . If you set the source and destination servers to the same server instance , you will make a copy of a database . In this case you must rename the database at a later point in the wizard . The source database name can be used for the copied or moved database only if name conflicts do not exist on the destination server . If name conflicts exist , you must resolve them manually on the destination server before you can use the source database name there . <p> Detach the database from the source server , copy the database files ( . mdf , . ndf , and . ldf ) to @ @ @ @ @ @ @ @ @ @ destination server . This method is usually the faster method because the principal work is reading the source disk and writing the destination disk . No SQL Server logic is required to create objects within the database , or create data storage structures . This method can be slower , however , if the database contains a large amount of allocated but unused space . For instance , a new and practically empty database that is created allocating 100 MB , copies the entire 100 MB , even if - - - - - Note <p> This method makes the database unavailable to users during the transfer . <p> When a database is copied , the original database files are always reattached to the source server . Use this box to reattach original files to the source database if a database move can not be completed . <p> This method reads the definition of each database object on @ @ @ @ @ @ @ @ @ @ database . Then it transfers the data from the source tables to the destination tables - - - - - Note <p> Database users can continue to access the database during the transfer . <p> From the Select Database page , select the database or databases you want to move or copy from the source server to the destination server . See Limitations and Restrictions in the Before You Begin section of this topic . <p> From the Configure Destination Database page , change the database name if appropriate and specify the location and names of the database files . This page appears once for each database being moved or copied . <p> From the Select Database Objects page , select the objects to include in the move or copy operation . This page is only available when the source and destination are different servers . To include an object , click the object name in the Available @ @ @ @ @ @ @ @ @ @ to move the object to the Selected related objects box . To exclude an object , click the object name in the Selected related objects box , and then click the &lt;&lt; button to move the object to the Available related objects box . By default all objects of each selected type are transferred . To choose individual objects of any type , click the ellipsis button next to any object type in the Selected related objects box . This opens a dialog box where you can select individual objects . <p> From the Location of Source Database Files page , specify a file system share that contains the database files on the source server . This is required if the source and destination server instances are on different computers . <p> Provide a path for the location of the log file . This option is only available if the text file logging option is selected . <p> From the Schedule the Package page , specify when you want the move or copy operation to start . If you are not a system administrator , you must specify a SQL @ @ @ @ @ @ @ @ @ @ Services ( SSIS ) Package execution subsystem . <p> Select an available proxy account . To schedule the transfer , there must be at least one proxy account available to the user , configured with permission to the SQL Server Integration Services package execution subsystem.To create a proxy account for SSIS package execution , in Object Explorer , expand SQL Server Agent , expand Proxies , right-click SSIS Package Execution , and then click New Proxy.Members of the sysadmin fixed server role can select the SQL Server Agent Service Account , which has the necessary permissions . <p> From the Complete the Wizard page , review the summary of the selected options . Click Back to change an option . Click Finish to create the database . During the transfer , the Performing operation page monitors status information about the execution of the Copy Database Wizard . <p> After you use the Copy Database Wizard to upgrade a database from an earlier version of SQL Server to SQL Server 2012 , the database becomes available immediately and is automatically upgraded . If the database has full-text indexes , the upgrade @ @ @ @ @ @ @ @ @ @ depending on the setting of the Full-Text Upgrade Option server property . If the upgrade option is set to Import or Rebuild , the full-text indexes will be unavailable during the upgrade . Depending the amount of data being indexed , importing can take several hours , and rebuilding can take up to ten times longer . Note also that when the upgrade option is set to Import , if a full-text catalog is not available , the associated full-text indexes are rebuilt . For information about viewing or changing the setting of the Full-Text Upgrade Option property , see Manage and Monitor Full-Text Search for a Server Instance.If the compatibility level of a user database was 90 or 100 before upgrade , it remains the same after upgrade . If the compatibility level was 80 or less before upgrade , in the upgraded database , the compatibility level is set to 90 , which is the lowest supported compatibility level in SQL Server 2012 . For more information , see ALTER DATABASE Compatibility Level ( Transact-SQL ) . 
@@45151535 @5151535/ <p> So as part of the latest using your own DNS server with Linux machines inside Windows Azure I had a bit of a problem . Even though my corporate DNS server was set up correctly , DNS entries were correct , etc . Ubuntu would just take for ever to resolve the name . The idea is that if you ping myWebServer it would resolve automatically to **29;5601;TOOLONG and then ping the host but it would take like 30 seconds to finally decide to try the dns-search value I set up at /etc/network/interfaces . I finally realized what was the problem when I opened /etc/resolv.conf . I was suspecting it was trying other domains for search first and then when they timed out- mine would get used . Take a look at the file : <p> As you can see the Azure guys via DHCP decided to add a dns-search parameter of **35;5632;TOOLONG which comes before my DNS Zone of **30;5669;TOOLONG ! So if I wanted to ping MySQLServer it would first try LONG ... and then **42;5701;TOOLONG . Effectively until the first name resolution fails will it @ @ @ @ @ @ @ @ @ @ this can have an impact ! So lesson learned , always use FQDNs , lol . Anyway , there is a way to fix this in case you really like to just type MyServer and be done . <p> Because this " error " - is caused by DHCP for listening to those silly Microsoft guys we need to tell the DHCP client to be smarter than that . To set resolv.conf to what- you need- it to be regardless of what the DHCP server says you need to edit - /etc/dhcp/dhclient.conf and play with the following : <p> supersede domain-name " **30;5745;TOOLONG " ; <p> prepend domain-search " **30;5777;TOOLONG " ; <p> So here is how it works : <p> Use Supersede if you want to have a value overwrite whatever the DHCP server sends ( if it does n't  provide the value then it wont be superseded . <p> Use Prepend if you want to add a value before whatever the DHCP server sends . Remember that if you just wanted to add something you should use /etc/network/interfaces or **30;5809;TOOLONG where you can find the base and head @ @ @ @ @ @ @ @ @ @ /etc/resolv.conf while head to the top of /etc/resolv.conf . <p> That 's all , happy domain name resolving ! ( Do n't  forget to have the resolv.conf regenerated you need to run the dhcp client again and the easiest way is executing /etc/init.d/networking restart or service networking restart ) <p> Here is a sample of the changes I mad to the /etc/dhcp/dhclient.conf- file with the modifications in bold : <p> # Configuration file for /sbin/dhclient , which is included in Debians <p> # dhcp3-client package . <p> # <p> # This is a sample configuration file for dhclient . See dhclient.confs <p> # man page for more information about the syntax of this file <p> # and a more comprehensive list of the parameters understood by <p> # dhclient . <p> # <p> # Normally , if the DHCP server provides reasonable information and does 
@@45151538 @5151538/ 55329 @qwx465329 <h> Tag Archive : $ssl session i 'd <p> Changes with NginX 1.4.5 Below are the list of changes from the 1.4.4 release of NginX to stable version 1.4.5 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.5 11 Feb 2014 * ) Bugfix : the $sslsessionid variable contained full session serialized instead of just a session i 'd . Thanks to 
@@45151539 @5151539/ 55329 @qwx465329 <h> How to : Configure memcache to use a unix socket <h> How to : Configure memcache to use a unix socket <p> I have a server which is going to have its own memcached instance installed on itself so I was thinking using a unix socket instead of tcp should improve the overall performace at least a little . I had tried doing the same with MySQL and I was pleased on the improved speed if only a few ms ( it adds up I think ) . Well , with no further due here it is : <p> First , you need to edit the main configuration file : - # nano /etc/memcached.conf <p> # Start with a cap of 64 megs of memory . Its reasonable , and the daemon default # Note that the daemon will grow to this size , but does not start out holding this much # memory -m 64 <p> # Default connection port is 11211-p 11211 <p> # Run the daemon as root . The start-memcached will default to running as root if no # -u command is @ @ @ @ @ @ @ @ @ @ which IP address to listen on . The default is to listen on all IP addresses # This parameter is one of the only security measures that memcached has , so make sure # its listening on a firewalled interface. -l 127.0.0.1 <p> # Limit the number of simultaneous incoming connections . The daemon default is 1024 # -c 1024 <p> # Lock down all paged memory . Consult with the README and homepage before you do this # -k <p> The main aspects we are going to look is the port , listening port , socket , and octal file mode : <p> Comment out the default port like so : <p> # -p 11211 <p> Comment out the IP address option : <p> # -l 127.0.0.1 <p> Add the following line to indicate you want to use a socket : <p> -s /var/run/memcached.sock <p> Optionally you could configure the octal file mode of the socket using the -a paramter : <p> -a 0766 <p> Note that the two TPC options -l ( listening ip ) and -p ( port ) were commented out and we introduced socket specific @ @ @ @ @ @ @ @ @ @ file mode ) . <p> Furthermost note that we used the /var/run folder to avoid worrying about creating a special directory for memcache . It is though a good idea to establish a dedicated space to separate all memcache sockets and use a name that indicates the instance on the configuration and socket files . This though requires a little more work . After deciding on your nomenclature and directory we are going to configure the init.d script to create said folder if it is not available . Edit the memcached init.d script and find the following lines were you will insert the bold code : 
@@45151540 @5151540/ <h> Resolved : Passwords must meet complexity requirements <h> Resolved : Passwords must meet complexity requirements <p> Recently we were working on synchronizing passwords between an onPremise AD deployment and Azure Directory Services . As part of that we had to enable a strong password policy ( You can configure this security setting by opening the appropriate policy and expanding the console tree : Computer ConfigurationWindows SettingsSecurity SettingsAccount PoliciesPassword Policy ) which resulted in lots of " Passwords must meet complexity requirements " errors . <p> Reading and lurking around we found that a strong password policy is defined on a Windows Server as a policy that at a minimum complies with the following requirements : <p> The samAccountName is checked in its entirety only to determine whether it is part of the password . If the samAccountName is less than three characters long , this check is skipped . <p> The displayName is parsed for delimiters : commas , periods , dashes or hyphens , underscores , spaces , pound signs , and tabs . If any of these delimiters are found , the displayName is split and @ @ @ @ @ @ @ @ @ @ be included in the password . Tokens that are less than three characters in length are ignored , and substrings of the tokens are not checked . For example , the name " Erin M. Hagens " is split into three tokens : " Erin , " " M , " and " Hagens . " Because the second token is only one character long , it is ignored . Therefore , this user could not have a password that included either " erin " or " hagens " as a substring anywhere in the password . <p> Passwords must contain characters from three of the following five categories : <p> Uppercase characters of European languages ( A through Z , with diacritic marks , Greek and Cyrillic characters ) <p> Lowercase characters of European languages ( a through z , sharp-s , with diacritic marks , Greek and Cyrillic characters ) <p> Base 10 digits ( 0 through 9 ) <p> Nonalphanumeric characters : ! @#$%&amp;*-+= ( ) : ; " &lt;&gt; , . ? / <p> Any Unicode character that is categorized as an alphabetic character but is @ @ @ @ @ @ @ @ @ @ Asian languages . <p> We actually got a randomly generated password that to our best knowledge complied with all the policies . So what were we doing wrong ? After several ties and failures , we concluded that the issue was the password length . This seems ironic as the longer the password is the safest it results but apparently there are limits . We were using 25 characters and then 30 characters in order to provide several of the required character categories multiple times . <p> We finally resolved that a password can not be longer than 16 characters in length when using Azure Directory Services via Windows Essentials Experience . I am actually not sure if that applies as well in regular scenarios or if it is particular of this method . We generally recommend long passwords for service accounts and I do n't  remember encountering this issue before . The one thing that blows my mind is why you get the message " Passwords must meet complexity requirements " when really the problem is the length not its complexity . <p> When you install Windows Server Essentials @ @ @ @ @ @ @ @ @ @ a wizard that at some point informs you that in order to proceed with the sync you need to enable the strong password policy . At that point possibly the restriction is put in place but I 'm not sure if it was there all along or not . 
@@45151541 @5151541/ <p> I have been working with Ubuntu more lately and ran into the need to direct traffic going to one server ( via IP ) to go to a new server but I could n't change the clients configuration . Because part of the services were already migrated over I could not change the IP address to the old server . Because of this I required a way for the new server to answer on the new IP address assigned to it but also listen to the old servers IP address . To do this you need your server to have more than one IP address which is possible ( have done it with Windows Servers for several years ) so just needed to find out how to do it on Ubuntu <h> Solution <p> If you need an additional IP address just for the moment you can add it to any interface on your machine with <p> sudo ip address add **34;5841;TOOLONG dev &lt;interface&gt; <p> for- example <p> sudo ip address add 172.16.100.17/24 dev eth0 <p> would add 172.16.100.17 using a 24bit netmask to the list of addresses configured @ @ @ @ @ @ @ @ @ @ with <p> ip address show eth0 <p> and you can delete this address again with <p> sudo ip address del 172.16.100.17/24 dev eth0 <p> Of course these changes are lost when you reboot your machine . <p> To make the additional addresses permanent you can edit the file /etc/network/interfaces by adding as many stanzas as needed of the form : <p> Note we left the dhcp on the first line , so you will end with 3 ip addresses , 2 static ones provided above and one assigned to you by the DHCP server which will be your primary address . <p> To activate these settings without a reboot use ifdown/ifup likeso : <p> sudo ifdown eth0 &amp;&amp; sudo ifup eth0 <p> It is essential to put those two commands into one line if you are remoting into the server because the first one will drop your connection ! Given in this way the ssh-session will survive and youll be able to reconnect . Exercise caution as if there is an error networking will be dropped but because of the config error it wo n't come back up . 
@@45151544 @5151544/ 55329 @qwx465329 <h> Resolved : Blank pages when using NginX with php-fpm <h> Resolved : Blank pages when using NginX with php-fpm <p> As of late I have been a bit busy so keeping up with updates to the web server has been pretty much neglected . Because of that I decided to switch to the nginx.org supported distribution to get the latest updates although that means a distribution without any of the extra plugins I 've come to use . I decided this was an acceptable compromise due primarily to time issues . So pretty much there is where my problems began . I had a fully functional installation and all of a sudden after I switched distributions I got a blank screen . I tried everything : setting permissions again , monitoring the logs for errors , configuring again Nginx and PPH-FPM . Everything checked , the status pages , the ping stubs , the logs showed no errors so this was quite the mystery . There were no indications as to what would result in blank pages being shown yet no errors on Nginx or PHP-FPM . @ @ @ @ @ @ @ @ @ @ found the issue : The fastcgi.conf file was missing the definition of- PATHTRANSLATED . Apparently the previous distribution I was using had included this definition in the file but the nginx.org distribution has two fastcgi configuration files ( fastcgiparams- or fastcgi.conf ) <p> and one of them is missing this definition . It was just a matter of adding- the following line and you have yourself a functioning php site once again : <p> fastcgiparam PATHTRANSLATED **31;5877;TOOLONG ; <p> I hope this helps others . Seems like a really silly thing but without it PHP-FPM does not have the required configuration to be able to function properly . <h> dax <p> This did just help me thank you so much ! Got my site back up and running ! For reference , I was upgrading from nginx 1.6.2 in the default Debian 8 jessie repositories , to 1.12 stable from the nginx.org repositories . I was just about to restore my DigitalOcean snapshot , when you got me back up ! Thanks again ! 
@@45151545 @5151545/ 55329 @qwx465329 <h> . Net : Supported Console Colors <h> . Net : Supported Console Colors <p> The days of writing applications that write to the console seem to be over for a while now . Notwithstanding , we find ourselves using the console from time to time to help us debug or quickly test application logic . In my case , I am currently using the console to monitor logging information from my application . It is terribly useful to be able to see progress as it happens . It sort of reminds me of using tail -f in Ubuntu against a log file . As part of my effort to make this log reading more easy I decided I wanted to color code the messages based on what they were : Errors / Warnings / Info / Debug , etc . I kept asking myself if Windows is able to support colored console as Ubuntu does , and I am happy to report it does ! So here is when the question arises : 
@@45151547 @5151547/ <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got @ @ @ @ @ @ @ @ @ @ ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <p> How to : Configure Dynamic DNS Service on the Unifi Security Gateway via config.gateway.json Thankfully the latest versions of Ubiquitis Unifi Controller allow the user to configure dynamic DNS services via the Web UI . However , there are cases when you rather keep the configuration on the config.gateway.json file . In my case the main motivation to use <p> Google IP address ranges Recently I decided to try the Google Compute Engine Cloud , and one of the things I liked right from the beginning was the ability to connect via SSH to your Ubuntu box . I am going to have to read a bit more about it but basically it seems like it generates <p> Resolved : The " **37;5910;TOOLONG " task could not be loaded from the assembly I recently have been working on an @ @ @ @ @ @ @ @ @ @ build errors that although they were not stopping the build and allowed me to continue with the applications execution , they were starting to get quite annoying . This <p> How to : Get the Sum of the Values from List in C#.Net ? Currently I am working on a new project in which I need to calculate some statistics that involve some basic statistics which require the total amount . The easy way out is just to do a foreach statement and iterate through the list adding <p> . Net : Supported Console Colors The days of writing applications that write to the console seem to be over for a while now . Notwithstanding , we find ourselves using the console from time to time to help us debug or quickly test application logic . In my case , I am currently using the console to monitor logging information 
@@45151549 @5151549/ <h> How to : Clear or Flush DNS cache on a Mac OS X computer <h> How to : Clear or Flush DNS cache on a Mac OS X computer <p> Most operating systems- implement a local DNS cache for a number of reasons : lower the load on DNS servers , faster domain name resolution , etc . This is great for a number of reasons , but it also poses a problem from time to time . Most users access sites that are robust and their IP addresses do n't  change , but for developers or systems administrators , when working on testing or development environments a change in hosts or ip addresses is not uncommon . So if you are playing with your web hosts and you need your web browser to start talking to a different web host then you need to clear or flush that DNS cache . Also noteworthy is that most operating systems offer a local " hosts " file that serves as a simple name resolution override . So even if the DNS server you are using resolves to a particular @ @ @ @ @ @ @ @ @ @ hosts file . Because it is a lightweight and fast alternative , it is very simple . <p> If you come from a Windows background you can clear/flush the DNS cache through one simple command that has been around for ages ( at least a solid decade ) : ipconfig /flushdns . I must confess I was hoping an ifconfig /flushdns on my mac would do the trick but not . In the case of OS X there seems to be a myriad- of options , depending on which version of the operating system you are using . I say this because through time I am always in need to flush my DNS cache and I am always learning a new / different way and previous methods stop working on newer versions . <h> The Commands <p> Pre-snow leopard you can run this command line on your terminal ( Mac OS X versions 10.3 and 10.4 ) : - lookupd -flushcache . <p> From Mac OS X versions 10.5 and 10.6 you can use the following instruction : - dscacheutil -flushcache <p> For OSX versions 10.7 and 10.8 we have @ @ @ @ @ @ @ @ @ @ around ) : - sudo killall -HUP mDNSResponder <p> And now , for OS X version 10.9 we got a new twist again : let 's do what we did back in 10.8 and 10.6 at the same time : - dscacheutil -flushcache ; sudo killall -HUP mDNSResponder <p> Finally , For OS X 10.10 Yosemite we 've got an even more complicated set of commands : - sudo discoveryutil mdnsflushcache ; sudo discoveryutil udnsflushcaches <p> So now we have a new command called discoverutil and we must perform an mdns cache flush and a urns cache flush in order to effectively flush all DNS caches . <h> How to check the cache stats <p> Sometimes you need to check to make sure the cache has been flushed or sometimes you are just curious to see how is the DNS cache performing . The command discoveryutil has a few options to check on those two caches we spoke of before : - udnscachestats and- mdnscachestats . By using these two we are able to obtain statistics such as these : <p> Those statistics will show you the number of cached domain names @ @ @ @ @ @ @ @ @ @ see those values drop to 0 out of x . That 's a pretty good indication the clearing of the cache worked . 
@@45151550 @5151550/ <h> Exchange Server 2013 : HTTP 500 Errors for ECP and OWA ( Fresh Install ) <h> Exchange Server 2013 : HTTP 500 Errors for ECP and OWA ( Fresh Install ) <p> Sometime ago I changed my domain name ( you can read more about it here : - How to : Rename an Active Directory Domain Name ) to better address some business needs but unfortunately I had my Exchange Server offline as we are mostly using Exchange Online now . The issue is that if you have the servers offline and you complete the domain name change your old servers do n't  get the new changes . This ended up resulting on us being unable to bring that server back to fully functional status and losing our ability to manage our Exchange Hybrid configuration . Long story short I needed a new server to manage Exchange Online . I decided to grab a new VM and deploy Exchange Server 2013 CU2 but I encountered a blank HTTP 500 page after providing my credentials to enter Exchange Server 2013 ECP . Below is a brief story of @ @ @ @ @ @ @ @ @ @ know by now , HTTP 500 error messages are generally attributed to either Certificate misconfiguration or Authentication misconfiguration of OWA and ECP . <p> Certificate problems arise when the certificates you are using on the backend are corrupted or there might be another issue like validation , etc . This generally can be easily resolved by assigning a new certificate to your Exchange WebServer . You can do this via Exchange Shell or the not so recommended way : directly on IIS . <p> Authentication problems arise when you are not using the same authentication methods on your front and backend Exchange WebSites . The default recommended approach is Forms Based Authentication . The recommendation at this point is to use the Exchange Management Shell to establish both the OWA and ECP sites to accept Forms Based Authentication . <p> Finally , there are some weird issues that might require you to completely remove OWA and ECP virtual directories ( again , you can use the Exchange Management Shell ) and recreate them . By default Forms Based Authentication should be enabled but feel free to double check . <p> @ @ @ @ @ @ @ @ @ @ there referring to HTTP 500 error when accessing ECP . But in my case it was something much worse . <p> I had tried the three solutions described above with no avail and proceeded to upgrade to Cumulative Update 3 and the most recent Service Pack 1 in hopes that would do the trick . After many failures I came across a post that narrated a similar situation : <p> User had installed an Exchange Server from scratch to the letter ( per Microsofts instructions ) <p> User got Error 500 regardless <p> So he finally got Microsoft on the line and after much troubleshooting they determined a setting on AD was the culprit . I obviously tried deleting those entries and had no luck but it pointed me in the right direction : Something got really messed up on the backend that I could n't use Exchange ECP . I could had spent some time on figuring out exactly what was messing up my setup but I decided there were too many variables and too many configurations I was better off starting from scratch . As I mentioned , we @ @ @ @ @ @ @ @ @ @ was not a big deal for us . <p> Once you have wiped all the Exchange Server Configuration information from Active Directory you are ready to proceed with the installation . You can follow this simple instructions to quickly deploy it : How to Install Exchange Server 2013 SP1 . You 'll notice that this time around there is no HTTP 500 blank page after you install Exchange and everything works as it should ! <p> Fortunately it seems that the hybrid configuration is also stored online or somewhere else . After re-installing Exchange 2013 SP1 ( with a previous CU I did have some issues ) it picked up my hybrid deployment and from elsewhere in AD my users and groups . Because of this I only did a Hybrid sync to reconfigure my connections between onPremise and Cloud Exchanges , assigned a certificate and establish my externals URLs and I 'm all good and working again ! Yay ! 
@@45151551 @5151551/ 55329 @qwx465329 <h> Tag Archive : Graphical User Interface <p> How to : Get the size of your Databases in MySQL Query Browser Imagine you want to transfer data from one server to another and you need to know the size of the databases you are thinking of transfering . Unlike Microsoft SQL Server which graphical interface allows you to see the size of the database files <p> How to : Connect to a Remote File System with Nautilus using SSH Nautilus supports connecting to a remote file system using an SSH connection . Before there used to be clear instructions but now it all has gone away with a more simple interface that has left some users confused . But fear not , all you have <p> How to : Remote Desktop into an Ubuntu Server in Windows Azure Basically there are three parts to enabling a remote desktop session into an Ubuntu Server . But before we get into all that I feel the need to point this out : Ubuntu Server is sort of meant to run without a GUI and managed through <p> How to @ @ @ @ @ @ @ @ @ @ " Contributors to the Ubuntu documentation wiki " Lately I have been spending a lot of time using Ubuntu and as a Windows user I realized what I have always known : Life without a GUI is hard . Unless you are constantly using commands or have memorized them you <p> How to : Enable DHCP and Networking on Ubuntu Server By default this is not an issues as you can configure networking from the setup wizard when you initially deploy your server . I unfortunately had some issues with networking with my VM so when I finally solved them I realized the Ubuntu Server had no network <p> How to : Manage the Certificate Store on your local machine using the command prompt or PowerShell It seems that as of late I am playing a lot with certificates in order to- authenticate traffic across the network . Some of the most useful shell commands I have found are listed below , hopefully theyll help you manage your <p> How to : Visualize the Message Tracking Log in Exchange Server 2013 Ive always been a big GUI person , but @ @ @ @ @ @ @ @ @ @ the trend . Microsoft has been no exception to this trend , over the past few releases they have increasingly focused on correctly decoupling their GUI from their <p> How to add your Ubuntu computer to your Active Directory Domain when your Windows SBS Domain ends in . local using Likewise There are a number of options in order to get Active Directory integration with your Ubuntu systems . I personally liked Likewise as it provides a GUI and does a lot of the steps for <p> What is csrss.exe ? Many people while running through the Task Manager come across csrss.exe . With the amount of spyware , trojans and other malicious code running around the Internet it is always good to do a check every now and then that your computer is okay . Csrss.exe actually stands for Client/Server Runtime Subsystem and it is 
@@45151552 @5151552/ 55329 @qwx465329 <h> Tag Archive : Transport Layer Security <p> How to : Move your NginX website to HTTPs- / SSL It comes at no surprise that a lot of people are looking into moving their sites to HTTPs due to recent events : Googles decision to give ranking points to sites that use SSL / HTTPs and eavesdropping by governments world wide . There are a number <p> Changes with- NginX- 1.7 Below are the list of changes from the 1.5.13 release of NginX to version 1.7 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update includes several new Features- so updating to this version is not considered urgent/critical . Changes with nginx 1.7.0 24 Apr 2014 * ) Feature : backend SSL certificate verification . * ) <p> How to : Improve SSL performance on NginX You would be surprised but a lot of people face SSL performance issues when using NginX . I recently deployed SPDY over SSL for my sites and came to realize that SPDY was in fact much slower than using standard HTTP @ @ @ @ @ @ @ @ @ @ Resolved : Zemanta Editorial Assistant issues with https/SSL One of the main reasons why I had left my admin panel open to server non-ssl requests was that Zemanta Editorial Assistant would not work properly behind HTTPS . I did some looking around and found that the same scripts were being sent over to the browser so I <p> Changes with NginX 1.4.5 Below are the list of changes from the 1.4.4 release of NginX to stable version 1.4.5 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.4.5 11 Feb 2014 * ) Bugfix : the $sslsessionid variable contained full session serialized instead of just a session i 'd . Thanks to <p> Changes with- NginX- 1.5.10 Below are the list of changes from the 1.5.9 release of NginX to version 1.5.10 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.10 04 Feb 2014 * ) Feature : the ngxhttpspdymodule now uses SPDY 3.1 protocol . Thanks to Automattic and MaxCDN for sponsoring this work . * ) Feature : @ @ @ @ @ @ @ @ @ @ custom NginX distribution available on this site , there are a few packages you can chose from depending on your needs that can be deployed . Below is the list of packages , additional information and description : Different packages available : Package : - nginx Architecture : all Depends : nginx-full nginx-light , This <p> Changes with- NginX- 1.5.8 Below are the list of changes from the 1.5.7 release of NginX to version 1.5.8 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.8 17 Dec 2013 * ) Feature : IPv6 support in resolver. * ) Feature : the " listen " directive supports the " fastopen " parameter . Thanks to Mathew Rodley. * ) <p> How to : Install the NewRelic Server Monitoring agent on Ubuntu NewRelic is a company that offer monitoring services for all your web applications . It has support for a wide variety of technologies , primarily does centered around web applications/sites . In order to install the NewRelic Server Monitoring agent on an Ubuntu box simply follow these instructions : 
@@45151554 @5151554/ <p> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now <p> Resolved : Can not alter user dbo I was working on setting up a new Microsoft SQL Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying <p> Pro Tip : Upgrade to PHP7 As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got @ @ @ @ @ @ @ @ @ @ ? So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another <p> You 're probably here because the old yahoo finance query does not work in Excel anymore . As you can read in the updates section of my previous post- How to : Obtain historical stock prices from Yahoo finance ( you can query them via Excel too ) , the issue is Yahoo changing the URL and probably blocking Excel from accessing it . This post is going to focus in using another Yahoo service though building the URL to make the API call is different so better to start a new post . Ill be doing a Part III regarding using Google instead . The key thing to note here is that though this will give us a treasure of information , we wont have access to the historical data from what I can see . So go to @ @ @ @ @ @ @ @ @ @ need historical data . <p> So far , so good . Now comes the fun part . Like I mentioned earlier , there is a treasure of information available ; all you need is knowing how to ask and Yahoo to be willing to give it to us for free still . So , here are the different letters we use with the f parameter to indicate what data we want back : <p> Pricing <p> a " ask <p> b " bid <p> b2 " ask ( realtime ) <p> b3 " bid ( realtime ) <p> p " previous close <p> o " open <p> Dividends <p> y " dividend yield <p> d " dividend per share <p> r1 " dividend pay date <p> q " ex-dividend date <p> Date <p> c1 " change <p> c " change &amp; percentage change <p> c6 " change ( realtime ) <p> k2 " change percent <p> p2 " change in percent <p> d1 " last trade date <p> d2 " trade date <p> t1 " last trade time <p> Averages <p> c8 " after hours change <p> c3 " commission @ @ @ @ @ @ @ @ @ @ 's high <p> k1 " last trade ( realtime ) with time <p> l " last trade ( with time ) <p> l1 " last trade ( price only ) <p> t8 " 1 yr target price <p> m5 " change from 200 day moving average <p> m6 " percent change from 200 day moving average <p> m7 " change from 50 day moving average <p> m8 " percent change from 50 day moving average <p> m3 " 50 day moving average <p> m4 " 200 day moving average <p> Misc <p> w1 " day 's value change <p> w4 " day 's value change ( realtime ) <p> p1 " price paid <p> m " day 's range <p> m2 " day 's range ( realtime ) <p> g1 " holding gain percent <p> g3 " annualized gain <p> g4 " holdings gain <p> g5 " holdings gain percent ( realtime ) <p> g6 " holdings gain ( realtime ) <p> t7 " ticker trend <p> t6 " trade links <p> i5 " order book ( realtime ) <p> l2 " high limit <p> l3 " low limit <p> @ @ @ @ @ @ @ @ @ @ realtime ) <p> s6 " revenue <p> 52 Week Pricing <p> k " 52 week high <p> j " 52 week low <p> j5 " change from 52 week low <p> k4 " change from 52 week high <p> j6 " percent change from 52 week low <p> k5 " percent change from 52 week high <p> w " 52 week range <p> Symbol Info <p> v " more info <p> j1 " market capitalization <p> j3 " market cap ( realtime ) <p> f6 " float shares <p> n " name <p> n4 " notes <p> s " symbol <p> s1 " shares owned <p> x " stock exchange <p> j2 " shares outstanding <p> Volume <p> v " volume <p> a5 " ask size <p> b6 " bid size <p> k3 " last trade size <p> a2 " average daily volume <p> Ratios <p> e " earnings per share <p> e7 " eps estimate current year <p> e8 " eps estimate next year <p> e9 " eps estimate next quarter <p> b4 " book value <p> j4 " EBITDA <p> p5 " price / sales <p> p6 " @ @ @ @ @ @ @ @ @ @ " P/E ratio ( realtime ) <p> r5 " PEG ratio <p> r6 " price / eps estimate current year <p> r7 " price /eps estimate next year <p> s7 " short ratio <p> Impressed ? I am . But wait , there is more ! You can access data from a number of exchanges around the world . Here is the list : <h> Exchanges available via Yahoo Finance <p> Country <p> Exchange <p> Suffix <p> Delay <p> Data Provider <p> United States of America <p> American Stock Exchange <p> N/A <p> 15 min <p> Direct from Exchange <p> United States of America <p> BATS Exchange <p> N/A <p> Real-time <p> Direct from Exchange <p> United States of America <p> Chicago Board of Trade <p> . CBT <p> 10 min 55330 @qwx465330 <p> United States of America <p> Chicago Mercantile Exchange <p> . CME <p> 10 min 55330 @qwx465330 <p> United States of America <p> Dow Jones Indexes <p> N/A <p> Real-time 55330 @qwx465330 <p> United States of America <p> NASDAQ Stock Exchange <p> N/A <p> 15 min <p> Direct from Exchange <p> United States of America <p> New @ @ @ @ @ @ @ @ @ @ 55330 @qwx465330 <p> United States of America <p> New York Commodities Exchange <p> . CMX <p> 30 min 55330 @qwx465330 <p> United States of America <p> New York Mercantile Exchange <p> . NYM <p> 30 min 55330 @qwx465330 <p> United States of America <p> New York Stock Exchange <p> N/A <p> 15 min <p> Direct from Exchange <p> United States of America <p> OTC Bulletin Board Market <p> . OB <p> 20 min <p> Direct from Exchange <p> United States of America <p> Pink Sheets <p> . PK <p> 15 min <p> Direct from Exchange <p> United States of America <p> S &amp; P Indices <p> N/A <p> Real-time 55330 @qwx465330 <p> Argentina <p> Buenos Aires Stock Exchange <p> . BA <p> 30 min 55330 @qwx465330 <p> Austria <p> Vienna Stock Exchange <p> . VI <p> 15 min <p> Telekurs <p> Australia <p> Australian Stock Exchange <p> . AX <p> 20 min 55330 @qwx465330 <p> Belgium <p> Brussels Stocks <p> . BR <p> 15 min <p> Brazil <p> BOVESPA " Sao Paolo Stock Exchange <p> . SA <p> 15 min 55330 @qwx465330 <p> Canada <p> Toronto Stock Exchange <p> . @ @ @ @ @ @ @ @ @ @ Venture Exchange <p> . V <p> 15 min 55330 @qwx465330 <p> Chile <p> Santiago Stock Exchange <p> . SN <p> 15 min 55330 @qwx465330 <p> China <p> Shanghai Stock Exchange <p> . SS <p> 30 min 55330 @qwx465330 <p> China <p> Shenzhen Stock Exchange <p> . SZ <p> 30 min 55330 @qwx465330 <p> Denmark <p> Copenhagen Stock Exchange <p> . CO <p> 15 min <p> Telekurs <p> France <p> Euronext <p> . NX <p> 15 min <p> Telekurs <p> France <p> Paris Stock Exchange <p> . PA <p> 15 min <p> Telekurs <p> Germany <p> Berlin Stock Exchange <p> . BE <p> 15 min <p> Telekurs <p> Germany <p> Bremen Stock Exchange <p> . BM <p> 15 min <p> Telekurs <p> Germany <p> Dusseldorf Stock Exchange <p> . DU <p> 15 min <p> Telekurs <p> Germany <p> Frankfurt Stock Exchange <p> . F <p> 15 min <p> Telekurs <p> Germany <p> Hamburg Stock Exchange <p> . HM <p> 15 min <p> Telekurs <p> Germany <p> Hanover Stock Exchange <p> . HA <p> 15 min <p> Telekurs <p> Germany <p> Munich Stock Exchange <p> . MU <p> 15 min <p> Telekurs @ @ @ @ @ @ @ @ @ @ 15 min <p> Telekurs <p> Germany <p> XETRA Stock Exchange <p> . DE <p> 15 min <p> Telekurs <p> Hong Kong <p> Hong Kong Stock Exchange <p> . HK <p> 15 min 55330 @qwx465330 <p> India <p> Bombay Stock Exchange <p> . BO <p> 15 min 55330 @qwx465330 <p> India <p> National Stock Exchange of India <p> . NS <p> 15 min <p> National Stock Exchange of India <p> Indonesia <p> Jakarta Stock Exchange <p> . JK <p> 10 min 55330 @qwx465330 <p> Israel <p> Tel Aviv Stock Exchange <p> . TA <p> 20 min <p> Telekurs <p> Italy <p> Milan Stock Exchange <p> . MI <p> 20 min <p> Telekurs <p> Japan <p> Nikkei Indices <p> N/A <p> 30 min 55330 @qwx465330 <p> Mexico <p> Mexico Stock Exchange <p> . MX <p> 20 min <p> Telekurs <p> Netherlands <p> Amsterdam Stock Exchange <p> . AS <p> 15 min <p> Telekurs <p> New Zealand <p> New Zealand Stock Exchange <p> . NZ <p> 20 min 55330 @qwx465330 <p> Norway <p> Oslo Stock Exchange <p> . OL <p> 15 min <p> Telekurs <p> Portugal <p> Lisbon Stocks <p> . LS <p> @ @ @ @ @ @ @ @ @ @ SI <p> 20 min 55330 @qwx465330 <p> South Korea <p> Korea Stock Exchange <p> . KS <p> 20 min 55330 @qwx465330 <p> South Korea <p> KOSDAQ <p> . KQ <p> 20 min 55330 @qwx465330 <p> Spain <p> Barcelona Stock Exchange <p> . BC <p> 15 min <p> Telekurs <p> Spain <p> Bilbao Stock Exchange <p> . BI <p> 15 min <p> Telekurs <p> Spain <p> Madrid Fixed Income Market <p> . MF <p> 15 min <p> Telekurs <p> Spain <p> Madrid SE C.A.T.S . <p> . MC <p> 15 min <p> Telekurs <p> Spain <p> Madrid Stock Exchange <p> . MA <p> 15 min <p> Telekurs <p> Sweden <p> Stockholm Stock Exchange <p> . ST <p> 15 min <p> Telekurs <p> Switzerland <p> Swiss Exchange <p> . SW <p> 30 min <p> Telekurs <p> Taiwan <p> Taiwan OTC Exchange <p> . TWO <p> 20 min 55330 @qwx465330 <p> Taiwan <p> Taiwan Stock Exchange <p> . TW <p> 20 min 55330 @qwx465330 <p> United Kingdom <p> FTSE Indices <p> N/A <p> 15 min <p> Telekurs <p> United Kingdom <p> London Stock Exchange <p> . L <p> 20 min <p> Telekurs <p> @ @ @ @ @ @ @ @ @ @ to use around the world ! <h> How to : Move all Active Directory Roles ( FSMO ) from one server to another in Windows Server 2016 <p> Thankfully over time Microsoft has made it easy to move the Active Directory Roles ( currently 5 ) from one server to another . Back in the day , most of them used to be " hidden " all over the place but now we observe that they are getting grouped together and the change can easily be made via the UI . For example , your Active Directory Users and Machines GUI allows you to easily move 3 of the 5 roles . Back in the day you needed to register some weird Dll and open a mysterious snap-in for the Management Console but those days are over . Today though I am going to focus on how we can move all 5 roles from within Powershell . This is important as with just one simple but powerful command , we can achieve the transfer of all the roles without having to go to different places to do it . <h> @ @ @ @ @ @ @ @ @ @ can be easily achieved using the- netdom query fsmo command , it will return something like this : <h> Step 2 : Move the roles to another server : <p> This is done using the following command : - **41;5949;TOOLONG -Identity DC-02 -OperationMasterRole SchemaMaster , DomainNamingMaster , PDCEmulator , RIDMaster , InfrastructureMaster <p> As you can observe from the command , you are specifying the five Operation Master Roles you wish to transfer over to the DC-02.Cloudingenium.com server . You could , however , specify only one or more roles . This allows you to spread the roles around as you best see fit . The command will then ask you to confirm if you wish to move each of the roles to the server . You can chose between Yes , Yes to All , No , No to All , Suspend and Help . <h> Step 3 : Validate the Move <p> Run again the netdom query fsmo to make sure the change happened correctly . Keep in mind the replication of the roles could take some time . <h> Resolved : Can not alter user dbo <p> @ @ @ @ @ @ @ @ @ @ Server 2016 instance and importing databases from the old server when I ran into this issue . During the import ( Attach ) process , I assigned the user the application uses to connect as dbo ( The application requires dbo rights btw just saying before anyone says what a great best practice that is , lol . ) The problem I ran into is that I mapped my user to dbo when I needed just to assign those rights and dbo as the default schema . Figured I would map another user to dbo but that 's when I started getting the " Can not alter user dbo ' " error . I was like fine , let 's delete the user , err " Can not alter user dbo ' " . Ok , let 's detach the database and import it back in , err " Can not alter user dbo ' " . Ok , let 's rename it , err " Can not alter user dbo ' " I guess you get my point now . <h> Solution <p> From what I understood , dbo being the owner of @ @ @ @ @ @ @ @ @ @ all the restrictions on making any changes to that user so if you cant make changes to it then what are you supposed to do ? Well , there is a SQL statement you can execute against your database that would change ( oh wait , I thought we could n't make changes well , you cant but you can do this one change using this one method ) the owner of the database effectively changing the dbo mapping to another user . <p> Here is the command , e sure to replace the information within the brackets to your appropriate database and user . <h> Pro Tip : Upgrade to PHP7 <p> As you probably figured out , we run a WordPress site which it in itself runs on PHP . Many complaints have surface around the performance of php 5. x and even hhvm came out as a result of that . We had a lot of interest in hhvm , but we never got it quite working . If you also use WordPress you probably also ran into some quirks when using hhvm . Because of that , @ @ @ @ @ @ @ @ @ @ the php developer community took notice , and decided to make the new version a worthy competitor of hhvm and the like . So here we are , with PHP 7 already released for sometime we decided to dive in and see what we got , and we liked it . <p> So how faster is php7 when compared to php5 ? Much I would say . I woud love to have tons of metrics to share but we do n't  collect that many to begin with . However , we do have a few where you can see what were talking about : <p> 1 ) Web Transaction Time <p> We had an average web transaction time of 350 ms with the low end around 300 ms and the high end around 500 ms . Php processing time took around 100 ms per request , so most of the time was spent doing web external calls . Fast forward to php 7 and we got an average web transaction time of 140 ms with the low end close to 100 ms and the high end around 230 ms . @ @ @ @ @ @ @ @ @ @ processing time ranges from 37.5 ms to 87.5 ms , but most is basically at 50 ms . This means that overall we were able to cut our web processing times in half both as a total and as just php processing times . <p> On a site note : We also saw a drop in our throughput with the same number of visitors ( thereabouts ) so we are not sure what is driving that and if that is having an impact . Update : We found out a lot of those calls was a caching service that was preloading pages to accelerate the site although seems at the end of the day it really was slowing it down . We also implemented better security at the login page which resulted in lower number of requests . So at the end of the day we would need to go back to php5 to make a fair comparison seeing we did other changes that also resulted in this positive results . <p> We are also observing a higher number of php warnings , hopefully the wordpress team will address those @ @ @ @ @ @ @ @ @ @ <p> Overall our server usage also dropped . However , we also upgraded to a newer version of the OS which could also have an impact . Regardless , we are observing 1/3 memory usage overall like 1/5- CPU usage although we never used much to begin with . <p> 3 ) Front end <p> This is an even more subjective perspective . I have though , had a very positive impression using the web application . The time it takes for it to respond back I have noticed it is faster , which means something if a human being can notice it . Remember that although the page is generated in milliseconds on the web server , on average our pages are fully rendered after 9 seconds on the users browsers . Most of the time spent doing that is precisely page rendering right after COM processing . Most of our visitors use Google Chrome . Our faster load times are round across the norther part of the world ( North America , Europe , Russia , while the slowest load times are found in Argentina , India , @ @ @ @ @ @ @ @ @ @ from all over the world , there are hardly any from Africa for example so take this information lightly . <p> Well , that pretty much ends the review of php7 from our side . We are really excited to see such positive results and glad the technology stack we use finally is mostly compatible with it . We look forward this improving our readers experience and satisfaction . Thanks ! <h> How to : Cache static HTML with CloudFlare ? <p> So I have been working with CloudFlare for quite sometime but every now and then when my web server goes down CloudFlare seems to be unable to serve my pages while it comes back . I had to create my own cache server for it but it involved another point of failure and it slowed down the response times . I am sure at one point CloudFlare with its Always Online feature took care of my site but now I am not sure . So the question came to my mind how do I cache my web servers responses that are HTML . I do run a WordPress @ @ @ @ @ @ @ @ @ @ be incorrect but 90%+ of the content- once published does not change . Surely there could be some false/positives ( if I may call them that ) like mobile and desktop clients get the same flavor of the site , or incorrect behavior when publishing comments , etc . But based on the sites usage , mobile is less than 10% of the visitors and comments are rare , so more users are impacted by our web server going offline than by getting a desktop theme on a mobile device . So the lesser of two evils , we had to go with CloudFlare caching . The question then became why is it not caching html ? <h> What we found out <p> We needed to understand CloudFlares caching offerings to figure out how to achieve what we wanted ( if possible ) and we came up with this information : <h> What are CloudFlares caching levels ? <p> June 02 , 2016 08:58 <h> You can set CloudFlare 's CDN to cache static content according to these levels : <p> No Query String / Basic : Only delivers resources @ @ @ @ @ @ @ @ @ @ Ignore Query String / Simple : Delivers the same resource to everyone independent of the query string ( note : this will also remove the query string from the request to your origin ) . <p> Standard / Aggressive : Delivers a different resource each time the query string changes . <h> Our user agent <h> Why we 're crawling <p> If your server ever goes offline , CloudFlare will serve a limited copy of your cached website to keep it online for your visitors . CloudFlare builds the Always Online version of your website , so your most popular pages are represented . CloudFlare is caching pages when you see the crawler in your logs . <h> Crawling frequency <p> sounds like it should cache HTML otherwise how could it keep my site offline every time my server goes down ? But somehow it does n't  and that was really bugging me . Thankfully , the previous information we recovered ( Note : CloudFlare , by default , does not cache HTML content . You need to write a- Page Rule to cache static HTML content. ) gave us @ @ @ @ @ @ @ @ @ @ Cache static HTML content using Page Rules <p> Well , you can cache static and non-static HTML content the problem is that non-static content will be a snapshot in time so when it changes because it is cached the site visitors will get always the same page . So it is something not recommended . So let 's see how CloudFlare suggests we do this : <h> How do I cache static HTML ? <p> The first step is creating a pattern and then applying a rule to that pattern . You 'll need to find or create a way to differentiate static versus dynamic content by the URL . Some possibilities could be creating a directory for static content , appending a unique file extension to static pages , or adding a query parameter to mark content as static . Here are three examples of patterns you could create for each of those options : <p> You 'll want to design the pattern to only describe pages you know are static . <p> Click Cache everything- in the Custom caching dropdown menu . <p> Click Add rule . <p> If you see @ @ @ @ @ @ @ @ @ @ everything rule , it means you need to override the origin cache directive with an " Edge Cache TTL " setting . <p> If the Cache-Control header is set to " private " , " no-store " , " no-cache " , - or " max-age=0 " , or if there is a cookie in the response , then CloudFlare will not cache the resource , unless a Page Rule is set to cache everything and an Edge Cache TTL is set . <p> So , if you are lazy and your site permits it , just example.com/ the whole thing to get it cached . Keep the Cache-Control header set to no-cache when you are doing a dynamic page and make sure the Edge Cache TTL is not set and you 're golden ! <p> Another idea is to again , cache everything , but then with another rule lower the cache level on the parts of the site that are dynamic ( the opposite approach . ) So as you can see , you can get creative with it and figure what approach works best for you . <p> @ @ @ @ @ @ @ @ @ @ user to configure dynamic DNS services via the Web UI . However , there are cases when you rather keep the configuration on the config.gateway.json file . In my case the main motivation to use the config.gateway.json method is because you can set up the Dynamic DNS service for each Ethernet- port , and when you are using two WAN ports you probably need one for each of your public IPs . Whatever the reason , you can configure your Dynamic DNS service settings via the configuration json file , and we will teach you how : <h> The General Config file layout <p> Here we show you the layout/template of the configuration file to manage the Dynamic DNS service . Remember this is a json file so make sure to validate it before you commit it and abide to the schema when adding the relevant sections to avoid breaking something else . <p> Packages are manually installed via the dpkg command ( Debian Package Management System ) . dpkg is the backend to commands like apt-get and aptitude , which in turn are the backend for GUI install apps @ @ @ @ @ @ @ @ @ @ the lines of : <p> dpkg &gt; apt-get , aptitude &gt; Synaptic , Software Center <p> But of course the easiest ways to install a package would be , first , the GUI apps ( Synaptic , Software Center , etc .. ) , followed by the terminal commands apt-get and aptitude that add a very nice user friendly approach to the backend dpkg , including but not limited to packaged dependencies , control over what is installed , needs update , not installed , broken packages , etc .. Lastly the dpkg command which is the base for all of them . <p> Since dpkg is the base , you can use it to install packaged directly from the command line . <p> INSTALL A PACKAGE <p> sudo dpkg -i DEBPACKAGE <p> For example if the package file is called askubuntu2.0.deb then you should do sudo dpkg -i askubuntu2.0.deb . If dpkg reports an error due to dependency problems , you can run sudo apt-get install -f to download the missing dependencies and configure everything . If that reports an error , you 'll have to sort out the dependencies @ @ @ @ @ @ @ @ @ @ dependencies ? . <p> REMOVE A PACKAGE <p> sudo dpkg -r PACKAGENAME <p> For example if the package is called askubuntu then you should do sudo dpkg -r askubuntu . <h> Google IP address ranges <p> Recently I decided to try the Google Compute Engine Cloud , and one of the things I liked right from the beginning was the ability to connect via SSH to your Ubuntu box . I am going to have to read a bit more about it but basically it seems like it generates a keypair , sets it up on your server and then opens a terminal connection via ssh on your web browser how cool is that ? ! In my case I wanted to increase the security by limiting the IP- addresses that can access the SSH port . Here is where this post comes along . If you look at the documentation basically it says you need to allow all Google IP addresses because the web client may connect from any Google IP I know , a lot of IPs to authorize but I guess it is better than opening the @ @ @ @ @ @ @ @ @ @ am probably going to enable and disable the rule manually but open it to all Google IP addresses for convenience of using the web client to initiate an ssh connection . <h> The ranges <p> Well , the IP ranges that Google owns/manages vary throughout time . Fortunately , there are ways to obtain the lates versions at any given point . Unfortunately , I am not sure how can you dynamically update them in the Google Cloud Engines firewall rules so I am manually adding them . In order to identify them I am relying on the spf record google offers : - spf.google.com <p> So there you have it . The first block is called netblocks which contains all the blocks in the IPv4 space , then you have netblocks2 which contains all the blocks in the IPv6 space which leaves netblocks3 which contains an IPv4 address range but I am not sure why not simply include it in netblocks . If anyone knows the reason or has a good guess please share ! <p> If you wish to query this information anytime yourself , you can do @ @ @ @ @ @ @ @ @ @ nslookup -q=TXT spf.google.com 8.8.8.8 <p> nslookup -q=TXT netblocks.google.com 8.8.8.8 <p> nslookup -q=TXT netblocks2.google.com 8.8.8.8 <p> nslookup -q=TXT netblocks3.google.com 8.8.8.8 <p> So it is possible to automate the update of the ranges via a shell script for example or an API call but for me I think it is not necessary seeing updates shouldnt happen too often and if there is an issue I can always update as required . <h> Resolved : The " **37;5992;TOOLONG " task could not be loaded from the assembly <p> I recently have been working on an ASP.Net application and at some point I started to get build errors that although they were not stopping the build and allowed me to continue with the applications execution , they were starting to get quite annoying . <p> This is a sample of the error message that tormented me for ages but because it was n't a show stopper I left it there for almost a week : <p> The " **37;6031;TOOLONG " task could not be loaded from the assembly LONG ... Could not load file or assembly LONG ... or one of its dependencies . The @ @ @ @ @ @ @ @ @ @ the declaration is correct , that the assembly and all its dependencies are available , and that the task contains a public class that implements **31;6070;TOOLONG . <p> Thankfully the solution was a simple one . If you take a look at the file in reference it points you to a packages location. - It turns out that NuGet packages were committed to the repository and are breaking everything . To resolve this , you simply need to delete that packages folder ( usually under &lt;project name&gt; &lt;project name&gt; packages . ) I am afraid of deleting anything , so I went ahead and simply renamed the folder packages.broken and after that proceeded to build all again . The nice thing is that NuGet fetches the packages automatically on build so the folder got repopulated , nothing broke and my error was gone ! After that I proceeded to delete the packages.broken folder and move on with my life . <h> How to : Get the Sum of the Values from List in C#.Net ? <p> Currently I am working on a new project in which I need to calculate @ @ @ @ @ @ @ @ @ @ total amount . The easy way out is just to do a foreach statement and iterate through the list adding the values . But somehow , I wanted something that looked fancier but really cleaner and much easier to maintain and understand . Because of that , I came across the Sum method found in a list . In my case , because I was obtaining all the data from an API and it was returned in a Json it came as a string . So adding the complexity of adding a list of objects string properties . <p> In order to achieve the end result : " Summing all string representations of numbers stored as properties in a list of objects in a clean , easy to manage way " we rely on LINQ : 
@@45151555 @5151555/ <h> How to : Determine your Memory Usage &amp; Free Memory in Ubuntu <p> One thing I have discovered with Ubuntu is that there is more than one way to skin a cat . All wo n't yield the same result but you can achieve similar things through different approaches . Finding out how much memory you have used , how much is available , and what processes are using the most memory are all possible via different approaches . I will cover some of the commands I generally use but by no means is this list exhaustive . There is probably a better way out there to do some of these tasks I do n't  know about , but I feel confident these are pretty common approaches . <h> I.- /proc/meminfo <p> I do n't  use this one often but thought it should be the first one as this is the source for many of the commands I will talk about later . This file stores information about memory usage . I find the following command to be cool as it color codes the output but a simple cat- @ @ @ @ @ @ @ @ @ @ <p> This is probably my go to command . It tells you how much free memory you have but it is a bit tricky . The first thing to consider is what unit you want the information to be displayed . I started just running free and it output sizes in Kilobytes . Use free -m to display it in megabytes or free -g to show gigabytes . Seems though there is no rounding so if you are using 3.9gb you will get only 3 displayed seems I am missing something there . Finally , the if you use -t then you get the totals at the bottom . <p> Using free -t really gives you a picture of what I was saying earlier about the results being tricky . Let 's examine the output of free -m -t first : <p> total <p> used <p> free <p> shared <p> buffers <p> cached <p> Mem : <p> 670 <p> 607 <p> 63 <p> 0 <p> 92 <p> 330 <p> - -/+ buffers/cache : <p> 184 <p> 486 <p> Swap : <p> 4095 <p> 8 <p> 4087 <p> Total : <p> 4766 @ @ @ @ @ @ @ @ @ @ used column , adding 607 + 184 + 8 does not equal 615 . What you should be focusing on here is the row after Mem : . This rows give you the real memory usage adding or subtracting buffers/cache . So really your system in this case is using 607 mb of memory but most of that is allocated to a buffer/cache . If you monitor the processes you will add up to 184 mb of used memory even though 607mb have been allocated . For all practical purposes the " - -/+ buffers/cache : " row shows how much RAM you 've used and how much is available to the system . <p> VMStat is another alternative to Free I do n't  use often but you might want to give it a try . <h> III . Top <p> Although mostly used by me to monitor CPU usage , you can sort the screen however you see fit and see the processes that use the most memory . It comes installed by default with Ubuntu so you can always count on it . While the screen is displaying the @ @ @ @ @ @ @ @ @ @ available and how to sort . Use q to quit . <p> If you like Top , take a look at atop and htop which are similar but you generally have to install them . 
@@45151556 @5151556/ <h> How to : Install the Language Packs for SharePoint 2013 to deploy site on a different language <p> While installing- SharePoint- 2013 I ran in to several issues that appear- to have corrected by installing SharePoint 2013 in English on a Windows 2012 Server in English ( same language ) . I should note I was trying to deploy a SPserver in Spanish- on a W2012 English server . Regardless , after many of the issues disappearing by installing it in the same language and error codes more easily accessible now that the event log messages are in English I wanted to still deploy sites in Spanish . <p> After you have downloaded the language pack all you need to do is follow this simple instructions in order to deploy another language : <p> To install the language pack : <p> Download the file by clicking the Download button ( from the link above ) and saving the file to your hard disk . Do n't  forget to select the correct language you wish to deploy . <p> Run the setup program . <p> On the Read the @ @ @ @ @ @ @ @ @ @ select the I accept the terms of this agreement check box , and then click Continue . <p> The setup wizard runs and installs the language pack . <p> Rerun the SharePoint Products and Technologies Configuration Wizard , using the default settings . <p> Note : - If you do not run the SharePoint- Products and Technologies Configuration Wizard after you install a language pack , the language pack will not be installed properly . Rerun the SharePoint Products and Technologies Configuration Wizard : <p> Click Start , point to All Programs , point to Microsoft SharePoint 2013 Products , and then click SharePoint Products and Technologies Configuration Wizard . <p> On the Welcome to SharePoint Products and Technologies page , click Next . <p> Click Yes- in the dialog box that alerts you that some services might need to be restarted during configuration . <p> On the Modify Server Farm Settings page , click Do not disconnect from this server farm , and then click Next . <p> If the Modify SharePoint- Central Administration Web Administration Settings page appears , do not modify any of the default settings , @ @ @ @ @ @ @ @ @ @ SharePoint Products and Technologies Configuration Wizard page , click Next . 
@@45151559 @5151559/ 55329 @qwx465329 <h> Category Archive : Hyper-V 2012 <p> How to : Remotely manage a Hyper-V Server / Enable Firewall exceptions Unfortunately one of the things you will find out when you install a Hyper-V Server 2012 is that it is completely locked down . You are provided- with a very neat utility that allows you to easily perform common tasks like enabling remote management , configuring your <p> How to : Configure Hyper-V Replica using certificate-based authentication ( https ) Disclaimer : Before you get too deep into the article I have not been able to do this for a Hyper-V Core Windows Server- Installation ( the free Hyper-V version you download off of Microsoft ) . I get the certificate and everything to show up but last-minute- it throws and error 
@@45151560 @5151560/ <h> Most commented posts <h> Author 's posts listings <p> Jiuzhai Valley National Park , China Photo by : Richard Janecki Blue and green lakes as well as waterfalls dot Jiuzhai Valley National Park in southwestern China north of Chengdu . The area was declared a UNESCO World Heritage Site in 1992 , in part for the natural beauty and in part for its endangered plant and animal species . <p> Las Pozas , Xilitla , Mexico World Monuments Fund Las Pozas , which means " the pools " in Spanish , is a collection of surrealist structures created by English aristocrat Edward James . Born into wealth , James left his English mansion to create a fantasyland amid central Mexicos jungle . The 20 acres also include a staircase to nowhere . <p> United Airlines : Update Priority This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it ca n't be secured at the time it is @ @ @ @ @ @ @ @ @ @ More Points " has been announced and will continue through 3/31/12 . Details are provided below : Promotion period : 1/1/12-3/31/12 ; eligible nights are within these dates only ( ie , a stay 3/30-4/1 would only earn bonus points for 3/30-31 ) You must register prior to your hotel stays to be eligible Most Hilton <p> LONG ... Attached is the PDF containing the list of international phone numbers you can use to call the 1k United line. - intlcallsupport1Konly <p> Just a couple days go I received an email for a promotion with AA . Its a pretty sweet deal and its great for anyone trying to secure status with them this year or get an early start next year . Sadly I 've only flown American once this year so this does n't  help me much . Qualify For <p> In order to register visit the following Hilton site : LONG ... Free Gold Status + 40,000 HHonors Points four 4 stays ! Put in the code : 411945 Put in your HHonors Number Put in your HHonors Pin/Password or sign up for new account . Wait a @ @ @ @ @ @ @ @ @ @ see your account 
@@45151561 @5151561/ <p> Resolved : Microsoft Office has detected a problem with your Information Rights Management configuration . About a week ago one of our client computers started facing issues regarding the ability to access the Information Rights Management IRM server . We use Windows Azures IRM service ( Azure Rights Management ) so there are a few things you need to do <p> How to : Synchronize content between SharePoint 2013 and Outlook One of the great features of SharePoint- 2013 is its ability to synchronize a variety of information with Outlook . You can connect a SharePoint- calendar , library , contact list , or other type of list with Outlook . I think by far the calendar and contact list are the most used <p> " Microsoft outlook can not provide scripting support . The feature is not available " when using Microsoft Office in a Terminal Services environment As the message indicates scripting support is not available in Microsoft Outlook while using it in a terminal server environment . There could be several reasons ; in some cases it is just a matter of whatever 
@@45151565 @5151565/ 55329 @qwx465329 <h> Client Application Services <p> So , I was trying to do an application I have been avoiding for the past 8 years and the first thing I began working at was the authentication part of it . I remembered ASP.Net had membership and roles features with their authentication and login functionality so I was looking around for that . This time I decided to make it simple by just doing a Windows application using WPF ( Windows Presentation Foundation ) . To my surprise it seems you can only use the authentication in ASP.Net and there is nothing built-in for a Windows Application . This time around though ( ASP.Net 3.5 ) they have the Client application services which " enables your Windows-based applications to use the ASP.Net login ( authentication ) , roles , and profile ( settings ) services " ( Visual Studio 2008 Services Screen ) . It is a pain to figure out how to set everything but here is the general overview of what I 've discovered thus far : <p> You need some sort of database to store your users @ @ @ @ @ @ @ @ @ @ You need a web service to provide authentication services for your Windows application <p> You need to check out this walkthrough . After hours of internet search I think this is your one place to learn 90% of what you need . 
@@45151567 @5151567/ <h> How to : Easily explore technical artifacts in SharePoint 2010 and even delete site collections and webs <p> As a SharePoint developer you probably find yourself in the situation where using the SharePoint central administration becomes a rather painful task as many of them take several clicks and it is not easy to obtain a nice tree view of all the information you need and take actions on them . However , if you look around you 'll come around the SharePoint Developer Explorer . It is a great tool for any SharePoint developer and you 'll come to appreciate it soon enough . 
@@45151568 @5151568/ 55329 @qwx465329 <h> Tag Archive : Syntax highlighting <p> Changes with- NginX- 1.5.10 Below are the list of changes from the 1.5.9 release of NginX to version 1.5.10 . This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES Changes with nginx 1.5.10 04 Feb 2014 * ) Feature : the ngxhttpspdymodule now uses SPDY 3.1 protocol . Thanks to Automattic and MaxCDN for sponsoring this work . * ) Feature : 
@@45151569 @5151569/ <h> How to : Have WordPress communicate with MySQL via Socket <p> As I keep looking into how to improve the performance of website one of the recurrent points mentioned is to use Linux sockets where possible . I really do n't  have much experience and I can see how avoiding the TCP stack might help but I figured at least it could n't hurt . If you are able to establish a socket connection to MySQL you might also be in the camp of those interested in having WordPress connect to MySQL via a Linux socket connection vs using an IP Address . I searched but it seems there are not that many people doing this , so I thought I would share it so you can also give it a try to your site if you want . 
@@45151570 @5151570/ 55329 @qwx465329 <h> Tag Archive : Advanced Packaging Tool <p> How to : Install Ruby in an Ubuntu Server to use it with NewRelics Plugins : Memcached &amp; NginX Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . Also , most of them require other applications to be installed as well like Ruby <p> How to : Install Ruby on Rails using RVM This is just a quick complement of the article How to : Install the NginX Agent for NewRelic in an Ubuntu server . As I mentioned there if you need Ruby for something other than running a plugin agent like in a development web server RVM is the way <p> How to : Install NginX If you want to Install NginX , we currently host a PPA that tries to keep up with the latest updates from NginX . Sometimes we make changes to the build when something breaks or does n't  work but most of the time we use the same package that is provided by the NginX <p> @ @ @ @ @ @ @ @ @ @ in Ubuntu Lately as I log in to an Ubuntu box I get messages indicating that there are security updates available for Ubuntu but I have struggled to get them installed . 13 packages can be updated . 13 updates are security updates . Traditional- one would use apt-get update ; <p> Ubuntu : Basics of software repositories Ubuntu uses- apt- for package management . Apt stores a list of repositories or software channels in the file /etc/apt/sources.list This is important to note as if you want to add sources for software packages you are going to either edit this file or add a referenced source file in the same format . <p> How to : Install Java JRE on an Ubuntu computer There were the good old days when everyone was friends with everyone . MySQL was own by people committed to open source , Sun Microsystems was still around developing Java for the world , and the Linux community was thriving . Fastfoward 2013 and you 've got Sun Microsystems buying MySQL . <p> How to : Install MySQL in an Ubuntu Server If you are looking into deploying @ @ @ @ @ @ @ @ @ @ whatever flavors you like best , Ubuntu is a good OS to do so . I say that because they make the installation super easy from the command line with- apt-get , it does n't  get much simpler <p> Resolve : Your web-server does not have the mcrypt module installed . Without it , encryption will be a lot slower . ( How to install mcrypt in an Ubuntu server ) I have installed UpdraftPlus in order to manage backups of the site . The downside is they seem to be moving towards having you buy addons for a lot of 
@@45151571 @5151571/ 55329 @qwx465329 <h> Category Archive : . Net <p> Resolved : The " **37;6103;TOOLONG " task could not be loaded from the assembly I recently have been working on an ASP.Net application and at some point I started to get build errors that although they were not stopping the build and allowed me to continue with the applications execution , they were starting to get quite annoying . This <p> How to : Get the Sum of the Values from List in C#.Net ? Currently I am working on a new project in which I need to calculate some statistics that involve some basic statistics which require the total amount . The easy way out is just to do a foreach statement and iterate through the list adding <p> . Net : Supported Console Colors The days of writing applications that write to the console seem to be over for a while now . Notwithstanding , we find ourselves using the console from time to time to help us debug or quickly test application logic . In my case , I am currently using the console to monitor logging @ @ @ @ @ @ @ @ @ @ on a small programming project on the site . It has been a while since Ive done any programming , so seeing all the new technologies and cloud offerings I must confess I am as excited and fascinated as I am daunted by how much there is and how far <p> Pro Tip : Never use type double for money / financial applications When you are new into coding financial applications one of the mistakes people tend to do is using a data type that its precision / significant digits are not enough to properly represent numers . If you 've taken some CS classes you know how decimals <p> After writing code using entity framework for sometime and seeing the code base continue to grow I realized the importance of coding with performance in mind . There are things that come to mind like eager loading , but that alone wont do much as your application continues to grow . Below are some key best practices I <p> Recently I came across the need to convert a string stored in the database into an enumeration in our application . I @ @ @ @ @ @ @ @ @ @ nor having to maintain that code . Fortunately there is a command in . Net that allows you to parse that string into any enumeration which <p> Sometimes you need to publish a site anyone should be able to see or maybe even add information to . In order to do this you need to have Anonymous access enabled on the site . One step many people miss is that anonymous access has to be enabled at the web application level otherwise it is 
@@45151572 @5151572/ 55329 @qwx465329 <h> Category Archive : Windows Server 2012 <p> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker I am going to have to start with a confession , " Resolved " might be a bit of a stretch . For starters , the best case scenario looks like recovering the information but you 'll need a temporary place where to store it , <p> How to : Enable Shadow Copy or Previous Version in Windows 2012 R2 ? I recently had a bit of an issue with a program . It uses an access database and one of the employees modified it but we needed to revert the changes . Seemed simple enough , just reach out to pick a previous version from the <p> Resolved : Passwords must meet complexity requirements Recently we were working on synchronizing passwords between an onPremise AD deployment and Azure Directory Services . As part of that we had to enable a strong password policy ( You can configure this security setting by opening the appropriate policy and expanding the console tree @ @ @ @ @ @ @ @ @ @ error 0x80c80037 No sync share is configured for the user . I am still trying to figure out why Workfolders works at times and then it complete stops working indicating the following error messages : Log Name : **39;6142;TOOLONG Source : **27;6183;TOOLONG Event I 'd : 4016 Description : No sync share is configured for the user . User : UserName ; <p> Resolved : Essentials Server Integration must be done on the Domain Controller Recently we performed a migration from our on premise Exchange server to Microsofts cloud solution : Exchange Online . Because of that I wanted to perform an integration at the domain controller level using Windows Server Essentials Experience . Unfortunately whether I tried using a member server <p> Resolve : Error 0x800B0100 when you try to install Windows Updates or Microsoft Updates I recently had to re-install a MultiPoint server and with over 80 updates I guess I should not be surprised something went wrong . I ended up trying to install one update at a time and I was successful in getting about half <p> The IO operation at logical block address 0 for Disk @ @ @ @ @ @ @ @ @ @ MPIO ) like when using iSCSI it is possible you might run across the message : The IO operation at logical block address X for Disk X was retried . This initially looks like trouble although truly it is more <p> Windows Server 2012 " File System Resiliency ( ReFS ) or Data Deduplication ( NTFS ) ? One of the features of Windows Server 2012 R2 that Ive been doing research as of late is the new file system Resilient File System ReFS ) . The big question as you can imagine is whether to use it or not . What became clear <p> How to : Re-enable Remote Desktop with PowerShell after you 've blocked it with your own firewall rule Obtained from : - LONG ... The Big Problem : I set up an Azure virtual machine running Windows Server 2012 . I accidentally disabled the Remote Desktop Windows firewall rule ( while I was remotely connected ) . The connection dropped as you would expect . I 
@@45151573 @5151573/ <h> How to : Install a . deb file via the Command line ? <p> Packages are manually installed via the dpkg command ( Debian Package Management System ) . dpkg is the backend to commands like apt-get and aptitude , which in turn are the backend for GUI install apps like the Software Center and Synaptic . <p> Something along the lines of : <p> dpkg &gt; apt-get , aptitude &gt; Synaptic , Software Center <p> But of course the easiest ways to install a package would be , first , the GUI apps ( Synaptic , Software Center , etc .. ) , followed by the terminal commands apt-get and aptitude that add a very nice user friendly approach to the backend dpkg , including but not limited to packaged dependencies , control over what is installed , needs update , not installed , broken packages , etc .. Lastly the dpkg command which is the base for all of them . <p> Since dpkg is the base , you can use it to install packaged directly from the command line . <p> INSTALL A PACKAGE <p> @ @ @ @ @ @ @ @ @ @ file is called askubuntu2.0.deb then you should do sudo dpkg -i askubuntu2.0.deb . If dpkg reports an error due to dependency problems , you can run sudo apt-get install -f to download the missing dependencies and configure everything . If that reports an error , you 'll have to sort out the dependencies yourself by following for example How do I resolve unmet dependencies ? . <p> REMOVE A PACKAGE <p> sudo dpkg -r PACKAGENAME <p> For example if the package is called askubuntu then you should do sudo dpkg -r askubuntu. 
@@45151575 @5151575/ <h> Category Archive : American Airlines <p> Just a couple days go I received an email for a promotion with AA . Its a pretty sweet deal and its great for anyone trying to secure status with them this year or get an early start next year . Sadly I 've only flown American once this year so this does n't  help me much . Qualify For <p> Below are the promotion code rules obtained from AA.com. - This should allow you to book travel and get a 15% discount , just be sure to follow the rules and good luck ! - Promotion Codes are only valid on AA.com for flights operated by American Airlines , American Eagle and AmericanConnection . Any reservation made using a Promotion Code <p> AA/Hyatt Travelers , American Airlines and Hyatt have teamed together to launch a promotion that gives you the opportunity to earn up to 60,000 AA miles by staying at Hyatt properties now through 10/15/10 . The details : Each Hyatt stay is worth 3,000 AA miles ; after 20 stays , you will have earned the maximum amount of miles @ @ @ @ @ @ @ @ @ @ counting on at least 500 miles credited to your account ( specially when you fly very small distances ) . Is it true many Airlines are changing their policies so that now you 'll accrue miles based on actual milage ? 
@@45151576 @5151576/ <h> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker <h> Resolved : How to recover an accidentally deleted volume ( partition ) in a Virtual Disk protected by BitLocker <p> I am going to have to start with a confession , " Resolved " might be a bit of a stretch . For starters , the best case scenario looks like recovering the information but you 'll need a temporary place where to store it , test it ( make sure its fine not corrupted ) , etc . I cant stress enough that as soon as you have messed something up you should read everywhere all you can to avoid following one method and screwing your chances of real recovery with it . I say this because if you write on top of your old data its gone probably for good . You have to be extra careful and be mindful that one method might mess up your chances with another one , so err on the side of caution . By this point all bets are off @ @ @ @ @ @ @ @ @ @ should not be your only alternative . If- the information is valuable you 're better off hiring a professional than trying to fix it yourself . So all I can really say now is I take no responsibility , this method seemed to have worked for others and it worked for me thankfully it does not mean it will work for you . So now that you understand the risks involved ( if not please abort ) , let 's get on with it ! <p> So as you probably figured out , I deleted a volume ( partition/drive ) accidentally . I was trying to delete one that had an issue so I backed up my data on that volume , then moved it over to another , and then I deleted it so I could recreate elsewhere oh wait its still there oh wait where is my other data volume oh sh ! t ! And that is how I lost my entire long easter weekend . I tried several utilities on the internet ( albeit I distrust so many third parties on my server but what choice did I @ @ @ @ @ @ @ @ @ @ an article from Microsoft : - How To Recover an Accidentally Deleted NTFS or FAT32 Dynamic Volume . Basically it states : <h> To Recover a Deleted NTFS Volume <p> Re-create the exact same volume but choose not to format it . This may be difficult if you do not remember the exact size you had created originally , especially because the Disk Management snap-in tends to round partition sizes . <p> Using Dskprobe.exe , recover the backup boot sector for the NTFS volume from the end of the volume . Because it is a dynamic volume you may need to use Dmdiag.exe to help find the backup boot sector , or search for it by using Dskprobe.exe ( on the Tools menu , click Search Sectors ) . <p> After rewriting the NTFS boot sector , quit Dskprobe . <p> In Disk Management , click Rescan Disks on the Action menu . This should mount the volume for immediate use . <p> so I did that the best I could . Fortunately my partition covered the whole drive so I just used the max size while creating it . @ @ @ @ @ @ @ @ @ @ you do then you just made the recovery considerably less likely and harder and out of the scope of this post . By now the partition showed up as RAW ( not formatted ) and all the recovery utilities where unable to recover the information I needed . By now I had lost almost all hope until I started to look at the actual data looking for any information on any sector when I realized everything looked like garbage eh I mean , random text that might as well be encrypted data ! So here is where things took a turn and my immediate future looked brighter ! The deleted volume was encrypted using BitLocker , and because of that the OS was unable to recognize the formatting on the drive and what not . Doing some research I found a system utility designed for scenarios such as this . <h> Solution <p> Find your decryption key for BitLocker . You get a few choices between : <p> Recovery Key <p> Recovery Password <p> Password <p> Key Package <p> Find a suitable storage location to store your decrypted data <p> @ @ @ @ @ @ @ @ @ @ to recover <p> The volume/partition must have at least the same amount of available- space as the one you are trying to recover ( total size of both include used and free space ) <p> so now that you have everything you need let 's get started ! <p> First of , take a deep breath and make sure you do n't  accidentally make this thing worse . Double , Triple check everything to make sure you are doing the right thing . So let 's get started <h> Step I Recreate the partition from Disk Management <p> Ill start off with a big warning : Do NOT format the volume/partition . There are several ways to launch Disk Management ( compmgmt.msc ) , so go ahead and use your favorite method and launch it with administrative rights ( as you 'll need them to create the partition . ) Now that you have it open , time to go look for the Unallocated space where you had the partition you are trying to restore . <p> Once identified , let 's proceed to recreate the partition . In my case , I right @ @ @ @ @ @ @ @ @ @ . Keep in mind we are trying to recreate the lost partition , so you need to provide all the information identically as it was on the deleted volume ( size , etc . ) For more advanced scenarios you could specify the start and end sectors but if you are like me and used the entire disk the wizard should be enough . Again , be careful , one of the steps in the wizard reads " Format Partition . " Make sure you select the option " Do not format this volume " to avoid data loss . Do n't  forget to mount it to a drive letter so you can work on it later on ( in my case I have assigned it the letter E. ) Once you 're done you 'll see in the Disk Management console that your partition appears as RAW . If you did n't  have BitLocker on your partition it should ( based on what I read ) show your original partition and you should had recover access to your data . In our case because we used BitLocker the information is encrypted @ @ @ @ @ @ @ @ @ @ enabled drive . Here is where Step II comes in <h> Step II Use- repair-bde to unencrypt your BitLocker volume <p> Due to Microsoft wanting to be careful and safe with your data , this tool is basically a read-only tool . It will not repair your lost volume , but rather would read it , decrypt it , and save the unencrypted information elsewhere . This is why we need a storage location where we can store the entire content of the encrypted volume ( not just the used space , but the entire space of the volume . ) To do so , we are going to use the repair-bde tool as follows : <p> This is what Ive seen most people use . I have n't troubleshoot enough but I did ran into some issues using an Imagine file . I tried to mount it with no luck ( The disk image file is corrupted. ) and people suggested using 7Zip to open the image file . That worked fine but all the information I got out was corrupted . I did experience an issue where the progress @ @ @ @ @ @ @ @ @ @ to have it continue progressing . The same thing happened at different progress %s which might had been the root cause of the data corruption . <p> Because of those issues , I prepared a new volume where to store the data . Instead of using an image file I used a volume instead like so : <p> and voil+ ! You got yourself a volume ( B : ) with the information that used to be stored in the encrypted drive ! <p> I am still double checking all my data is there and that no information is corrupt but thus far it is incredibly promising . All the files I have tried to access are there and displaying properly . Until the users come in tomorrow morning I wo n't know for sure if this was 100% successful but from what Ive seen I believe so . Hopefully this guide saves your life- data as well ! <h> Additional knowledge <p> So here are a few bits and pieces of additional knowledge that might help you when facing this issue : <h> Q : I get an error when @ @ @ @ @ @ @ @ @ @ information . ( 0x80310000 ) " <p> A : You are not providing the right key/password to decrypt your BitLocker drive . As the message at the end says " ERROR : BitLocker is not suspended on this volume . Try another key protector . " <h> Q : The image file shows up as corrupted . Could not open it with 7Zip either <p> A : If your drive is over 2 TB in size it most likely is GPT formatted . If you use diskpart you 'll see that such a formatted disk comes with a " Reserved " system partition : <p> In my case , when I deleted my partition , partition 1 of 128mb remained there so I had no issues . You should use diskpart to see if that is the case . If you do n't  have it then partition alignment wont be the same without it . A potential solution would be to grab a similar drive , partition it , and recreate the partitions identically to replicate as they were before you deleted them in the troubled drive . I am no @ @ @ @ @ @ @ @ @ @ pick the right tool and not overwrite your data . <h> Q : This imagine file looks familiar , can I mount it on a Mac ? <p> A : Sure can . Should work with windows as well not sure what to make it of though . 
@@45151577 @5151577/ <p> GLINDA ( spoken ) Elphie , listen to me . Just say you 're sorry , before its too late : ( sung ) You can still be with the Wizard What you 've worked and waited for You can have all you ever wanted <p> ELPHABA ( spoken ) I know ( sung ) But I do n't  want it No , I cant want it Anymore <p> Something has changed within me Something is not the same I 'm through with playing by the rules Of someone elses game Too late for second-guessing Too late to go back to sleep Its time to trust my instincts Close my eyes and leap ! <p> GLINDA Ca n't I make you understand ? Youre having delusions of grandeur <p> ELPHABA I 'm through accepting limits Cuz someone says they 're so Some things I can not change But till I try , I 'll never know ! Too long Ive been afraid of Losing love I guess I 've lost Well , if that 's love It comes at much too high a cost ! I 'd sooner buy Defying gravity Kiss me goodbye I 'm defying gravity And @ @ @ @ @ @ @ @ @ @ come with me . Think of what we could do , together <p> ( sung ) Unlimited Together were unlimited Together well be the greatest team Theres ever been Glinda Dreams , the way we planned em <p> GLINDA If we work in tandem <p> BOTH Theres no fight we can not win Just you and I Defying gravity With you and I Defying gravity <p> ELPHABA Theyll never bring us down ! ( spoken ) Well ? Are you coming ? <p> GLINDA ( spoken ) Elphie , you 're trembling Here , put this around you <p> ( sung ) I hope you 're happy Now that you 're choosing this <p> ELPHABA ( spoken ) You too ( sung ) I hope it brings you bliss <p> BOTH I really hope you get it And you do n't  live to regret it I hope you 're happy in the end I hope you 're happy , my friend <p> ELPHABA So if you care to find me Look to the western sky ! As someone told me lately : " Evryone deserves the chance to fly ! " And if I 'm flying @ @ @ @ @ @ @ @ @ @ me Take a message back from me Tell them how I am Defying gravity I 'm flying high Defying gravity And soon I 'll match them in renown And nobody in all of Oz No Wizard that there is or was Is ever gon na bring me down ! 
@@45151578 @5151578/ 55329 @qwx465329 <h> Tag Archive : NewRelic <p> How to : Install the MySQL Agent for NewRelic in an Ubuntu server As part of a series of posts ( How to : Install the Memcached Agent for NewRelic in an Ubuntu server and- How to : Install the NginX Agent for NewRelic in an Ubuntu server ) I am including instructions on how to install the MySQL monitoring agent <p> How to : Install the Memcached Agent for NewRelic in an Ubuntu server Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . For example , take this instructions on how to install the Memcached agent to get monitoring on your web server : <p> How to : Install Ruby in an Ubuntu Server to use it with NewRelics Plugins : Memcached &amp; NginX Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . Also , most of them require other applications to be installed as well like Ruby @ @ @ @ @ @ @ @ @ @ in an Ubuntu server Everytime I try to install a plugin for New Relic I find myself trying to figure out how to perform each of the steps . For example , take this instructions on how to install the NginX agent to get monitoring on your web server : <p> How to : Install the NewRelic Server Monitoring agent on Ubuntu NewRelic is a company that offer monitoring services for all your web applications . It has support for a wide variety of technologies , primarily does centered around web applications/sites . In order to install the NewRelic Server Monitoring agent on an Ubuntu box simply follow these instructions : 
@@45151579 @5151579/ <h> Monthly Archives : July 2011 <p> I do n't  know where this road is going to lead me But I 'm hoping that with you I can make it through I 've had enough of this life to lead me Right up to the edge of the world I knew <p> Chorus I cant wait just to see another day If it means its one more day that I 'm with you Down this road well look back at all we 've known Found a love that cant grow old Just passing through In the world I knew <p> I 've never seen a sunrise like this one Its like the whole world is waking up for us They say tomorrow cant promise anything So I 'll take every moment and make it now <p> Chorus From the world I knew <p> Ill be loving your light Till it fades away Tell the world I know Cause it will never change If something feels so right Just cant turn the page Theres too much to lose , were just passing through <p> Born with the voice of an angel A boy with the earth @ @ @ @ @ @ @ @ @ @ had made other plans He was only a man of the people With barely his clothes to his name But when he sang there was magic Touched by loves sacred flame La Fiamma Sacra <p> Holy fire in his soul Born to conquer the Dark A man who came , to carry the flame Awakening la Fiamma Sacra <p> He sang to the soul of a nation A voice for the weak and the strong A world of fabulous stories Came to life in his song With a gift for the whole of creation He gave not for fortune or fame A simple man blessed with magic Touched by loves sacred flame La Fiamma Sacra <p> Holy fire in his soul Born to conquer the dark A man who came to carry the flame With a voice that can speak to the heart <p> Holy fire in his soul Born to conquer the dark A man who came to carry the flame Awakening la Fiamma Sacra 
@@45151581 @5151581/ <h> How to : Easily explore technical artifacts in SharePoint 2010 and even delete site collections and webs <p> As a SharePoint developer you probably find yourself in the situation where using the SharePoint central administration becomes a rather painful task as many of them take several clicks and it is not easy to obtain a nice tree view of all the information you need and take actions on them . However , if you look around you 'll come around the SharePoint Developer Explorer . It is a great tool for any SharePoint developer and you 'll come to appreciate it soon enough . 
@@45151582 @5151582/ 55329 @qwx465329 <h> Tag Archive : MX record <p> How to : Set up multiple Administrator accounts in Microsofts Live Domains Sometimes you want to distribute the administrative tasks- of maintaining a system across a number of individuals to- provide- business continuity in- case you are not available . Fortunately if you are using Live Domains you can have multiple administrators to manage your domain ( create accounts , etc . ) Obviously each 
@@45151583 @5151583/ 55329 @qwx465329 <h> Category Archive : Unifi <p> How to : Configure Dynamic DNS Service on the Unifi Security Gateway via config.gateway.json Thankfully the latest versions of Ubiquitis Unifi Controller allow the user to configure dynamic DNS services via the Web UI . However , there are cases when you rather keep the configuration on the config.gateway.json file . In my case the main motivation to use <p> How to : Install a valid SSL Certificate for Ubiquiti Networks Unifi Controller Problem If you are like me , you are probably sick by now of having the certificate error page pop up everytime you visit the controllers page . I personally think Ubiquiti should make it as easily as uploading a web certificate via the GUI <p> How to : Clone the mac address for the WAN interface on a Ubiquiti Unifi Security Gateway Background Nowadays you somewhat expect that cloning a mac address on a gateway/router would be a basic feature but as Apple has taught us we do n't  know what we want even if we need it . But getting back into <p> UniFi How to @ @ @ @ @ @ @ @ @ @ LONG ... Overview The file- config.gateway.json- is used for advanced configuration of the USG . This file allows you to make customizations persistent across provisions . When making customizations via the config.gateway.json file it is best to extract only the customizations that ca n't be performed via the controller UI. 
@@45151584 @5151584/ 55329 @qwx465329 <h> Tag Archive : HTTP <p> Changes with- NginX- 1.6 Below are the list of changes from the 1.5.13 release of NginX to version 1.6 This is provided as reference only and was obtained from : - http : //nginx.org/en/CHANGES . This update makes the development / mainline version 1.5.13 part of the stable branch. - Because of this several new features are included and we highly suggest people upgrade to <p> How to : Install NginX If you want to Install NginX , we currently host a PPA that tries to keep up with the latest updates from NginX . Sometimes we make changes to the build when something breaks or does n't  work but most of the time we use the same package that is provided by the NginX <p> Q ) How to resolve the error message " You must specify a non-autogenerated machine key to store passwords in the encrypted format . " when using ASP.Net membership : When using ASP.Net membership provider you might encounter the error " You must specify a non-autogenerated machine key to store passwords in an encrypted format " like this one : LONG ... was unhandled 
@@45151585 @5151585/ <h> How to : Install NginX <h> How to : Install NginX <p> If you want to Install NginX , we currently host a PPA that tries to keep up with the latest updates from NginX . Sometimes we make changes to the build when something breaks or does n't  work but most of the time we use the same package that is provided by the NginX team . Below are the instructions on how to install NginX using our PPA or the NginX team-s . <h> I. Add our Repository <p> The first step is adding our repository to your Ubuntu install in order to get NginX . Below are the two repositories we maintain and the two repositories the NginX team maintains . Be sure to select / use only one of them . <p> Mainline Repositories : <p> sudo add-apt-repository **32;6212;TOOLONG <p> &amp; <p> sudo add-apt-repository- ppa:nginx/development <p> Stable Repositories : <p> sudo add-apt-repository **30;6246;TOOLONG <p> &amp; <p> sudo add-apt-repository- ppa:nginx/stable <p> We obviously recommend our repositories as the information posted on the site pertains to those , but you may chose the NginX teams repositories @ @ @ @ @ @ @ @ @ @ ( only when we need different behavior do we make changes to the build but usually by the next release they 've addressed the issues ) . <p> After you have added the repository , execute- sudo apt-get update to get the latest binaries . <h> II . Install NginX <p> Thus far easy right ? Well , now it is still easy but you need to decide which package to install . If you read our Custom NginX Distribution post you can find out more about the different packages and what they contain . Sometimes we recommend the basic NginX package , but in other cases you need extra modules found in the Extras package or you want to play with NAXSI to protect your server . The choice is yours ! We are going to use the extras packages as we believe is the most complete one and a good place to start . <p> Simply execute : sudo- apt-get install nginx-extras to get it install on your server <h> III . Configure NginX <p> Now it is time to configure NginX . This is probably the most tedious @ @ @ @ @ @ @ @ @ @ with NginX you are going to need to read on how to properly configure the different sections and where those configuration files are stored . You will also need to decide on a number of options / settings that fine tune the performance and HTTP communication parameters of your new Web Server . Below is a small and quick getting started guide to orient you . This should help you figure out where to start so you can more quickly get up to speed with NginX : <p> Open the main configuration file : /etc/nginx/nginx.conf <p> Add your sites configuraiton files at : **26;6278;TOOLONG <p> Make symbolic links to /etc/nginx/sites-enabled for the configuration sites you want to enable ( like what Apache does ) <p> You can use common files and include them through the include directive . <p> Files on the /etc/nginx/conf.d- are included automatically so you can create different sections for ease of management instead of filling up your main configuration file . 
@@45151586 @5151586/ 55329 @qwx465329 <h> Tag Archive : PageSpeed <p> Resolved : Zemanta Editorial Assistant issues with https/SSL One of the main reasons why I had left my admin panel open to server non-ssl requests was that Zemanta Editorial Assistant would not work properly behind HTTPS . I did some looking around and found that the same scripts were being sent over to the browser so I <p> How to : Improve the performance / speed of WordPress running in IIS To be entirely honest I am still struggling with this topic . However I have identified a few points that should help improve the performance / speed of a WordPress site running in IIS . Now , if a distinction should be made it would be 
@@45151587 @5151587/ 55329 @qwx465329 <h> Tag Archive : China <p> Why is my Exchange 2013 Server generating a lot of emails from email protected and email protected ? At first I was very worried about this . My servers were sending out some- SPAM- ( forgot to turn on the Sender I 'd so a few of Non delivery emails were being sent out , whoops ) and there had been some port scans detected 
@@45151588 @5151588/ <h> How to : Speed up CrashPlan by Reassigning Cache Folder to a Different Directory <h> How to : Speed up CrashPlan by Reassigning Cache Folder to a Different Directory <p> I 'm guessing if you 've bumped into this article is because you have deployed CrashPlan and you 're looking up for ways to speed it up . I have been a customer for several years now and I am happy with the solution from a cost/benefit perspective . One issue though is that we have our main storage server with Terabytes worth of data . What happens now is that CrashPlan out of the gate is not tuned up for handligh Terabytes worth of backup . So my first experience hitting the 2/3 Terabytes was CrashPlan simply crashing . You can read more about it here : - How to : Prevent CrashPlan Pro from shutting down abruptly . Basically the Java Virtual Machine needed more RAM in order to load and display what was going on with the backup engine . The more data and complexity you add , the more memory it needs . If you have n't tweaked @ @ @ @ @ @ @ @ @ @ Moving on into the next tune up you can perfom . Let me picture you our scenario : As you know CrashPlan installs on the main system drive and the cache drive is stored on the Application Data folder , which coincidentally it is on the main system drive ( by default ) . So one day I come into the office and by noon we are having several issues that can be tracked to our storage server , particulary , its performance . I could barely log in and finally when I start looking at the performance monitor I notice my system drive is 100% active with a huge Disk Queue Length . Baffled as nothing is stored on the system drive I sent in and found file on a CrashPlanCache folder having the highes Total Bytes per second across the board . <p> What I learned that day is that CrashPlan is actively reading and writting to a cache store as it is performing a backup and validating if it needs to upload it or not . Our system drive although SSD was not designed to take that @ @ @ @ @ @ @ @ @ @ figure out how to move that into our RAID drive which was designed for IO intensive operations . <h> Solution <p> As I pointed out , our solution was to move the CrashPlans cache store to a drive designed to handled IO intensive operations . In order to perform this feat you need to change the configuration via an XML offered by Code 42 developers ( Thank you ! ) although it does come with a big disclaimer : <p> The information presented here is intended to offer information to advanced users . However , Code42 does not design or test products for the use described here . This information is presented because of user requests . <p> Our Customer Champions can not assist you with unsupported processes , so you assume all risk of unintended behavior . You may want to search our support forum for information from other users . <h> Overview <p> CrashPlan stores a cache of temporary information on your computer , including information about your destinations , the data you have on your computer , and a number of settings that help CrashPlan run fast @ @ @ @ @ @ @ @ @ @ recommended you clear it to resolve the issue . <p> Alternatively , you can move the cache to a drive with more storage or higher IO speeds as described in this article ; however , please note that this is an- unsupported process . <h> Recommended Solution <p> Find- my.service.xml If you used the default install location , then the file is located in the following directory : <p> Windows Vista , 7 , 8 , 10 , Server 2008 , and Server- 2012 : C : **26;6306;TOOLONG view this hidden folder , open Windows Explorer and paste the path in the address bar. - If you installed per user , see the file and folder hierarchy- for file locations . <p> Windows XP : C : Documents and SettingsAll UsersApplication DataCrashPlanconf To view this hidden folder , open Windows Explorer and paste the path in the address bar. - If you installed per user , see the file and folder hierarchy- for file locations . <p> OS X : /Library/Application **25;6334;TOOLONG you installed per user , see the file and folder hierarchy- for file locations . <p> Linux : @ @ @ @ @ @ @ @ @ @ in a text editor as an administrator ( Windows ) or with an editor that has root permissions ( Mac ) See External Resources for more information <p> Find the line enclosed by- **35;6388;TOOLONG <p> Change the file path inside the &lt;cachePath&gt; section to the file path where you want to move the cacheFor example : D : ProgramsCacheStorage <p> Navigate to the appropriate directory below and delete the cache folder : <p> Windows Vista , 7 , 8 , 10 , Server 2008 , and Server- 2012 : C : **27;6425;TOOLONG view this hidden folder , open Windows Explorer and paste the path in the address bar. - If you installed per user , see the file and folder hierarchy- for file locations . <p> Windows XP : - C : Documents and SettingsAll UsersApplication DataCrashPlancacheTo view this hidden folder , open Windows Explorer and paste the path in the address bar. - If you installed per user , see the file and folder hierarchy- for file locations . <p> Deleting the cache does not impact the data stored in your backups or change your settings . CrashPlan will @ @ @ @ @ @ @ @ @ @ Solution <p> Alternatively , if you are comfortable with creating soft links ( also know as symbolic links or symlinks ) , you can use a soft link- to redirect data from the original cache folder to an alternative location . <h> External Resources <p> Please note that the instructions linked below are provided as a reference , but they have not been tested by- Code42. 
@@45151589 @5151589/ <h> A site dedicated to travel ( promotions , review , tips &amp; tricks ) <h> Category Archive : Airlines <p> United Airlines : Update Priority This is usually a popular questions , especially for those who are trying to use an actual upgrade like the regional or global . Below is the upgrade priority and explains how the use of an regional or global upgrade if it ca n't be secured at the time it is applied to a <p> LONG ... Attached is the PDF containing the list of international phone numbers you can use to call the 1k United line. - intlcallsupport1Konly <p> Just a couple days go I received an email for a promotion with AA . Its a pretty sweet deal and its great for anyone trying to secure status with them this year or get an early start next year . Sadly I 've only flown American once this year so this does n't  help me much . Qualify For <p> Earn 50 bonus miles per check in ( per day ) via Facebook or Foursquare , within 2 miles of approved airports within the @ @ @ @ @ @ @ @ @ @ , you will receive a confirmation message to the social media chosen , containing information about nearby retailers " you can opt out of these messages if you 'd like <p> GET AWAY WITH DOUBLE THE MILES Earn double bonus miles for a limited time on your next flight . Right now is a great time to rack up bonus miles. - Register by November- 1st , then book your ticket and complete your travel between September 15 and November 15 , 2011 , to earn your double flown bonus miles on select <p> Marriott Hotelers , More Stays , More Miles Earn up to 30k bonus miles from 9/1/11-12/31/11 Beginning with your 2nd stay , you 'll earn bonus miles in addition to the general miles you earn : 2nd stay = +1000 miles 3rd stay = +1500 miles 4th stay = +1500 miles 5th stay or more = +2000 miles You must <p> Amtrak Travelers , Amtrak has once again released its fall promotion , giving rewards members an opportunity to earn twice the points between 9/7 and 11/23 . In order to qualify , you must register @ @ @ @ @ @ @ @ @ @ your reservation . Registration can be completed at : https : **34;6487;TOOLONG . <p> If you are planning to start flying Southwest this is a great time to sign up and create your Rapid Rewards card . Currently they have a promotion so when you open an account you get 250 points . Use the following link while the promotion lasts : - LONG ... <p> Delta currently- has a promotion through which you can enjoy 1,000 miles when you use the Delta app to check in between June 30 and September 7 , 2011 . Unfortunately there is only one bonus per account for the duration of the promotion , but at least you 'll be 1,000 miles closer to a reward milestone . So hurry 
@@45151591 @5151591/ <h> Deployment Tool for the bootable Unattended Windows installation <p> Every time I need to create an unattended Windows Installation CD , or simply create a bootable CD out of the Windows CD files , I use a tool called nLite which is really good IMMO . You can download it at : http : //www.nliteos.com/ or try http : //www.vlite.net/ if you are trying to make it work with Vista <p> The main reason I wanted to write about this is because every time I need that tool , I forget the name and try for like 30 min to guess it . This program has a wide range of options that let you configure many aspects of the installation as well as components to install and the ability to slipstream a service pack ( useful if you want to make a Windows XP with SP3 CD out of your Windows XP with SP2 CD and XP SP3 ) 
@@45151592 @5151592/ 55329 @qwx465329 <h> Tag Archive : Computer data storage <p> How to : Configure Swappiness in Ubuntu In one of my previous articles on how to setup virtual memory / swap memory in Ubuntu I covered how to install a swap space for a Windows Azure VM . I deployed swap mostly as a precaution against running out of memory which could result on applications unable to 
@@45151594 @5151594/ 55329 @qwx465329 <h> Tag Archive : Email alias <p> How to : Add an email Alias to your Outlook.com account There are several different kinds of aliases that you can create for an Outlook.com account . The easiest way by far is to use what I call for a lack of a better name an " on demand " alias . After that your best choice would be to 
@@45151595 @5151595/ 55329 @qwx465329 <h> Tag Archive : ClassPNP.sys <p> My current problem is that I- recently- got a new VM from a data center and I need to run it on my machine . However , Windows comes up with the- bsod ( blue screen of death ) with a Stop error of- unmountablebootvolue 0x000000ED . Ive been looking for several options on the internet that range from making this Hard Drive IDE 
@@45151596 @5151596/ <h> Tag Archive : Active Directory <p> How to : Resolve error " Computer/Name Domain Changes . The following error occurred attempting to join the domain : The requested resource is in use . " One of my latest missteps was not completing the migration of all servers to the new domain name when I performed an Active Directory Domain Name change . Well , one of the unintended consequences <p> How to : Recover an Exchange server using the RecoverServer switch So a bit of background : I was performing a migration from onPremise to Cloud Mailboxes and the hardware the VM was running on was not the fastest . So I had this great idea : Perform a scheduled fail over to the faster server and continue working <p> What is http : **37;6523;TOOLONG ? The web address : - http : **38;6562;TOOLONG is generally used by applications like the Windows Azure Active Directory Sync tool ( to sync with Microsofts online Azure AD service ) to authenticate against those services . <p> How to : Manage the Certificate Store on your local machine using the command @ @ @ @ @ @ @ @ @ @ am playing a lot with certificates in order to- authenticate traffic across the network . Some of the most useful shell commands I have found are listed below , hopefully theyll help you manage your <p> How to add your Ubuntu computer to your Active Directory Domain when your Windows SBS Domain ends in . local using Likewise There are a number of options in order to get Active Directory integration with your Ubuntu systems . I personally liked Likewise as it provides a GUI and does a lot of the steps for <p> Migration notes from Windows SBS 2011 Standard to Windows Server 2012 Essentials As you probably have already found out , Microsofts successor for Windows Small Business Server 2011 is no other than Windows 2012 Essentials . Many people have consider this a let down as Windows 2012 Essentials loses a lot of the SBS 2011 premium features 
@@45151597 @5151597/ <h> How to : Get WooCommerce to automatically recreate pages <h> How to get WooCommerce to automatically recreate pages <p> As a WooCommerce user you are likely to encounter the need to recreate the pages the system creates during install at one point in your life . Sometimes you just accidentally delete a page , in other scenarios you are messing with the backend ( database ) and need to reinstall the system , but in other cases such as mine from one version to the next there is this new functionality in a new page you are simply missing because you have the old set of pages . Another possible scenario is that you set up your system in the default language ( probably English ) and after that you wish to change to another language ( say Spanish ) but those pages now at least their names are in English . <p> This could be very frustrating , as the next best thing you could possibly come up with would be to reinstall wordpress and WooCommerce or use another installation to copy the page contents to yours @ @ @ @ @ @ @ @ @ @ force WooCommerce to reinstall your pages and therefore you can have the latest and greatest versions in the currently supported language your instance is running on . <h> Workaround 1 : <p> In order to force the page creation dialog to reappear simply follow these steps : <p> Simply go to WooCommerce &gt; Pages <p> Unset all of the select boxes for pages ( including the shop base option ) <p> De-activate the WooCommerce plugin <p> Re-activate the WooCommerce plugin <p> It will ask you whether or not you want to install the pages , yay ! <h> Workaround 2 : <p> If you notice what happens when you install WooCommerce during the first time or after you follow workaround 2 is that a URL parameter is used to indicate the WooCommerce settings page to recreate the pages . In that case you could follow this simple instructions to do the same : <p> Make sure you have deleted the current pages . I am not certain this is necessary but this way you ensure the names are correct and the URLs are not in use by other pages . @ @ @ @ @ @ @ @ @ @ if there is any impact . <p> Now , enter the admin backend and go to WooCommerce &gt; Settings . Take a look at the URL : LONG ... <p> I have personally tried both workarounds/solutions with no problems . I have been bugged by not having all the pages set on one of my installations plus the ones I had were in a different language . This made it possible for me to get the latest pages on the correct language without too much effort . Do share your experience and let me know if you have any issues 